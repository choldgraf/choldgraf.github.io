{"version":"1","records":[{"hierarchy":{"lvl1":"About me"},"type":"lvl1","url":"/about","position":0},{"hierarchy":{"lvl1":"About me"},"content":"I‚Äôm the Executive Director of \n\n2i2c: the International Interactive Computing Collaboration, a non-profit dedicated to open source infrastructure for interactive computing in research and education.","type":"content","url":"/about","position":1},{"hierarchy":{"lvl1":"About me","lvl2":"What I do"},"type":"lvl2","url":"/about#what-i-do","position":2},{"hierarchy":{"lvl1":"About me","lvl2":"What I do"},"content":"Open Source and Open Community\n\nI work on projects that make research and education more effective, efficient, inclusive, and transparent. I also believe that it‚Äôs important for open source technical projects to have healthy and inclusive communities behind them.\n\nI contribute to a number of open-source projects that support the research and education community. In particular I am a core contributor to many projects in the \n\nPyData ecosystem, and in particular the following major projects:\n\nProject Jupyter - an open platform and community for interactive data science.\n\nJupyterHub - shared infrastructure for interactive computing on shared infrastructure.\n\nthe Binder Project - tools and standards for interactive, reproducible, sharable repositories.\n\nthe Executable Books Project - open source tools that facilitate publishing computational narratives using the Jupyter ecosystem.\n\nFor an idea of the kinds of projects that I work on, check out my \n\nmy GitHub page\n\nScientific Research and Open Scholarship\n\nI used to study cognitive and computational neuroscience, using predictive modeling to study the ways in which the human auditory system understands speech.\n\nA good example of my work is this paper about \n\nüí¨ representations influencing the perception of noisy speech.\n\nTo learn about these methods, see my methods paper about \n\nüß† encoding and decoding models of speech perception.\n\nI‚Äôve also been involved with several projects across other fields in academic / scientific research, especially ‚Äúmeta‚Äù issues in open source communities and open standards and practices in scientific fields.\n\nFor a list of publications and scholarly artifacts in which I‚Äôve been involved,\ncheck out \n\nmy ORCID page or \n\nmy Google Scholar page.","type":"content","url":"/about#what-i-do","position":3},{"hierarchy":{"lvl1":"About me","lvl2":"CV"},"type":"lvl2","url":"/about#cv","position":4},{"hierarchy":{"lvl1":"About me","lvl2":"CV"},"content":"Here‚Äôs some more ‚Äúofficial‚Äù CV-style info, if that‚Äôs what you‚Äôre looking for.","type":"content","url":"/about#cv","position":5},{"hierarchy":{"lvl1":"About me","lvl3":"Education","lvl2":"CV"},"type":"lvl3","url":"/about#education","position":6},{"hierarchy":{"lvl1":"About me","lvl3":"Education","lvl2":"CV"},"content":"B.S. in Neuroscience, Tulane University, 2009\n\nM.S. in Neuroscience, Tulane University, 2010\n\nPh.D. in Neuroscience, University of California at Berkeley, 2017\n\nIf you want a hard-copy CV, you can find a \n\nreasonably up-to-date CV here.","type":"content","url":"/about#education","position":7},{"hierarchy":{"lvl1":"About me","lvl2":"About this website"},"type":"lvl2","url":"/about#about-this-website","position":8},{"hierarchy":{"lvl1":"About me","lvl2":"About this website"},"content":"This is an experiment in hosting my personal website and blog via Sphinx extensions instead of using Jekyll. It uses the \n\npydata sphinx theme along with \n\nablog for blogging, and the \n\nmyst-parser and \n\nmyst-nb packages for writing posts in Markdown and Jupyter Notebooks.","type":"content","url":"/about#about-this-website","position":9},{"hierarchy":{"lvl1":"About me","lvl2":"A rough timeline"},"type":"lvl2","url":"/about#about-timeline","position":10},{"hierarchy":{"lvl1":"About me","lvl2":"A rough timeline"},"content":"Below is a (somewhat experimental) rough timeline of what I‚Äôve been up to over the past several years.\n\n2020: Co-Founded \n\n2i2c\n\n2i2c is a non-profit organization that makes interactive computing environments more accessible to the research and education community. It supports the delpoyment of open infrastructure for this community, and also supports the open source communities underneath this infrastructure.\n\n2017 Joined the Jupyter Project\n\nAfter finishing graduate school I began a Post-Doc with \n\nFernando Perez, focusing my work around applying and developing the Jupyter ecosystem to support workflows in research and education. Here are a few of the main things I did during this time:\n\nCo-lead the \n\nBinder Project for interactive, reproducible, sharable computational environments.\n\nCo-lead the \n\nJupyterHub team, which builds tools for deploying interactive computing environments on shared infrastructure.\n\nServed as ‚Äúcommunity architect‚Äù for the \n\nBerkeley Data Science Education Program, which runs JupyterHubs on campus for education.\n\nCo-launched the \n\nJupyter Book project, a tool for building books from collections of notebooks and markdown files.\n\n2011 Graduate school at Berkeley\n\nMy scientific training began as a neuroscientist, where I studied the ways that humans process speech and sound. I used computational models and machine learning to tie patterns of brain activity to spectral features of sound. In particular I focused on speech in noise to understand how our brain fills in missing details in auditory information.\n\n2009 Graduated from Tulane\n\nGraduated from Tulane University with a B.S. and an M.S. in Neuroscience.","type":"content","url":"/about#about-timeline","position":11},{"hierarchy":{"lvl1":"Blog"},"type":"lvl1","url":"/blog","position":0},{"hierarchy":{"lvl1":"Blog"},"content":"Below are a few of the latest posts in my blog.\nYou can see a full list by year to the left.\n\nBetter blog lists with the MyST AST\n\n\n\n\nOn my journey to learn more about writing with [the new MyST engine](https:///mystmd.org), I built upon [my recent update to my blog infrastructure](./programmatic-myst-with-jupyter.md) and made some improvements to my blog post list.\nHere's what it looks like now:\n\n````{note} Click here to see how it looks now\n:class: dropdown\n```{postlist}\n:number: 3\n```\n````\n\nHere's a quick rundown\n\nDate: November 09, 2024 | Author: Chris Holdgraf\n\nGenerate MyST with Jupyter and insert it into content programmatically\n\n\n\n\n\nWhile I've been [converting my blog to use the new MyST engine](./mystmd-with-the-blog.md), I discovered a useful MyST feature. It's not yet possible to [natively parse Jupyter Markdown outputs as MyST](https://github.com/jupyter-book/mystmd/issues/1026) but there's a workaround if you don't mind generating a temporary file.\n\nThe trick is to _write to a temporary file_\n\nDate: November 04, 2024 | Author: Chris Holdgraf\n\nRe-building my blog with MySTMD\n\n\n\nWow it has been a long time since I've last-written here. It turns out that having two small children and a very demanding job means you don't have as much time for blogging. But that's a whole different blog post...\n\nI've decided to convert my blog to use the new [MyST\n\nDate: November 01, 2024 | Author: Chris Holdgraf\n\nA few random opportunities in AI for Social Good\n\n\n\n\nRecently a few friends have reached out asking if I knew of any opportunities to work on AI-related things that also have some kind of pro-social tie-in.\nI think a lof people see AI as a technology with a lot of potential, but in an environment of companies that don't seem\n\nDate: October 02, 2023 | Author: Chris Holdgraf\n\nA Sphinx directive for social media embeds\n\n\n\n\n:::{note} This probably doesn't work anymore\nI've since moved my blog to use [the MyST Document Engine](https://mystmd.org) so this example will no longer work on my personal blog. See [this permalink for the latest working version](https://github.com/choldgraf/choldgraf.github.io/blob/ae8ee9792c74aac72f46c645d19352abc439d572/blog/2023/social-directive.md).\n:::\n\nI often want to link to social and other types of web-based media in my Sphinx\n\nDate: February 15, 2023 | Author: Chris Holdgraf\n\nReport from FOSDEM23: beautiful chaos in a conference\n\n\n\n\nI recently attended [FOSDEM 2023](https://fosdem.org/2023/), my first FOSDEM!\nI had heard of the conference before, but hadn't really looked into it too much.\nFortunately, after some urging from friends and social media, I took a deeper look and decided I should join to see what all the fuss was about.\n\nHere are a\n\nDate: February 06, 2023 | Author: Chris Holdgraf\n\nBundle extensions with your Sphinx theme\n\n\n\n\nSphinx is great because it has a ton of useful extensions that let you grow its functionality.\nHowever, a downside of this is that users have to actually _learn about_ those extensions and activate them manually.\nIt's not hard, but it's a non-trivial amount of discovery work.\n\nOne way to solve this is\n\nDate: January 19, 2023 | Author: Chris Holdgraf\n\nInstall dependencies from GitHub with `pyproject.toml` or `requirements.txt`\n\n\n\n\nThis is a short post to demonstrate how to install packages directly from GitHub with `pyprojects.toml` or `requirements.txt`, including custom branches and commits.\nIt will focus on `pyprojects.toml` because this is newer and there's less information about it, but the general pattern holds for `requirements.txt` as well.\n\nIn `pyproject.toml`, you can specify\n\nDate: December 31, 2022 | Author: Chris Holdgraf\n\nReport from the JupyterLite workshop: WebAssembly is pretty cool\n\n\n\n\nI recently attended [the JupyterLite community workshop in Paris](https://blog.jupyter.org/community-workshop-jupyterlite-e992c61f5d7f?source=collection_home---6------6-----------------------), here are some quick thoughts from the three-day event[^ack].\n\n[^ack]: Many thanks to the [QuantStack](http://quantstack.com/) team for organizing this event, and to [OVHCloud](https://www.ovhcloud.com/en/) for providing a physical space for everyone. \n\nFor those without any background, JupyterLite is a distribution of Jupyter's user\n\nDate: December 10, 2022 | Author: Chris Holdgraf\n\nLoad and plot a remote font with Matplotlib\n\n\n\n\nAs part of [my `sphinx-social-previews`](https://github.com/choldgraf/sphinx-social-previews) prototype, I wanted to be able to use the [Roboto Font from Google](https://fonts.google.com/specimen/Roboto) in image previews.\nHowever, Roboto is often not loaded on your local filesystem, so it took some digging to figure out how to make it possible to load via [Matplotlib's text plotting functionality](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.text.html).\n\nHere's\n\nDate: December 06, 2022 | Author: Chris Holdgraf\n\nHow to update Sphinx options during the build\n\n\n\n\nAs part of [the `pydata-sphinx-theme`](https://github.com/pydata/pydata-sphinx-theme/pull/1075) we have a few settings that auto-enable extensions and configure them on behalf of the user.\nIt has always been mysterious to me how to do this properly **during the Sphinx build**.\nIt's easy to configure things with `conf.py` ahead of time, but what if you want\n\nDate: December 05, 2022 | Author: Chris Holdgraf\n\nAutomatically update pre-commit hook versions\n\n\n\n\nI figured out a way to automatically update all of the git `pre-commit` hook versions at once!\n\n[`pre-commit`](https://pre-commit.com/) is a useful command line tool for running simple commands before every `git` commit.\nI use it to enforce things like [`flake8`](https://flake8.pycqa.org/) and [`black`](https://github.com/psf/black) in many of my projects.\n\nHowever, I find it really annoying\n\nDate: December 03, 2022 | Author: Chris Holdgraf\n\n`subprocess.run` can execute shell commands directly\n\n\n\n\nI often run shell commands in Python via the [`subprocess.run` command](https://docs.python.org/3/library/subprocess.html#subprocess.run).\nOne thing that has always bugged me is that this required you to split commands into a list before it'd work properly.\nFor example, you'd have to do:\n\n```python\nimport subprocess\nimport shlex\n\nsubprocess.run(*shlex.split(\"ls -l\"))\n```\n\nToday I discovered that you don't have to do this!\nThere's a\n\nDate: November 29, 2022 | Author: Chris Holdgraf\n\nFix phantom GitHub workflows in your ci-cd with protected branch rules\n\n\n\n\nHave you ever had a GitHub pull request show \"phantom\" workflows that never pass?\nThis looks like one or more workflows that are in a constant **waiting state**, with a yellow status indicator, and that never complete.\n\nIt looks something like this:\n\n```{image} https://user-images.githubusercontent.com/1839645/204134864-da2541f0-ff4f-4d9f-8c80-aa8c4437d8a0.png\n```\n\nIf you run into this, it may be because of\n\nDate: November 27, 2022 | Author: Chris Holdgraf\n\nCustom roles and domains in Sphinx with one line\n\n\n\n\nI was working on [the roles and structure section of the 2i2c Team Compass](https://compass.2i2c.org) and found a nifty feature in Sphinx that I hadn't known before.\n\nYou can currently add labels to any section with the following MyST Markdown structure:\n\n```md\n(mylabel)=\n\nAnd now I [reference it](mylabel).\n```\n\nHowever, there are no **semantics** attached to this\n\nDate: November 21, 2022 | Author: Chris Holdgraf\n\nAutomatically redirect folders in Sphinx websites\n\n\n\n\nI spent a bit of time today updating my website after some changes in the MyST-NB and Sphinx Design ecosystems.\nAlong the way, I decided to redirect `/posts/` to `/blog/`, since it seems `/blog/` is a much more common folder to use for blog posts.\n\nThis posed a problem, because [the `sphinx-rediraffe`\n\nDate: November 19, 2022 | Author: Chris Holdgraf\n\nAutomatically updating my publications page with ORCID and doi.org\n\n\n\n\nFor a while I've had a hand-crafted `.bibtex` file stored locally for [my `publications/` page](../../publications.md).\nHowever, manually updating local text file is a pain to remember, especially since there are many services out there that automatically track new publications.\n\n:::{admonition} Update!\nA [helpful suggestion on Twitter](https://twitter.com/temorrell/status/1594749942316208128) allowed me to include the full citation\n\nDate: November 19, 2022 | Author: Chris Holdgraf\n\nAsk Twitter: Why don't academic researchers use cloud services?\n\n\n\n\n_this is an experiment at making my [Twitter conversations](https://twitter.com/choldgraf) a bit more useful and archivable over time. It's going to be a bit messy and unpolished, but hopefully that makes it more likely I'll actually do it :-)_\n\nOver the past decade, cloud infrastructure has become increasingly popular in industry.\nAn ecosystem\n\nDate: September 05, 2022 | Author: Chris Holdgraf\n\nServing in two roles at once via pre-recorded tutorials\n\n\n\n\nAt AGU 2021 this year I was asked to give [a short tutorial introduction to Jupyter Book](https://www.youtube.com/watch?v=lZ2FHTkyaMU).\nThe tutorial was 30 minutes long, and the session was fully remote.\n\nThis posed a few challenges:\n\n- Tutorials almost **always** go over time - particularly if you're taking questions from attendees.\n- It is tricky to\n\nDate: December 17, 2021 | Author: Chris Holdgraf\n\nContributing to open source: A short guide for organizations\n\n\n\n\nOver the years I've had a recurring question from people who are in organizations both big and small: _how can we participate in open source communities?_\n\nWhether it is because of altruism or strategic importance, many companies, research groups, non-profits, etc _want_ to be involved in open source projects (particularly large\n\nDate: November 08, 2020 | Author: Chris Holdgraf\n\nA new blog with Sphinx\n\n\n\n\nI recently re-wrote all of the infrastructure for my blog so that it now builds on top of the Sphinx ecosystem! This is a short post to describe the reasons for doing so, and a bit about the implementation.\n\n````{image} images/sphinx-logo.png\n:class: bg-dark\n````\n\n\nThis is a great question. The answer to \"should you\n\nDate: October 10, 2020 | Author: Chris Holdgraf\n\nBuild a simple timeline with `sphinx-design`\n\n\n\n\n:::{warning} This probably doesn't work anymore\nI'm building my blog with the [MyST Markdown engine](https://mystmd.org) now, which means that all of this sphinx-specific stuff probably doesn't work anymore :-)\n\nIf you want to see a version of this page that worked, [check out this file in GitHub](https://github.com/choldgraf/choldgraf.github.io/blob/ae8ee9792c74aac72f46c645d19352abc439d572/blog/2020/sphinx-design-timeline.md).\n:::\n\nDate: January 22, 2020 | Author: Chris Holdgraf\n\nWhat do people think about rST?\n\n\n\n\nPublishing computational narratives has always been a dream of the Jupyter Project,\nand there is still a lot of work to be done in improving these use-cases. We've made\na lot of progress in providing open infrastructure for reproducible science with\n[JupyterHub](https://jupyterhub.readthedocs.io/en/stable/) and\n[the Binder Project](https://mybinder.org/), but what about the documents themselves?\nWe've recently been\n\nDate: January 22, 2020 | Author: Chris Holdgraf\n\nWhat would Python-style governance look like in Jupyter?\n\n\n\n\nThis is the second in a series of blog posts that explores what it'd look like to\ndirectly port the governance model of other communities into the Jupyter project.\nYou can find the [first post about Rust here](https://chrisholdgraf.com/rust-jupyter-governance).\n\n**Note**: These posts are meant as a thought experiment rather than a proposal. Moreover,\nall the\n\nDate: October 27, 2019 | Author: Chris Holdgraf\n\nWhat would Rust-style governance look like in Jupyter?\n\n\n\n\nAs I've written about before, I [like Rust's governance structure](https://chrisholdgraf.com/rust-governance).\nI mean, who can't get behind a community that\n[lists governance as a top-level page on its website](https://www.rust-lang.org/governance)?\n\nJupyter is currently in the middle of\n[figuring out the next phase of its governance structure](https://discourse.jupyter.org/t/governance-office-hours-meeting-minutes/1480/26),\nand so I have been thinking about\nwhat this might look like.\n\nDate: October 13, 2019 | Author: Chris Holdgraf","type":"content","url":"/blog","position":1},{"hierarchy":{"lvl1":"Coherence vs. Correlation - a simple simulation"},"type":"lvl1","url":"/blog/2015/2015-05-27-coherence-correlation","position":0},{"hierarchy":{"lvl1":"Coherence vs. Correlation - a simple simulation"},"content":"---\ncategory: til\ndate: '2015-05-27'\nkernelspec:\n  display_name: Python 3 (ipykernel)\n  language: python\n  name: python3\nlanguage_info:\n  codemirror_mode:\n    name: ipython\n    version: 3\n  file_extension: .py\n  mimetype: text/x-python\n  name: python\n  nbconvert_exporter: python\n  pygments_lexer: ipython3\n  version: 3.10.14\nredirect: coherence-correlation-comparison\ntags:\n- python\n- programming\n- timeseries\n- correlation\ntitle: Coherence correlation\n---\n\n\n\ncategory: til\ndate: ‚Äò2015-05-27‚Äô\nkernelspec:\ndisplay_name: Python 3 (ipykernel)\nlanguage: python\nname: python3\nlanguage_info:\ncodemirror_mode:\nname: ipython\nversion: 3\nfile_extension: .py\nmimetype: text/x-python\nname: python\nnbconvert_exporter: python\npygments_lexer: ipython3\nversion: 3.10.14\nredirect: coherence-correlation-comparison\ntags:\n\npython\n\nprogramming\n\ntimeseries\n\ncorrelation\ntitle: Coherence correlation\n\nNote - you can find the nbviewer of this post \n\nhere","type":"content","url":"/blog/2015/2015-05-27-coherence-correlation","position":1},{"hierarchy":{"lvl1":"Coherence vs. Correlation - a simple simulation"},"type":"lvl1","url":"/blog/2015/2015-05-27-coherence-correlation#coherence-vs-correlation-a-simple-simulation","position":2},{"hierarchy":{"lvl1":"Coherence vs. Correlation - a simple simulation"},"content":"A big question that I‚Äôve always wrestled with is the difference between correlation and coherence. Intuitively, I think of these two things as very similar to one another. Correlation is a way to determine the extent to which two variables covary (normalized to be between -1 and 1). Coherence is similar, but instead assesses ‚Äúsimilarity‚Äù by looking at the similarity for two variables in frequency space, rather than time space.\n\nThere was a nice paper that came out a while back that basically compared these two methods in order to see when they‚Äôd produce the same result, when they‚Äôd produce different results, and when they‚Äôd break down [1]. They made a lot of nice plots like this:\n\n\n\nHere I am recreating this result in the hopes of giving people a set of scripts to play around with, and giving a bit more intuition.\n\n[1] \n\nhttp://‚Äãwww‚Äã.ncbi‚Äã.nlm‚Äã.nih‚Äã.gov‚Äã/pubmed‚Äã/8947780\n\nFirst things first, we‚Äôll import some tools to use\n\nimport pandas as pd\nimport mne\nfrom itertools import product\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nsns.set_style('white')\n%matplotlib inline\n\n","type":"content","url":"/blog/2015/2015-05-27-coherence-correlation#coherence-vs-correlation-a-simple-simulation","position":3},{"hierarchy":{"lvl1":"Coherence vs. Correlation - a simple simulation","lvl2":"Creating our sine waves"},"type":"lvl2","url":"/blog/2015/2015-05-27-coherence-correlation#creating-our-sine-waves","position":4},{"hierarchy":{"lvl1":"Coherence vs. Correlation - a simple simulation","lvl2":"Creating our sine waves"},"content":"Recall that the equation for a sinusoidal wave is:Asin(2{\\pi}ft + 2\\pi\\phi)\n\nWhere f is the frequency of the wave, t\n\n indexes time, and 2\\pi\\phi\n\n defines a phase offset of the wave. Then, A\n\n scales the wave‚Äôs amplitude.\n\n# We can generate these sine wave parameters, then stitch them together\namplitude_values = [1, 3, 10, 15]\nphase_values = [0, .25, .33, .5]\nfreq = 2\nsignal_vals = list(product(amplitude_values, phase_values))\namps, phases = zip(*signal_vals)\n\n# We'll also define some noise levels to see how this affects results\nnoise_levels = [0, 2, 4, 8]\n\n# Now define how long these signals will be\nt_stop = 50\ntime = np.arange(0, t_stop, .01)\n\n# We're storing everything in dataframes, so create some indices\nix_amp = pd.Index(amps, name='amp')\nix_phase = pd.Index(phases, name='phase')\n\n# Create all our signals\nsignals = []\nfor noise_level in noise_levels:\n    sig_ = np.array([amp * np.sin(freq*2*np.pi*time + 2*np.pi*phase) for amp, phase in signal_vals])\n    noise = noise_level * np.random.randn(*sig_.shape)\n    sig_ += noise\n    ix_noise = pd.Index([noise_level]*sig_.shape[0], name='noise_level')\n    ix_multi = pd.MultiIndex.from_arrays([ix_amp, ix_phase, ix_noise])\n    signals.append(pd.DataFrame(sig_, index=ix_multi))\nsignals = pd.concat(signals, 0)\nsignals.columns.name = 'time'\n\n","type":"content","url":"/blog/2015/2015-05-27-coherence-correlation#creating-our-sine-waves","position":5},{"hierarchy":{"lvl1":"Coherence vs. Correlation - a simple simulation","lvl2":"Computing connectivity"},"type":"lvl2","url":"/blog/2015/2015-05-27-coherence-correlation#computing-connectivity","position":6},{"hierarchy":{"lvl1":"Coherence vs. Correlation - a simple simulation","lvl2":"Computing connectivity"},"content":"Now we‚Äôve got a bunch of sinewaves with the parameters chosen above. Next, we will calculate the coherence and the correlation between all pairs of signals. This way we can see how these values change for different kinds of input signals.\n\ncon_all = []\nfor ix_noise, sig in signals.groupby(level='noise_level'):\n    # Setting up output indices\n    this_noise_level = sig.index.get_level_values('noise_level').unique()[0]\n    ix_ref = np.where(sig.eval('amp==3 and phase==0'))[0][0]\n    ix_time = pd.Index(range(sig.shape[0]), name='time')\n    ix_cc = pd.Index(['cc']*sig.shape[0], name='con')\n    ix_coh = pd.Index(['coh']*sig.shape[0], name='con')\n\n    # Calculating correlation is easy with pandas\n    cc = sig.T.corr().astype(float).iloc[:, ix_ref]\n    cc.name = None\n    cc = pd.DataFrame(cc)\n    # We'll use MNE for coherenece\n    indices = (np.arange(sig.shape[0]), ix_ref.repeat(sig.shape[0]))\n    con, freqs, times, epochs, tapers = mne.connectivity.spectral_connectivity(\n        sig.values[None, :, :], sfreq=freq, indices=indices)\n    con_mn = con.mean(-1)\n    con_mn = pd.DataFrame(con_mn, index=cc.index)\n    \n    # Final prep\n    con_mn = con_mn.set_index(ix_coh, append=True)\n    cc = cc.set_index(ix_cc, append=True)\n    con_all += ([con_mn, cc])\ncon_all = pd.concat(con_all, axis=0).squeeze().unstack('noise_level')\n\n","type":"content","url":"/blog/2015/2015-05-27-coherence-correlation#computing-connectivity","position":7},{"hierarchy":{"lvl1":"Coherence vs. Correlation - a simple simulation","lvl2":"Visualizing results"},"type":"lvl2","url":"/blog/2015/2015-05-27-coherence-correlation#visualizing-results","position":8},{"hierarchy":{"lvl1":"Coherence vs. Correlation - a simple simulation","lvl2":"Visualizing results"},"content":"First off, we‚Äôll look at what happens to sine waves of varying parameters, for different levels of noise.\n\nRemember, each tuple is (amplitude, phase_lag). The first number controls how large the signal is, and the second controls the difference in phase between two sine waves.\n\nf, axs = plt.subplots(2, 2, figsize=(15, 10))\nfor ax, (noise, vals) in zip(axs.ravel(), con_all.iteritems()):\n    ax = vals.unstack('con').plot(ax=ax)\n    ax.set_title('Noise level: {0}'.format(noise))\n    ax.set_ylim([-1.1, 1.1])\n\nThat‚Äôs already an interesting picture - as you can see, coherence is far more robust to differences between the two signals. Here are a few thoughts:\n\nCorrelation varies widely (between 0 and 1) for differences in phase lag. However, coherence remains relatively stable.\n\nCoherence values are smaller in general for a signal with any noise\n\nHowever, coherence is more robust for increasing levels of noise, while correlations start to drop to 0\n\nTo illustrate number 1, let‚Äôs plot correlation and coherence against each other:\n\nplt_df = con_all.stack('noise_level').unstack('con').reset_index('noise_level')\nax = plt_df.plot('cc', 'coh', c='noise_level', kind='scatter',\n                 cmap=plt.cm.Reds, figsize=(10, 5), alpha=.5, s=50)\nax.set_title('CC vs Coherence')\n\nAs you can see here, coherence remains the same (except for when it occasionally increases to 1) while correlation is much more dependent on the phase relationship between the signals. Moreover, as the signal SNR degrades, the correlation shrinks to 0, while the coherence remains the same.\n\nLet‚Äôs take a look at how the correlation and coherence relate to the actual shape of the signals:\n\n# Set up a dataframe for plotting\nnoise_level = noise_levels[1]\nplt_df = con_all.copy()\nplt_df = con_all[noise_level].unstack('con')\n\n# Define 16 signals to plot\nsig_combinations = list(product([ix_ref], range(16)))\n\nplt_sig = signals.xs(noise_level, level='noise_level')\nn_combs = len(sig_combinations)\nf, axs = plt.subplots(n_combs/4, 4, figsize=(15, n_combs/3*5))\nfor (comp_a, comp_b), ax in zip(sig_combinations, axs.ravel()):\n    plt_sig.iloc[[comp_a, comp_b]].T.head(250).plot(ax=ax, legend=None)\n    ax.set_title('CC: {0}\\nCoh: {1}'.format(*plt_df.iloc[comp_b, :].values))\n\nAnother way of looking at it with scatterplots...\n\n# Set up a dataframe for plotting\nnoise_level = noise_levels[1]\nplt_df = con_all.copy()\nplt_df = con_all[noise_level].unstack('con')\n\n# Define 16 signals to plot\nsig_combinations = list(product([ix_ref], range(16)))\n\nplt_sig = signals.xs(noise_level, level='noise_level')\nn_combs = len(sig_combinations)\nf, axs = plt.subplots(n_combs/4, 4, figsize=(15, n_combs/3*5))\nfor (comp_a, comp_b), ax in zip(sig_combinations, axs.ravel()):\n    iia, iib = plt_sig.iloc[[comp_a, comp_b]].T.head(250).values.T\n    ax.scatter(iia, iib)\n    ax.set_title('CC: {0}\\nCoh: {1}'.format(*plt_df.iloc[comp_b, :].values))\n\nFinally, for a more direct comparison, we can look directly at the difference between the two as a function of both noise level and the sine wave parameters.\n\ndiff_con = con_all.stack('noise_level').unstack('con')\ndiff = diff_con['cc'] - diff_con['coh']\ndiff = diff.unstack('noise_level')\n\ndiff.plot(figsize=(15, 5))\n\nSo what does this mean? Well, the relationship between coherence and correlation is too complicated to sum it up in a single line or two. However, it is clear that correlation is more sensitive to differences between signals in time. Coherence, on the other hand, is more reliable for these differences. Moreover, correlation degrades quickly with an increase in noise, while coherence remains the same.\n\nAs such, if you care about understanding the relationship between two signals as it pertains to time, then perhaps correlation is the way to go. On the other hand, if you want a robust estimate of the amount of overlap in the structure of two signals, then coherence may be the best bet.","type":"content","url":"/blog/2015/2015-05-27-coherence-correlation#visualizing-results","position":9},{"hierarchy":{"lvl1":"Scraping craigslist"},"type":"lvl1","url":"/blog/2015/2015-08-30-craigslist-scrape","position":0},{"hierarchy":{"lvl1":"Scraping craigslist"},"content":"","type":"content","url":"/blog/2015/2015-08-30-craigslist-scrape","position":1},{"hierarchy":{"lvl1":"Scraping craigslist","lvl2":"Overview"},"type":"lvl2","url":"/blog/2015/2015-08-30-craigslist-scrape#overview","position":2},{"hierarchy":{"lvl1":"Scraping craigslist","lvl2":"Overview"},"content":"In this notebook, I‚Äôll show you how to make a simple query on Craigslist using some nifty python modules. You can take advantage of all the structure data that exists on webpages to collect interesting datasets.\n\nimport pandas as pd\nfrom bs4 import BeautifulSoup as bs4\n%pylab inline\n\nFirst we need to figure out how to submit a query to Craigslist. As with many websites, one way you can do this is simply by constructing the proper URL and sending it to Craigslist. Here‚Äôs a sample URL that is returned after manually typing in a search to Craigslist:\n\nhttp://sfbay.craigslist.org/search/eby/apa?bedrooms=1&pets_cat=1&pets_dog=1&is_furnished=1\n\nThis is actually two separate things. The first tells craigslist what kind of thing we‚Äôre searching for:\n\nhttp://sfbay.craigslist.org/search/eby/apa says we‚Äôre searching in the sfbay area (sfbay) for apartments (apa) in the east bay (eby).\n\nThe second part contains the parameters that we pass to the search:\n\n?bedrooms=1&pets_cat=1&pets_dog=1&is_furnished=1 says we want 1+ bedrooms, cats allowed, dogs allowed, and furnished apartments. You can manually change these fields in order to create new queries.","type":"content","url":"/blog/2015/2015-08-30-craigslist-scrape#overview","position":3},{"hierarchy":{"lvl1":"Scraping craigslist","lvl2":"Getting a single posting"},"type":"lvl2","url":"/blog/2015/2015-08-30-craigslist-scrape#getting-a-single-posting","position":4},{"hierarchy":{"lvl1":"Scraping craigslist","lvl2":"Getting a single posting"},"content":"So, we‚Äôll use this knowledge to send some custom URLs to Craigslist. We‚Äôll do this using the requests python module, which is really useful for querying websites.\n\nimport requests\n\nIn internet lingo, we‚Äôre posting a get requests to the website, which simply says that we‚Äôd like to get some information from the Craigslist website.  With requests, we can easily create a dictionary that specifies parameters in the URL:\n\nurl_base = 'http://sfbay.craigslist.org/search/eby/apa'\nparams = dict(bedrooms=1, is_furnished=1)\nrsp = requests.get(url_base, params=params)\n\n# Note that requests automatically created the right URL:\nprint(rsp.url)\n\n# We can access the content of the response that Craigslist sent back here:\nprint(rsp.text[:500])\n\nWow, that‚Äôs a lot of code. Remember, websites serve HTML documents, and usually your browser will automatically render this into a nice webpage for you. Since we‚Äôre doing this with python, we get back the raw text. This is really useful, but how can we possibly parse it all?\n\nFor this, we‚Äôll turn to another great package, BeautifulSoup:\n\n# BS4 can quickly parse our text, make sure to tell it that you're giving html\nhtml = bs4(rsp.text, 'html.parser')\n\n# BS makes it easy to look through a document\nprint(html.prettify()[:1000])\n\nBeautiful soup lets us quickly search through an HTML document. We can pull out whatever information we want.\n\nScanning through this text, we see a common structure repeated <p class=\"row\">. This seems to be the container that has information for a single apartment.\n\nIn BeautifulSoup, we can quickly get all instances of this container:\n\n# find_all will pull entries that fit your search criteria.\n# Note that we have to use brackets to define the `attrs` dictionary\n# Because \"class\" is a special word in python, so we need to give a string.\napts = html.find_all('p', attrs={'class': 'row'})\nprint(len(apts))\n\nNow let‚Äôs look inside the values of a single apartment listing:\n\n# We can see that there's a consistent structure to a listing.\n# There is a 'time', a 'name', a 'housing' field with size/n_brs, etc.\nthis_appt = apts[15]\nprint(this_appt.prettify())\n\n# So now we'll pull out a couple of things we might be interested in:\n# It looks like \"housing\" contains size information. We'll pull that.\n# Note that `findAll` returns a list, since there's only one entry in\n# this HTML, we'll just pull the first item.\nsize = this_appt.findAll(attrs={'class': 'housing'})[0].text\nprint(size)\n\nWe can query split this into n_bedrooms and the size. However, note that sometimes one of these features might be missing. So we‚Äôll use an if statement to try and capture this variability:\n\ndef find_size_and_brs(size):\n    split = size.strip('/- ').split(' - ')\n    if len(split) == 2:\n        n_brs = split[0].replace('br', '')\n        this_size = split[1].replace('ft2', '')\n    elif 'br' in split[0]:\n        # It's the n_bedrooms\n        n_brs = split[0].replace('br', '')\n        this_size = np.nan\n    elif 'ft2' in split[0]:\n        # It's the size\n        this_size = split[0].replace('ft2', '')\n        n_brs = np.nan\n    return float(this_size), float(n_brs)\nthis_size, n_brs = find_size_and_brs(size)\n\n# Now we'll also pull a few other things:\nthis_time = this_appt.find('time')['datetime']\nthis_time = pd.to_datetime(this_time)\nthis_price = float(this_appt.find('span', {'class': 'price'}).text.strip('$'))\nthis_title = this_appt.find('a', attrs={'class': 'hdrlnk'}).text\n\n# Now we've got the n_bedrooms, size, price, and time of listing\nprint('\\n'.join([str(i) for i in [this_size, n_brs, this_time, this_price, this_title]]))\n\n","type":"content","url":"/blog/2015/2015-08-30-craigslist-scrape#getting-a-single-posting","position":5},{"hierarchy":{"lvl1":"Scraping craigslist","lvl2":"Querying lots of postings"},"type":"lvl2","url":"/blog/2015/2015-08-30-craigslist-scrape#querying-lots-of-postings","position":6},{"hierarchy":{"lvl1":"Scraping craigslist","lvl2":"Querying lots of postings"},"content":"Cool - so now we‚Äôve got some useful information about one listing. Now let‚Äôs loop through many listings across several locations.\n\nIt looks like there is a ‚Äúcity code‚Äù that distinguishes where you‚Äôre searching. Here is a not up to date list: \n\nlink\n\nWithin the Bay Area, there are also a lot of sub-regional locations, which we‚Äôll define here, then loop through them all.\n\nNote that the s parameter tells Craiglist where to start in terms of the number of results given back. E.g., if s==100, then it starts at the 100th entry.\n\nloc_prefixes = ['eby', 'nby', 'sfc', 'sby', 'scz']\n\nWe‚Äôll define a few helper functions to handle edge cases and make sure that we don‚Äôt get any errors.\n\ndef find_prices(results):\n    prices = []\n    for rw in results:\n        price = rw.find('span', attrs={'class': 'result-price'})\n        if price is not None:\n            price = float(price.text.strip('$'))\n        else:\n            price = np.nan\n        prices.append(price)\n    return prices\n\ndef find_times(results):\n    times = []\n    for rw in apts:\n        if time is not None:\n            time = time['datetime']\n            time = pd.to_datetime(time)\n        else:\n            time = np.nan\n        times.append(time)\n    return times\n\nNow we‚Äôre ready to go. We‚Äôll loop through all of our locations, and pull a number of entries for each one. We‚Äôll use a pandas dataframe to store everything, because this will be useful for future analysis.\n\nNote - Craigslist won‚Äôt take kindly to you querying their server a bunch of times at once. Make sure not to pull too much data too quickly. Another option is to add a delay to each loop iteration. Otherwise your IP might get banned.\n\nprint(txt.prettify())\n\ndef find_size_and_brs(size):\n    split = size.strip().split('\\n')\n    split = [ii.strip().strip(' -') for ii in split]\n    if len(split) == 2:\n        n_brs = split[0].replace('br', '')\n        this_size = split[1].replace('ft2', '')\n    elif 'br' in split[0]:\n        # It's the n_bedrooms\n        n_brs = split[0].replace('br', '')\n        this_size = np.nan\n    elif 'ft2' in split[0]:\n        # It's the size\n        this_size = split[0].replace('ft2', '')\n        n_brs = np.nan\n    return float(this_size), float(n_brs)\n\n# Now loop through all of this and store the results\nresults = []  # We'll store the data here\n# Careful with this...too many queries == your IP gets banned temporarily\nsearch_indices = np.arange(0, 500, 100)\nloc_prefixes = ['eby']\nfor loc in loc_prefixes:\n    print loc\n    for i in search_indices:\n        url = 'http://sfbay.craigslist.org/search/{0}/apa'.format(loc)\n        resp = requests.get(url, params={'bedrooms': 1, 's': i})\n        txt = bs4(resp.text, 'html.parser')\n        apts = txt.findAll(attrs={'class': \"result-info\"})\n\n        # Find the size of all entries\n        size_text = [rw.findAll(attrs={'class': 'housing'})[0].text\n                     for rw in apts]\n        sizes_brs = [find_size_and_brs(stxt) for stxt in size_text]\n        sizes, n_brs = zip(*sizes_brs)  # This unzips into 2 vectors\n\n        # Find the title and link\n        title = [rw.find('a', attrs={'class': 'hdrlnk'}).text\n                      for rw in apts]\n        links = [rw.find('a', attrs={'class': 'hdrlnk'})['href']\n                 for rw in apts]\n\n        # Find the time\n        time = [pd.to_datetime(rw.find('time')['datetime']) for rw in apts]\n        price = find_prices(apts)\n\n        # We'll create a dataframe to store all the data\n        data = np.array([time, price, sizes, n_brs, title, links])\n        col_names = ['time', 'price', 'size', 'brs', 'title', 'link']\n        df = pd.DataFrame(data.T, columns=col_names)\n        df = df.set_index('time')\n\n        # Add the location variable to all entries\n        df['loc'] = loc\n        results.append(df)\n        \n# Finally, concatenate all the results\nresults = pd.concat(results, axis=0)\n\ndef seconds_to_days(seconds):\n    return seconds / 60. / 60. / 24.\n\n# We'll make sure that the right columns are represented numerically:\nresults[['price', 'size', 'brs']] = results[['price', 'size', 'brs']].convert_objects(convert_numeric=True)\nresults.index.name = 'time'\n\n# Add the age of each result\nnow = pd.datetime.utcnow()\nresults['age'] = [1. / seconds_to_days((now - ii).total_seconds())\n                  for ii in results.index]\n\n# And there you have it:\nresults.head()\n\nax = results.hist('price', bins=np.arange(0, 10000, 100))[0, 0]\nax.set_title('Mother of god.', fontsize=20)\nax.set_xlabel('Price', fontsize=18)\nax.set_ylabel('Count', fontsize=18)\n\ntarget_price = 2200.\ntarget_size = 1400.\nhighlight = pd.DataFrame([[target_price, target_size, 2, 'eby', 'Mine', 'None', 1, results['age'].max()]],\n                         columns=['price', 'size', 'brs', 'loc', 'title', 'link', 'mine', 'age'])\nresults['mine'] = 0\nresults = results.append(highlight)\n\nimport altair\ngraph = altair.Chart(results)\ngraph.mark_circle(size=200).encode(x='size', y='price',\n                                   color='mine:N')\n\nsmin, smax = (1300, 1500)\nn_br = 2\n# subset = results.query('size > @smin and size < @smax')\nfig, ax = plt.subplots()\nresults.query('brs < 4').groupby('brs').hist('price', bins=np.arange(0, 5000, 200), ax=ax)\nax.axvline(target_price, c='r', ls='--')\n\n\n# Finally, we can save this data to a CSV to play around with it later.\n# We'll have to remove some annoying characters first:\nimport string\nuse_chars = string.ascii_letters +\\\n    ''.join([str(i) for i in range(10)]) +\\\n    ' /\\.'\nresults['title'] = results['title'].apply(\n    lambda a: ''.join([i for i in a if i in use_chars]))\n\nresults.to_csv('../data/craigslist_results.csv')\n\n","type":"content","url":"/blog/2015/2015-08-30-craigslist-scrape#querying-lots-of-postings","position":7},{"hierarchy":{"lvl1":"Scraping craigslist","lvl2":"RECAP"},"type":"lvl2","url":"/blog/2015/2015-08-30-craigslist-scrape#recap","position":8},{"hierarchy":{"lvl1":"Scraping craigslist","lvl2":"RECAP"},"content":"To sum up what we just did:\n\nWe defined the ability to query a website using a custom URL. This is usually the same in structure for website, but the parameter names will be different.\n\nWe sent a get request to Craigslist using the requests module of python.\n\nWe parsed the response using BeautifulSoup4.\n\nWe then looped through a bunch of apartment listings, pulled some relevant data, and combined it all into a cleaned and usable dataframe with pandas.\n\nNext up I‚Äôll take a look at the data, and see if there‚Äôs anything interesting to make of it.\n\n","type":"content","url":"/blog/2015/2015-08-30-craigslist-scrape#recap","position":9},{"hierarchy":{"lvl1":"Scraping craigslist","lvl2":"Bonus - auto-emailing yourself w/ notifications"},"type":"lvl2","url":"/blog/2015/2015-08-30-craigslist-scrape#bonus-auto-emailing-yourself-w-notifications","position":10},{"hierarchy":{"lvl1":"Scraping craigslist","lvl2":"Bonus - auto-emailing yourself w/ notifications"},"content":"A few people have asked me about using this kind of process to make a bot that scrapes craigslist periodically. This is actually quite simple, as it basically involves pulling the top listings from craigslist, checking this against an ‚Äúold‚Äù list, and detecting if there‚Äôs anything new that has popped up since the last time you checked.\n\nHere‚Äôs a simple script that will get the job done. Once again, don‚Äôt pull too much data at once, and don‚Äôt query Craigslist too frequently, or you‚Äôre gonna get banned.\n\n# We'll use the gmail module (there really is a module for everything in python)\nimport gmail\nimport time\n\ngm = gmail.GMail('my_username', 'my_password')\ngm.connect()\n\n# Define our URL and a query we want to post\nbase_url = 'http://sfbay.craigslist.org/'\nurl = base_url + 'search/eby/apa?nh=48&anh=49&nh=112&nh=58&nh=61&nh=62&nh=66&max_price=2200&bedrooms=1'\n\n# This will remove weird characters that people put in titles like ****!***!!!\nuse_chars = string.ascii_letters + ''.join([str(i) for i in range(10)]) + ' '\n\nlink_list = []  # We'll store the data here\nlink_list_send = []  # This is a list of links to be sent\nsend_list = []  # This is what will actually be sent in the email\n\n# Careful with this...too many queries == your IP gets banned temporarily\nwhile True:\n    resp = requests.get(url)\n    txt = bs4(resp.text, 'html.parser')\n    apts = txt.findAll(attrs={'class': \"row\"})\n    \n    # We're just going to pull the title and link\n    for apt in apts:\n        title = apt.find_all('a', attrs={'class': 'hdrlnk'})[0]\n        name = ''.join([i for i in title.text if i in use_chars])\n        link = title.attrs['href']\n        if link not in link_list and link not in link_list_send:\n            print('Found new listing')\n            link_list_send.append(link)\n            send_list.append(name + '  -  ' + base_url+link)\n            \n    # Flush the cache if we've found new entries\n    if len(link_list_send) > 0:\n        print('Sending mail!')\n        msg = '\\n'.join(send_list)\n        m = email.message.Message()\n        m.set_payload(msg)\n        gm.send(m, ['recipient_email@mydomain.com'])\n        link_list += link_list_send\n        link_list_send = []\n        send_list = []\n    \n    # Sleep a bit so CL doesn't ban us\n    sleep_amt = np.random.randint(60, 120)\n    time.sleep(sleep_amt)\n\nAnd there you have it - your own little bot to keep you on the top of the rental market.","type":"content","url":"/blog/2015/2015-08-30-craigslist-scrape#bonus-auto-emailing-yourself-w-notifications","position":11},{"hierarchy":{"lvl1":"Craigslist data analysis"},"type":"lvl1","url":"/blog/2015/2015-09-27-craigslist-data-analysis","position":0},{"hierarchy":{"lvl1":"Craigslist data analysis"},"content":"","type":"content","url":"/blog/2015/2015-09-27-craigslist-data-analysis","position":1},{"hierarchy":{"lvl1":"Craigslist data analysis","lvl2":"Using Craigslist to compare prices in the Bay Area"},"type":"lvl2","url":"/blog/2015/2015-09-27-craigslist-data-analysis#using-craigslist-to-compare-prices-in-the-bay-area","position":2},{"hierarchy":{"lvl1":"Craigslist data analysis","lvl2":"Using Craigslist to compare prices in the Bay Area"},"content":"In the \n\nlast post I showed how to use a simple python bot to scrape data from Criagslist. This is a quick follow-up to take a peek at the data.\n\nNote - data that you scrape from Craigslist is pretty limited. They tend to clear out old posts, and you can only scrape from recent posts anyway to avoid them blocking you.\n\nNow that we‚Äôve got some craigslist data, what questions can we ask? Well, a good start would be to see where we want (or don‚Äôt want) to rent our house. Let‚Äôs compare the housing market in a few different regions of the Bay Area.\n\n# Seaborn can help create some pretty plots\nimport seaborn as sns\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport matplotlib as mpl\nimport numpy as np\n%matplotlib inline\nsns.set_palette('colorblind')\nsns.set_style('white')\n\n# First we'll load the data we pulled from before\nresults = pd.read_csv('../data/craigslist_results.csv')\n\n","type":"content","url":"/blog/2015/2015-09-27-craigslist-data-analysis#using-craigslist-to-compare-prices-in-the-bay-area","position":3},{"hierarchy":{"lvl1":"Craigslist data analysis","lvl2":"Price distributions"},"type":"lvl2","url":"/blog/2015/2015-09-27-craigslist-data-analysis#price-distributions","position":4},{"hierarchy":{"lvl1":"Craigslist data analysis","lvl2":"Price distributions"},"content":"As a start, let‚Äôs take a look at the distribution of home prices to get an idea for what we‚Äôre dealing with:\n\nf, ax = plt.subplots(figsize=(10, 5))\nsns.distplot(results['price'].dropna())\n\nThat‚Äôs not super useful - it looks like we have a highly skewed distribution with a few instances way out to the right. We‚Äôll convert to a log scale from here on to make it easier to comprehend:\n\nf, ax = plt.subplots(figsize=(10, 5))\nresults['logprice'] = results['price'].apply(np.log10)\nsns.distplot(results['logprice'].dropna())\nax.set(title=\"Log plots are nicer for skewed data\")\n\n# Don't forget the log mappings:\nprint(['10**{0} = {1}'.format(i, 10**i) for i in ax.get_xlim()])\n\nHowever, what we really want is to look at the breakdown of prices for a few locations in the bay area. Luckily Craigslist stores this in the URL of our search, so we can easily split this up with a pandas groupby:\n\nf, ax_hist = plt.subplots(figsize=(10, 5))\nfor loc, vals in results.groupby('loc'):\n    sns.distplot(vals['logprice'].dropna(), label=loc, ax=ax_hist)\n    ax_hist.legend()\n    ax_hist.set(title='San Francisco is too damn expensive')\n\nsummary = results.groupby('loc').describe()['logprice'].unstack('loc')\nfor loc, vals in summary.iteritems():\n    print('{0}: {1}+/-{2}'.format(loc, vals['mean'], vals['std']/vals.shape[0]))\n    \nprint('Differences on the order of: $' + str(10**3.65 - 10**3.4))\n\nThat‚Äôs a bit unsurprising - San Francisco is significantly more expensive than any other region in the area. Note that this is a log scale, so small differences at this scale == large differences in the raw values.\n\nHowever, it looks like the shapes of these prices are different as well. If any of these distributions aren‚Äôt symmetric around the center, then describing it with the mean +/- standard deviation isn‚Äôt so great.\n\nPerhaps a better way to get an idea for what kind of deal we‚Äôre getting is to directly calculate price per square foot. Let‚Äôs see how this scales as the houses go up.\n\n# We'll quickly create a new variable to use here\nresults['ppsf'] = results['price'] / results['size']\n\n# These switches will turn on/off the KDE vs. histogram\nkws_dist = dict(kde=True, hist=False)\nn_loc = results['loc'].unique().shape[0]\nf, (ax_ppsf, ax_sze) = plt.subplots(1, 2, figsize=(10, 5))\nfor loc, vals in results.groupby('loc'):\n    sns.distplot(vals['ppsf'].dropna(), ax=ax_ppsf,\n                 bins=np.arange(0, 10, .5), label=loc, **kws_dist)\n    sns.distplot(vals['size'].dropna(), ax=ax_sze,\n                 bins=np.arange(0, 4000, 100), **kws_dist)\nax_ppsf.set(xlim=[0, 10], title='Price per square foot')\nax_sze.set(title='Size')\n\nSo it looks like size-wise, there aren‚Äôt many differences here. However, with price per square foot, you‚Äôll be paying a lot more for the same space in SF.\n\nFinally, let‚Äôs take a look at how the price scales with the size. For this, we‚Äôll use a regplot to fit a line to each distribution.\n\n# Split up by location, then plot summaries of the data for each\nn_loc = results['loc'].unique().shape[0]\nf, axs = plt.subplots(n_loc, 3, figsize=(15, 5*n_loc))\nfor (loc, vals), (axr) in zip(results.groupby('loc'), axs):\n    sns.regplot('size', 'ppsf', data=vals, order=1, ax=axr[0])\n    sns.distplot(vals['ppsf'].dropna(), kde=True, ax=axr[1],\n                 bins=np.arange(0, 10, .5))\n    sns.distplot(vals['size'].dropna(), kde=True, ax=axr[2],\n                 bins=np.arange(0, 4000, 100))\n    axr[0].set_title('Location: {0}'.format(loc))\n\n_ = plt.setp(axs[:, 0], xlim=[0, 4000], ylim=[0, 10])\n_ = plt.setp(axs[:, 1], xlim=[0, 10], ylim=[0, 1])\n_ = plt.setp(axs[:, 2], xlim=[0, 4000], ylim=[0, .002])\n\nAnd now on top of one another\n\nf, ax = plt.subplots()\nlocs = [res[0] for res in results.groupby('loc')]\nfor loc, vals in results.groupby('loc'):\n    sns.regplot('size', 'ppsf', data=vals, order=1, ax=ax,\n                scatter=True, label=loc, scatter_kws={'alpha':.3})\n\n# If we want to turn off the scatterplot\nscats = [isct for isct in ax.collections\n         if isinstance(isct, mpl.collections.PathCollection)]\n# plt.setp(scats, visible=False)\n\nax.legend(locs)\nax.set_xlim([0, 4000])\nax.set_ylim([0, 10])\n\nBasically, lines that go down more steeply mean you get a better deal the bigger the place is.\n\nFor instance, if you‚Äôre in the southbay you might be paying $6/sqf for a 600 sq. ft. place, but $1/sqf for a 2000 sq. ft. place. On the other hand, San Francisco is pretty consistent, with a relatively flat line. This means that you‚Äôll be paying pretty much the same per square foot regardless of how big your place is. In fact, all of the other regions seem to follow the same trend - so if you‚Äôre looking for more efficient big-place finds, go with the South Bay.\n\nAlso note that this gives us information about the uncertainty in these estimates. The error bars are so wide for San Francisco because we don‚Äôt have many data points at high values (because there aren‚Äôt that many places >2000 square feet in SF). It‚Äôs anyone‚Äôs guess as to what this would cost.\n\n","type":"content","url":"/blog/2015/2015-09-27-craigslist-data-analysis#price-distributions","position":5},{"hierarchy":{"lvl1":"Craigslist data analysis","lvl2":"Text analysis"},"type":"lvl2","url":"/blog/2015/2015-09-27-craigslist-data-analysis#text-analysis","position":6},{"hierarchy":{"lvl1":"Craigslist data analysis","lvl2":"Text analysis"},"content":"Finally, we can also learn a bit from the text in the post titles. We could probably get better information by using the post text itself, but this would require some extra legwork looking up the URL of each entry and pulling the body of text from this. We‚Äôll stick with titles for now.\n\nTo do this, we‚Äôll use some text analysis tools in scikit-learn. This is good enough for our purposes, though if we wanted to do something fancier we could use something like gensim, word2vec, or nltk. (we‚Äôd also probably need a lot more data).\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.svm import LinearSVC\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\nimport string\n\nFirst we‚Äôll do some quick data cleaning - we‚Äôll only keep datapoints with a title, and then define some characters to remove so that the definition of ‚Äúa word‚Äù makes more sense.\n\nword_data = results.dropna(subset=['title'])\n\n# Remove special characters\nrem = string.digits + '/\\-+.'\nrem_chars = lambda a: ''.join([i for i in a if i not in rem])\nword_data['title'] = word_data['title'].apply(rem_chars)\n\nNext, we‚Äôll remove words that are too specific (from a geographical standpoint) to the regions we‚Äôre using. Otherwise you‚Äôll just get a bunch of clusters with streetnames etc. predicting the Bay Area region.\n\nloc_words = {'eby': ['antioch', 'berkeley', 'dublin', 'fremont', 'rockridge',\n                     'livermore', 'mercer', 'ramon'],\n             'nby': ['sausalito', 'marin', 'larkspur', 'novato', 'petaluma', 'bennett', \n                     'tiburon', 'sonoma', 'anselmo', 'healdsburg', 'rafael'],\n             'sby': ['campbell', 'clara', 'cupertino', 'jose'],\n             'scz': ['aptos', 'capitola', 'cruz', 'felton', 'scotts',\n                     'seabright', 'soquel', 'westside', 'ucsc'],\n             'sfc': ['miraloma', 'soma', 'usf', 'ashbury', 'marina',\n                     'mission', 'noe']}\n\n# We can append these to sklearn's collection of english \"stop\" words\nrand_words = ['th', 'xs', 'x', 'bd', 'ok', 'bdr']\nstop_words = [i for j in loc_words.values() for i in j] + rand_words\nstop_words = ENGLISH_STOP_WORDS.union(stop_words)\n\nFinally, we will vectorize this data so that it can be used with sklearn algorithms. This takes a list of ‚Äúbags‚Äù of words, and turns it into a list of vectors, where the length of each vector is the total number of words we‚Äôve got. Each position of the vector corresponds to 1 word. It will be ‚Äú1‚Äù if that word is present in the current item, and 0 otherwise:\n\nvec = CountVectorizer(max_df=.6, stop_words=stop_words)\nvec_tar = LabelEncoder()\n\ncounts = vec.fit_transform(word_data['title'])\ntargets = vec_tar.fit_transform(word_data['loc'])\nplt.plot(counts[:3].toarray().T)\nplt.ylim([-1, 2])\nplt.title('Each row is a post, with 1s representing presence of a word in that post')\n\nLet‚Äôs do a quick description of the most common words in each region. We can use our vectorized vocabulary and see which words were most common.\n\ntop_words = {}\nfor itrg in np.unique(targets):\n    loc = vec_tar.classes_[itrg]\n    # Pull only the data points assigned to the current loction\n    icounts = counts[targets == itrg, :].sum(0).squeeze()\n    \n    # Which counts had at least five occurrences\n    msk_top_words = icounts > 5\n    \n    # The inverse transform turns the vectors back into actual words\n    top_words[loc] = vec.inverse_transform(msk_top_words)[0]\n\nThen, we‚Äôll print the words that are unique to each area by filtering out ones that are common across locations:\n\nunique_words = {}\nfor loc, words in top_words.iteritems():\n    others = top_words.copy()\n    others.pop(loc)\n    unique_words[loc] = [wrd for wrd in top_words[loc]\n                         if wrd not in np.hstack(others.values())]\nfor loc, words in unique_words.iteritems():\n    print('{0}: {1}\\n\\n---\\n'.format(loc, words))\n\nApparently people in the North Bay like appliances, people in Santa Cruz like the beach, people in the East Bay need the Bart, and people in San Francisco have victorians...who knew.\n\nJust for fun we‚Äôll also do a quick classification algorithm to see if some machine learning can find structure in these words that separates one location from another:\n\nmod = LinearSVC(C=.1)\ncv = StratifiedShuffleSplit(targets, n_iter=10, test_size=.2)\n\ncoefs = []\nfor tr, tt in cv:\n    mod.fit(counts[tr], targets[tr])\n    coefs.append(mod.coef_)\n    print(mod.score(counts[tt], targets[tt]))\ncoefs = np.array(coefs).mean(0)\n\nDoesn‚Äôt look like it (those are horrible generalization scores), but we‚Äôll look at what coefficients it considered important anyway:\n\nfor loc, icoef in zip(vec_tar.classes_, coefs):\n    cut = np.percentile(icoef, 99)\n    important = icoef > cut\n    print('{0}: {1}'.format(loc, vec.inverse_transform(important)))\n\nYou may note that these are quite similar to the words that were unique to each location as noted above - such is the power of machine learning :)\n\n","type":"content","url":"/blog/2015/2015-09-27-craigslist-data-analysis#text-analysis","position":7},{"hierarchy":{"lvl1":"Craigslist data analysis","lvl3":"So what have we learned?","lvl2":"Text analysis"},"type":"lvl3","url":"/blog/2015/2015-09-27-craigslist-data-analysis#so-what-have-we-learned","position":8},{"hierarchy":{"lvl1":"Craigslist data analysis","lvl3":"So what have we learned?","lvl2":"Text analysis"},"content":"Well, you might say that we‚Äôve merely quantified what everybody already knows: San Francisco is expensive, really expensive. If you‚Äôre looking for a place in the Bay Area, you can expect to shell out a lot more for the same square footage.\n\nHowever, what‚Äôs also interesting is that apartments in the Bay Area don‚Äôt seem to obey the same rules that other regions do - they don‚Äôt necessarily become more economically efficient as the place gets bigger. This is in stark contrast to the south bay, where places are pretty expensive in general, but in ways that you‚Äôd expect for an apartment.\n\nFinally, there are probably lots of other cool things that you could do with these datasets, especially if you wanted to break things down by neighborhood and collect more data.","type":"content","url":"/blog/2015/2015-09-27-craigslist-data-analysis#so-what-have-we-learned","position":9},{"hierarchy":{"lvl1":"NIH grant analysis"},"type":"lvl1","url":"/blog/2015/2015-10-29-nih-grant-analysis","position":0},{"hierarchy":{"lvl1":"NIH grant analysis"},"content":"","type":"content","url":"/blog/2015/2015-10-29-nih-grant-analysis","position":1},{"hierarchy":{"lvl1":"NIH grant analysis","lvl2":"NIH Fellowship Success Rates"},"type":"lvl2","url":"/blog/2015/2015-10-29-nih-grant-analysis#nih-fellowship-success-rates","position":2},{"hierarchy":{"lvl1":"NIH grant analysis","lvl2":"NIH Fellowship Success Rates"},"content":"As I‚Äôm entering the final years of graduate school, I‚Äôve been applying for a few typical ‚Äúpre-doc‚Äù fellowships. One of these is the NRSA, which is notorious for requiring you to wade through forests of beaurocratic documents (seriously, their ‚Äúguidelines‚Äù for writing an NRSA are over 100 pages!). Doing so ends up taking a LOT of time.\n\nThis got me wondering what kind of success rates these grants have in the first place. For those who haven‚Äôt gone through the process before, it‚Äôs a bit opaque:","type":"content","url":"/blog/2015/2015-10-29-nih-grant-analysis#nih-fellowship-success-rates","position":3},{"hierarchy":{"lvl1":"NIH grant analysis","lvl3":"How the NRSA works","lvl2":"NIH Fellowship Success Rates"},"type":"lvl3","url":"/blog/2015/2015-10-29-nih-grant-analysis#how-the-nrsa-works","position":4},{"hierarchy":{"lvl1":"NIH grant analysis","lvl3":"How the NRSA works","lvl2":"NIH Fellowship Success Rates"},"content":"Basically, each NRSA grant is reviewed by a panel of people. They individually review your proposal (which is upwards of 20 pages), and assign it a score in one of several categories. The group then meets, and they give your proposal an ‚Äúimpact score‚Äù. This score is then compared with the distribution of scores for all the other proposals in your category, and then a cutoff is made based on the percentiles that they will fund.\n\nEach year, they change the cutoff, and thus the number of NRSAs, that will be funded. Importantly, this also differs significantly by the NIH branch that oversees your application. Some have much higher success rates than others. For those who are curious, I‚Äôm putting a list of the NIH acronyms and their respective centers at the end.","type":"content","url":"/blog/2015/2015-10-29-nih-grant-analysis#how-the-nrsa-works","position":5},{"hierarchy":{"lvl1":"NIH grant analysis","lvl3":"Some data!","lvl2":"NIH Fellowship Success Rates"},"type":"lvl3","url":"/blog/2015/2015-10-29-nih-grant-analysis#some-data","position":6},{"hierarchy":{"lvl1":"NIH grant analysis","lvl3":"Some data!","lvl2":"NIH Fellowship Success Rates"},"content":"I did a bit of digging and found a dataset for NRSA success rates over the last few years. It‚Äôs broken down by grant type, as well as by NIH branch. The results are interesting so I thought I‚Äôd put them out there.\n\nNote - this dataset and a few others can be found on the NIH website \n\nhere. I encourage you to check it out! Thank god for open data (even if it‚Äôs really hard to find).\n\n# First we'll import a few helpful tools for cleaning and plotting\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\n%matplotlib inline\n\n# Load the data - note that each row is a grant type/year/agency\ndata = pd.read_excel('./data/nih_nrsa_success_rates.xls', header=2)\ndata = data.convert_objects(convert_numeric=True)\ndata.head()\n\nFirst, we‚Äôll do some cleaning to remove annoying things like special characters and spaces. We‚Äôll focus on the ‚Äúsuccess rate2‚Äù variable, which describes the % of proposals that were funded that year.\n\n# Make the columns easier to play with\ncol_mapping = {col: col.strip(' ').replace(' ', '_').replace('/', '_').\\\n               strip().\\\n               lower() for col in data.columns}\ndata = data.rename(columns=col_mapping)\n\n# Remove missing values for success rate\ndata = data.dropna(subset=['success_rate2',\n                           'fiscal_year',\n                           'nih_institute_center'])\n\n# Remove all non-word characters from names\ndata['nih_institute_center'] = data['nih_institute_center'].\\\n    apply(lambda a: re.sub('[^a-zA-Z]', '', a))\n\n# Remove center values that aren't as useful\nremove_centers = [\"Total\", \"ODOther\"]\ndata = data.query('nih_institute_center not in @remove_centers')\n\nFirst, let‚Äôs take a look at some general trends over time\n\nf, axs = plt.subplots(3, 1, figsize=(8, 9))\ncombined = data.groupby('fiscal_year').sum()\ncoltypes = ['number_of_applications_reviewed',\n            'number_of_applications_awarded',\n            'total_funding3']\nfor ax, coltype in zip(axs, coltypes):\n    combined[coltype].plot.bar(ax=ax)\n    ax.set_title(coltype, fontsize=20)\nplt.tight_layout()\n\nEyeballing it, it looks like applications have been slightly increasing per year, while applications awarded have remained relatively flat. Let‚Äôs dig into that a little bit more. Rather than looking at the raw numbers, we‚Äôll focus on the success rate, or the % of applications that were funded.\n\nWe‚Äôll plot the success rates per year, per activity code. I‚Äôll plot a scatter + line for each NIH institute (in color) as well as the mean + a 3rd order polynomial fit for all of them lumped together (in black)\n\nfor grant, gvals in data.groupby('activity_code'):\n    if gvals.shape[0] > 10:\n        mn_grp = gvals.groupby('fiscal_year').mean()\n        lm = sns.lmplot('fiscal_year', 'success_rate2', data=gvals,\n                        hue='nih_institute_center', ci=None,\n                        line_kws={'alpha': .5}, scatter_kws={'alpha': .5},\n                        legend=False)\n        ax = lm.ax\n        ax.figure.set_size_inches(6, 4)\n\n        # If we have enough data, fit a fancier function to all points\n        if gvals.shape[0] > 10:\n            mn_reg = sns.regplot(\n                'fiscal_year', 'success_rate2', data=gvals,\n                scatter=False, line_kws={'linewidth': 5, 'c': 'k'},\n                order=3)\n        ax.set_title('Grant Activity Code: {0}'.format(grant), fontsize=20)\n        ax.set(ylim=[0, 1])\n        _ = plt.setp(mn_reg.collections[-1], color='k', alpha=.1)\n        ax.legend(loc='right', bbox_to_anchor=(1.2, .5),\n          ncol=1, fancybox=True, shadow=True, frameon=False, fontsize=8)\n        plt.tight_layout()\n\nAs you can see - some of these data aren‚Äôt well-modeled by a line in the first place. That‚Äôs because there‚Äôs a lot of missing data in here. However, especially for the more common grants (F31, F32) you can see some interesting (and mostly downward) trends.","type":"content","url":"/blog/2015/2015-10-29-nih-grant-analysis#some-data","position":7},{"hierarchy":{"lvl1":"NIH grant analysis","lvl2":"Digging in to specific grants"},"type":"lvl2","url":"/blog/2015/2015-10-29-nih-grant-analysis#digging-in-to-specific-grants","position":8},{"hierarchy":{"lvl1":"NIH grant analysis","lvl2":"Digging in to specific grants"},"content":"Next, we‚Äôll take a look at some more specific information about a select grant type. I‚Äôll focus on the F31 and F32 because that‚Äôs most related to what I‚Äôm applying for.\n\n# Pull the subset of data we want\ngrant_type = ['F31', 'F32']\nsubset = data.query('activity_code in @grant_type')\n\n# Average out the grant type column\nsubset = subset.groupby(['fiscal_year', 'nih_institute_center']).mean().reset_index()\n\nWe‚Äôll plot the successrate (y-axis) as a function of year (color) for each NIH center (x-axis).\n\nvariable_types = ['success_rate2', 'number_of_applications_reviewed']\nn_yrs = len(subset.fiscal_year.unique())\npal = sns.color_palette('RdBu_r', n_colors=n_yrs)\nfor var in variable_types:\n    f, ax = plt.subplots(figsize=(8, 4))\n    sns.barplot(x='nih_institute_center', y=var,\n                data=subset, hue='fiscal_year', palette=pal)\n    _ = plt.setp(ax.get_xticklabels(), rotation=90, fontsize=15)\n    _ = plt.setp(ax.get_yticklabels(), fontsize=15)\n    ax.legend(loc='right', bbox_to_anchor=(1.15, .5),\n          ncol=1, fancybox=True, shadow=True, frameon=False, fontsize=8)\n    ax.set_title(var, fontsize=20)\n\nIs there anything that we can learn from this? Let‚Äôs look at how the finding changed for each year:\n\nyearly_change = subset.set_index(['nih_institute_center', 'fiscal_year'])['success_rate2'].\\\n    unstack('nih_institute_center')\n\nsns.set_palette(sns.color_palette('rainbow', yearly_change.shape[1]))\nax = yearly_change.plot(figsize=(10, 5))\nax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.05),\n          ncol=7, fancybox=True, shadow=True, frameon=True)\n\nTo make things more comparable, we can normalize each timeseries by its first value, this will give everything the same starting point. We‚Äôll also smooth the curves a little bit to get an idea for general trends. We‚Äôll apply an exponentially-weighted moving average so recent points factor more into the average\n\n# Normalize data and apply a bit of smoothing\nyearly_change_norm = yearly_change / yearly_change.iloc[0]\nax = yearly_change_norm.apply(pd.stats.moments.ewma, span=5).\\\n    plot(figsize=(10, 5))\nax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.05),\n          ncol=7, fancybox=True, shadow=True, frameon=True)\n\nIt seems like some NIH groups have made out better than others. Maybe this simply because the number of total applications each received changed from one year to another.\n\nAs a final analysis, we can look at how the yearly change in applications is correlated with a success rate.\n\nPositive lines mean that more applications tend to have a higher success rate, negative lines mean more apps have a lower success rate. I‚Äôll also color each point by the fiscal year it came from - hotter colors mean more recent years.\n\ngrp_cent = subset.groupby('nih_institute_center')\nn_cl = 4\nn_rw = np.ceil(len(grp_cent) / float(n_cl)).astype(int)\nf, axs = plt.subplots(n_rw, n_cl, figsize=(3*n_cl, 2*n_rw))\nfor (cent, vals), ax in zip(grp_cent, axs.ravel()):\n    yr = vals['fiscal_year'] / 100\n    corr = np.corrcoef(*vals[['number_of_applications_reviewed', 'success_rate2']].values.T)[1, 0]\n    lc = str(np.where(corr > 0, 'g', 'r'))\n    sns.regplot('number_of_applications_reviewed', 'success_rate2',\n                data=vals, ax=ax, scatter_kws={'s': yr*5, 'c': yr, 'cmap': plt.cm.RdBu_r},\n                line_kws={'c': lc}, ci=None)\n    ax.set(ylim=[-.1, .8])\n#     sns.corrplot(vals.set_index(['fiscal_year']), cbar=False, ax=ax)\n    ax.set_title(cent, fontsize=10)\n    ax.annotate('Corr: {0: .02f}'.format(corr), (.1, .9),\n                xycoords='axes fraction', fontsize=10)\nplt.tight_layout()\n_ = plt.setp([ax.get_xticklabels() + ax.get_yticklabels()\n              for ax in axs.ravel()], fontsize=10)\n_ = plt.setp([ax.xaxis.label, ax.yaxis.label], fontsize=10)\n\nSo what did we learn from any of this? Well, it seems like the funding landscape at the NIH is changing, and some groups are making out better than others.\n\nThere are power players who‚Äôve been around for a while, like the NINDS, who haven‚Äôt seen a huge change in their granting behavior. Other groups, such as NIAAA, have seen their success rates decline steadily w/ increased applications. On the other hand, there are some centers that have actually shown both an increase in applications AND an increase in success rates (such as the NHGRI), which may reflect increased interest in those research endeavors from congress etc.\n\nThere‚Äôs a lot more to be read into this data, so I encourage people to check it out for themselves, or suggest things to try out on top of what‚Äôs already here. It‚Äôs been fun learning a bit about the landscape of grants at the NIH.\n\n","type":"content","url":"/blog/2015/2015-10-29-nih-grant-analysis#digging-in-to-specific-grants","position":9},{"hierarchy":{"lvl1":"NIH grant analysis","lvl2":"Appendix"},"type":"lvl2","url":"/blog/2015/2015-10-29-nih-grant-analysis#appendix","position":10},{"hierarchy":{"lvl1":"NIH grant analysis","lvl2":"Appendix"},"content":"","type":"content","url":"/blog/2015/2015-10-29-nih-grant-analysis#appendix","position":11},{"hierarchy":{"lvl1":"NIH grant analysis","lvl3":"NIH acronym lookup","lvl2":"Appendix"},"type":"lvl3","url":"/blog/2015/2015-10-29-nih-grant-analysis#nih-acronym-lookup","position":12},{"hierarchy":{"lvl1":"NIH grant analysis","lvl3":"NIH acronym lookup","lvl2":"Appendix"},"content":"\n\ndata['nih_institute_center'].unique()\nnih_centers = {u'NCCAM': 'National Center for Complementary and Alternative Medicine',\n                 u'NINDS': 'National Institute on Neurological Disorders and Stroke',\n                 u'NIDA': 'National Institute on Drug Abuse',\n                 u'NIDCD': 'National Institute on Deafness and Other Communication Disorders',\n                 u'NIDCR': 'National Institute of Dental and Craniofacial Research',\n                 u'NIEHS': 'National Institute of Environmental Health Sciences',\n                 u'NIMH':  'National Institute of Mental Health',\n                 u'NIAAA': 'National Institute on Alcohol Abuse and Alcoholism',\n                 u'NIA': 'National Institute on Aging',\n                 u'NIAID': 'National Institute of Allergy and Infectious Diseases',\n                 u'NIAMS': 'National Institute of Arthritis and Musculoskeletal and Skin Diseases',\n                 u'NCI': 'National Cancer Institute',\n                 u'NIDDK': 'National Institute of Diabetes and Digestive and Kidney Diseases',\n                 u'NIBIB': 'National Institute of Biomedical Imaging and Bioengineering',\n                 u'NEI': 'National Eye Institute',\n                 u'NIGMS': 'National Institute of General Medical Sciences',\n                 u'NICHD': 'National Institute of Child Health and Human Development',\n                 u'NHGRI': 'National Human Genome Research Institute',\n                 u'NHLBI': 'National Heart, Lung, and Bloog Institute',\n                 u'NINR': 'National Institute of Nursing Research',\n                 u'NCRR': 'National Center for Research Resources',\n                 u'NLM': 'National Library of Medicine'}\nfor ac, cent in nih_centers.iteritems():\n    print('{0}:\\t{1}'.format(ac, cent))","type":"content","url":"/blog/2015/2015-10-29-nih-grant-analysis#nih-acronym-lookup","position":13},{"hierarchy":{"lvl1":"The beauty of computational efficiency"},"type":"lvl1","url":"/blog/2016/2016-07-02-fft-time","position":0},{"hierarchy":{"lvl1":"The beauty of computational efficiency"},"content":"","type":"content","url":"/blog/2016/2016-07-02-fft-time","position":1},{"hierarchy":{"lvl1":"The beauty of computational efficiency","lvl2":"The beauty of computational efficiency and the FFT"},"type":"lvl2","url":"/blog/2016/2016-07-02-fft-time#the-beauty-of-computational-efficiency-and-the-fft","position":2},{"hierarchy":{"lvl1":"The beauty of computational efficiency","lvl2":"The beauty of computational efficiency and the FFT"},"content":"When we discuss ‚Äúcomputational efficiency‚Äù, you often hear people throw around phrases like O(n^2) or O(nlogn). We talk about them in the abstract, and it can be hard to appreciate what these distinctions mean and how important they are. So let‚Äôs take a quick look at what computational efficiency looks like in the context of a very famous algorithm: The Fourier Transform.","type":"content","url":"/blog/2016/2016-07-02-fft-time#the-beauty-of-computational-efficiency-and-the-fft","position":3},{"hierarchy":{"lvl1":"The beauty of computational efficiency","lvl3":"A short primer on the Fourier Transform","lvl2":"The beauty of computational efficiency and the FFT"},"type":"lvl3","url":"/blog/2016/2016-07-02-fft-time#a-short-primer-on-the-fourier-transform","position":4},{"hierarchy":{"lvl1":"The beauty of computational efficiency","lvl3":"A short primer on the Fourier Transform","lvl2":"The beauty of computational efficiency and the FFT"},"content":"Briefly, A Fourier Transform is used for uncovering the spectral information that is present in a signal. AKA, it tells us about oscillatory components in the signal, and has \n\na wide range of uses in communications, signal processing, and even neuroscience analysis.\n\nHere‚Äôs a \n\nQuora post that discusses Fourier Transforms more generally. The first explanation is fantastic and full of history and detail.\n\nThe challenge with the Fourier Transform is that it can take a really long time to compute. If you h ave a signal of length n, then you‚Äôre calculating n Fourier components for each point in the (length n) signal. This means that the number of operations required to calculate a fourier transform is n * n or O(n^2).\n\nFor a quick intuition into what a difference this makes. Consider two signals, one of length 10, and the other of length 100. Since the Fourier Transform is O(n^2), the length 100 signal will take 2 orders of magnitude longer to compute, even though it is only 1 order of magnitude longer in length.\n\nThink this isn‚Äôt a big deal? Let‚Äôs see what happens when the signal gets longer. First off, a very short signal:\n\n# We can use the `time` and the `numpy` module to time how long it takes to do an FFT\nfrom time import time\nimport numpy as np\nimport seaborn as sns\nsns.set_style('white')\n\n# For a signal of length ~1000. Say, 100ms of a 10KHz audio sample.\nsignal = np.random.randn(1009)\nstart = time()\n_ = np.fft.fft(signal)\nstop = time()\nprint('It takes {} seconds to do the FFT'.format(stop-start))\n\nThat‚Äôs not too bad - ~.003 seconds is pretty fast. But here‚Äôs where the O(n^2) thing really gets us...\n\n# We'll test out how long the FFT takes for a few lengths\ntest_primes = [11, 101, 1009, 10009, 100019]\n\n# Let's try a few slightly longer signals\nfor i_length in test_primes:\n    # Calculate the number of factors for this length (we'll see why later)\n    factors = [ii for ii in range(1, 1000) if i_length % ii == 0]\n    # Generate a random signal w/ this length\n    signal = np.random.randn(i_length)\n    # Now time the FFT\n    start = time()\n    _ = np.fft.fft(signal)\n    stop = time()\n    print('With data of length {} ({} factors), it takes {} seconds to do the FFT'.format(\n            i_length, len(factors), stop-start))\n\nWhoah wait a sec, that last one took way longer than everything else. We increased the length of the data by a factor of 10, but the time it took went up by a factor of 100. Not good. That means that if we want to perform an FFT on a signal that was 10 times longer, it‚Äôd take us about 42 minutes. 100 times longer? That‚Äôd take ~3 days.\n\nGiven how important the Fourier Transform is, it‚Äôd be great if we could speed it up somehow.\n\nYou‚Äôll notice that I chose a very particular set of numbers above. Specifically, I chose numbers that were primes (or nearly primes) meaning that they couldn‚Äôt be broken down into products of smaller numbers. That turns out to be really important in allowing the FFT to do its magic. When your signal length is a prime number, then you don‚Äôt gain any speedup from the FFT, as I‚Äôll show below.","type":"content","url":"/blog/2016/2016-07-02-fft-time#a-short-primer-on-the-fourier-transform","position":5},{"hierarchy":{"lvl1":"The beauty of computational efficiency","lvl3":"Enter the Fast Fourier Transform","lvl2":"The beauty of computational efficiency and the FFT"},"type":"lvl3","url":"/blog/2016/2016-07-02-fft-time#enter-the-fast-fourier-transform","position":6},{"hierarchy":{"lvl1":"The beauty of computational efficiency","lvl3":"Enter the Fast Fourier Transform","lvl2":"The beauty of computational efficiency and the FFT"},"content":"The Fast Fourier Transform (FFT) is one of the most important algorithms to come out of the last century because it drastically speeds up the performance of the Fourier Transform. It accomplishes this by breaking down all those n^2 computations into a smaller number of computations, and then putting them together at the end to get the same result. This is called factorizing.\n\nYou can think of factorizing like trying to move a bunch of groceries from your car to your fridge. Say you have 20 items in your car. One way to do this is to individually take each item, pull it from the car, walk to the house, place it in the fridge. It‚Äôd take you 20 trips to do this. Factorizing is like putting your 20 items into 2 grocery bags. Now you only need to make 2 trips to the house - one for each grocery bag. The first approach requires 20 trips to the house, and the second requires 2 trips. You‚Äôve just reduced the number of trips by an order of magnitude!\n\nThe FFT accomplishes its factorization by recognizing that signals of a certain length can be broken down (factorized) into smaller signals. How many smaller signals? Well, that depends on the length of the original signal. If a number has many factors, it means that it can be broken down into a product of many different, smaller, signals.\n\nIn practice, this means that if the input to an FFT has a lot of factors, then you gain a bigger speedup from the FFT algorithm. On one end, a signal with a length == a power of two will have a ton of factors, and yield the greatest speedups. A signal with length == a prime number will be the slowest because it has no factors. Below is a quick simulation to see how much of a difference this makes.\n\nHere are some useful links explaining Fourier Transforms, as well as the FFT:\n\nA Quora post with some great answers on the intuition behind the Fast Fourier Transform.\n\nThe wikipedia entry for FFTs also has some nice links.\n\nA post on the FFT from Jake Vanderplas is also a great explanation of how it works.\n\nimport pandas as pd\nfrom sklearn import linear_model\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n","type":"content","url":"/blog/2016/2016-07-02-fft-time#enter-the-fast-fourier-transform","position":7},{"hierarchy":{"lvl1":"The beauty of computational efficiency","lvl3":"The beautiful efficiency of the FFT","lvl2":"The beauty of computational efficiency and the FFT"},"type":"lvl3","url":"/blog/2016/2016-07-02-fft-time#the-beautiful-efficiency-of-the-fft","position":8},{"hierarchy":{"lvl1":"The beauty of computational efficiency","lvl3":"The beautiful efficiency of the FFT","lvl2":"The beauty of computational efficiency and the FFT"},"content":"To see what the FFT‚Äôs efficiency looks like, we‚Äôll simulate data of different lengths and see how long it takes to compute the FFT at each length. We‚Äôll create a random vector of gaussian noise ranging from length 1 to 10,000. For each vector, we‚Äôll compute the FFT, and time how long it took to compute. I‚Äôve already taken the liberty of doing this (repeated 3 times, and then averaged together). Those times are stored in fft_times.csv.\n\n# Let's read in the data and see how it looks\ndf = pd.read_csv('../data/fft_times.csv', index_col=0)\ndf = df.apply(pd.to_numeric)\ndf = df.mean(0).to_frame('time')\ndf.index = df.index.astype(int)\ndf['length'] = df.index.values\n\n# First off, it's clear that computation time grows nonlinearly with signal length\ndf.plot('length', 'time', figsize=(10, 5))\n\n# However, upon closer inspection, it's clear that there's much variability\nwinsize = 500\ni = 0\nj = i + winsize\ndf.iloc[i:j]['time'].plot(figsize=(10, 5))\n\nAs you can see, there appear to be multiple trends in the data. There seems to be a ‚Äúmost inefficient‚Äù line of growth in the data, as well as a ‚Äúmore efficient‚Äù and a ‚Äúmost efficient‚Äù trend. These correspond to lengths that are particularly good for an FFT.\n\nWe can use regression to find the ‚Äúlinear‚Äù relationship between length of signal and time of FFT. However, if there are any trends in the data that are nonlinear, then they should show up as errors in the regression model. Let‚Äôs see if that happens...\n\n# We'll use a regression model to try and fit how length predicts time\nmod = linear_model.LinearRegression()\nxfit = df['length']\nxfit = np.vstack([xfit, xfit**2, xfit**3, xfit**4]).T\nyfit = df['time'].reshape([-1, 1])\n\n# Now fit to our data, and calculate the error for each datapoint\nmod.fit(xfit, yfit)\ndf['ypred'] = mod.predict(xfit)\ndf['diff'] = df['time'] - df.ypred\n\n# As the length grows, the trends in the data begin to diverge more and more\nax = df.plot('length', 'diff', kind='scatter',\n             style='.', alpha=.5, figsize=(10, 5))\nax.set_ylim([0, .05])\nax.set_title('Error of linear fit for varying signal lengths')\n\nIt looks like there are some clear components of the data that don‚Äôt follow a linear relationship. Moreover, this seems to be systematic. We clearly see several separate traces in the error plot, which means that there are patterns in the data that follow different non-linear trends.\n\nBut we already know that the FFT efficiency will differ depending on the number of factors of the signal‚Äôs length. Let‚Äôs see if that‚Äôs related to the plot above...\n\n# We'll write a helper function that shows how many (<100) factors each length has\nfind_n_factors = lambda n: len([i for i in range(1, min(100, n-1)) if n % i == 0])\n\n# This tells us the number of factors for all lengths we tried\ndf['n_factors'] = df['length'].map(find_n_factors)\n\n# We now have a column that tells us how many factors each iteration had\ndf.tail()\n\nFinally, we can plot time to compue the FFT as a function of the number of factors for that signal length.\n\n# As we can see, the FFT time drops quickly as a function of the number of factors\nax = df.plot('n_factors', 'time', style=['.'], figsize=(10, 5), alpha=.1)\nax.set_xlim([0, 15])\nax.set_ylabel('Time for FFT (s)')\nax.set_title('Time of FFT for varying numbers of factors')\n\nThe fewer factors in the length of the signal, the longer the FFT takes.\n\nFinally, we can show how the length of computation time changes for each group of factors. We‚Äôll plot the signal length along with the time to compute the FFT, this time colored by the number of factors for each point.\n\n# We'll plot two zoom levels to see the detail\nf, axs = plt.subplots(2, 1, figsize=(10, 5))\nvmin, vmax = 1, 18\nfor ax in axs:\n    ax = df.plot.scatter('length', 'time', c='n_factors', lw=0, cmap=plt.cm.get_cmap('RdYlBu', vmax),\n                                           figsize=(10, 10), vmin=vmin, vmax=vmax, ax=ax, alpha=.5)\n    ax.set_xlabel('Length of signal (samples)')\n    ax.set_ylabel('Time to complete FFT (s)')\n    ax.set_title('Time to compute the FFT, colored by n_factors')\n_ = plt.setp(axs, xlim=[0, df['length'].max()])\n_ = plt.setp(axs[0], ylim=[0, .2])\n_ = plt.setp(axs[1], ylim=[0, .005])\nplt.tight_layout()\n\nEach of those colored traces spreading upwards represents a particular strategy that the FFT uses for that number of factors. As you can see, the FFT will take a lot longer (and scales exponentially) with fewer factors (see the red lines). It takes much less time (and scales more linearly) with more factors (see the blue lines).\n\nAnd that right there is the beauty of methods like the FFT. They leverage the structure of mathematics to take a computation that goes on for days, and figure out how to do it in seconds.","type":"content","url":"/blog/2016/2016-07-02-fft-time#the-beautiful-efficiency-of-the-fft","position":9},{"hierarchy":{"lvl1":"Could Brexit have happened by chance?"},"type":"lvl1","url":"/blog/2016/2016-07-08-voting-randomness","position":0},{"hierarchy":{"lvl1":"Could Brexit have happened by chance?"},"content":"As a scientist, watching the Brexit vote was a little bit painful. Though probably not for the reason you‚Äôre thinking. No, it wasn‚Äôt the politics that bothered me, but the method for making such an incredibly important decision. Let me explain...\n\nScientists are a bit obsessed with the concept of error. In the context of collecting data and anaylzing it, this takes the form of our ‚Äúconfidence‚Äù in the results. If all the data say the same thing, then we are usually pretty confident in the overall message. If the data is more complicated than this (and it always is), then we need to define how confident we are in our conclusions.\n\nWhich brings me to this gigantic nation-wide referendum vote. I couldn‚Äôt help but notice that the cutoff for winning / losing the vote was set at 50%. To me, this sounds crazy. If I simply flipped a coin at 50% and tallied the results each time, I‚Äôd get some difference between # heads and # tails that would vary around 50%. In the context of voting, it means that a yes/no split that‚Äôs really close to 50% might actually be too close to call.\n\nIn science, saying that a number is different from some other number requires that the difference falls outside of a certain region of uncertainty. It‚Äôs a way of saying ‚Äúyeah, I know that random fluctuations cause strange looking data sometimes, but my difference is so far from those fluctuations that I think there‚Äôs something real going on.‚Äù\n\nBut this is all a little abstract, so let‚Äôs try it out on some voting data...","type":"content","url":"/blog/2016/2016-07-08-voting-randomness","position":1},{"hierarchy":{"lvl1":"Could Brexit have happened by chance?","lvl2":"Simulating a national referendum vote"},"type":"lvl2","url":"/blog/2016/2016-07-08-voting-randomness#simulating-a-national-referendum-vote","position":2},{"hierarchy":{"lvl1":"Could Brexit have happened by chance?","lvl2":"Simulating a national referendum vote"},"content":"For a referendum vote to go through, it seems reasonable to say \"the people need to vote in numbers that are significantly different from random chance. To ask what ‚Äúrandom chance‚Äù looks like, we can use computer simulations.\n\nWe‚Äôll take on the task of assessing what national votes might look like if they happened completely randomly. Then, we can compare the actual results to our simulation in order to decide if we‚Äôve got a ‚Äúreal‚Äù result or not.\n\n# First, import a bunch of stuff that we'll use later\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom tqdm import tqdm\nimport seaborn as sns\nsns.set_style('white')\n%matplotlib inline\n\n# Initializing the simulation\n# We'll simulate ten thousand votes.\n# On each iteration, generate random votes and calculate the results \nn_votes = 10000\n\n# This is the actual difference in percentage points that we had between the sides\nactual_diff = 51.9 - 48.1\n\nFirst, we‚Äôll create a completely random vote. Each person randomly chooses between the two options: yes and no. Then, we compare the difference in percentage points between the two.\n\n# Create 10,000 citizens and assign each a random vote\ntotal_population = int(1e4)\ndiff = np.zeros(n_votes)\nfor ii in tqdm(range(n_votes)):\n    votes = np.random.rand(total_population)\n    yes = np.sum(votes < .5) / float(total_population)\n    no = np.sum(votes > .5) / float(total_population)\n    \n    # This is the difference in percentage points\n    diff[ii] = (yes - no) * 100\n\ndiff is a list of numbers representing the lead that ‚Äúyes‚Äù has over ‚Äúno‚Äù.  Remember, we‚Äôve randomly chosen these values, so they are the results you‚Äôd get if every single person in the country voted completely randomly.\n\nHow can we summarize the ‚Äúlimits of uncertainty‚Äù that diff defines? We can use percentiles to get an idea for the variability of this number. We‚Äôll take the 1st and the 99th percentile of our simulated differences as a proxy for the limits of what we‚Äôd expect if there were no true opinion in the population\n\n# Here we calculate 98% confidence interval on the difference\nclo, chi = np.percentile(diff, [1, 99])\n\nNow, we‚Äôll make a plot with 3 things:\n\nThe distribution of all our simulated differences\n\nA vertical black line for each limit of the confidence interval.\n\nA vertical red line representing the actual difference between yes/no that was reported\n\n# Let's look at the distribution of differences when voting is *totally* random\nf, ax = plt.subplots(figsize=(10, 5))\n_ = ax.hist(diff, bins=30)\n_ = ax.axvline(actual_diff, color='r', ls='--')\naxfill = ax.fill_between([clo, chi], *ax.get_ylim(), alpha=.1, color='k')\nax.set_title('Distribution of totally random splits', fontsize=20)\nax.legend([ax.lines[0], axfill], ['Actual Difference', 'Confidence Interval'], fontsize=12)\n_ = plt.setp(ax, xlim=[-15, 15])\n\nFor a vote to be ‚Äúdifferent‚Äù than 50%, it‚Äôd need to be outside our margin of error described by the grey rectangle. In this case, it seems that a totally random vote yields about 2% points of spread around 0, and that the recorded vote difference (~4%) is outside of the margin of error for 50%. So maybe we can conclude that the Brexit vote was significantly different from a random 50/50 vote.\n\nBUT - we also know that people don‚Äôt vote completely randomly. They are influenced by external factors, they talk to one another, they tend to vote similarly to those around them. This is why everybody could predict which districts would vote ‚Äúyes‚Äù and which would vote ‚Äúno‚Äù well before the election.\n\nSo, let‚Äôs build that in to our simulation...","type":"content","url":"/blog/2016/2016-07-08-voting-randomness#simulating-a-national-referendum-vote","position":3},{"hierarchy":{"lvl1":"Could Brexit have happened by chance?","lvl2":"Simulating a not-completely-random population"},"type":"lvl2","url":"/blog/2016/2016-07-08-voting-randomness#simulating-a-not-completely-random-population","position":4},{"hierarchy":{"lvl1":"Could Brexit have happened by chance?","lvl2":"Simulating a not-completely-random population"},"content":"So how exactly do we simulate the fact that people don‚Äôt vote totally randomly? There are a lot of ways to do this, but I‚Äôll take the semi-arbitrary decision to say that we could expect the same pattern of voting to occur within a district. That is - we can simulate random district votes instead of random individual votes. Moreover, we‚Äôll then weight that district‚Äôs percentage split by an amount proportional to that district‚Äôs size. Intuitively it doesn‚Äôt seem like this should make much difference in our simulation (we‚Äôre still totally randomly choosing the yes/no split), but let‚Äôs see what happens...\n\nFirst, I grabbed a list of each UK voting district, along with its size...\n\n# UK Population data pulled from\n# https://en.wikipedia.org/wiki/List_of_English_districts_by_population\npopulations = pd.read_csv('../data/uk_population.csv', header=None, index_col=0)\nheader = ['District', 'Population', 'Type', 'Ceremonial_County', 'Historic_County', 'English_Region']\npopulations.columns = header\n\n# Convert population to numbers\npopulations['Population'] = pd.to_numeric(populations['Population'].str.replace(',', ''))\n\n# Now, turn these populations into percentages\nn_areas = populations.shape[0]\ntotal_population = populations['Population'].sum()\npopulations['percent'] = populations['Population'].astype(float) / total_population\nplt.plot(populations['percent'])\nplt.title('Distribution of (sorted) region sizes within the UK', fontsize=15)\nplt.xlabel('District number')\nplt.ylabel('Population percentage')\n\nNow, we‚Äôll run the simulation. On each iteration, all the districts vote totally randomly. Then, the difference between ‚Äúyes‚Äù and ‚Äúno‚Äù is calculated for each. Finally, these differences are combined in a weighted average, where the weights are proportional to the district sizes. This means that bigger districts have a larger influence on the outcome, mimicking the way that the UK tallies votes.\n\ndiff = np.zeros(n_votes)\nfor ii in tqdm(range(n_votes)):\n    # Define a random split yes vs. no for each area\n    area_splits = np.random.rand(n_areas)\n    area_splits = np.vstack([area_splits, 1 - area_splits])\n    yes, no = area_splits\n    \n    # Now, calculate the difference and average these together, weighted by the area size\n    diffs = yes - no\n    diffs = np.average(diffs, weights=populations['percent'])\n    diff[ii] = diffs * 100\n\n# Here we calculate 99% confidence interval on the difference\nclo, chi = np.percentile(diff, [.5, 99.5])\n\n# Let's look at the distribution of differences\n# This time, voting is randomized for each region\nf, ax = plt.subplots(figsize=(10, 5))\n_ = ax.hist(diff, bins=30)\n_ = ax.axvline(actual_diff, color='r', ls='--')\naxfill = ax.fill_between([clo, chi], *ax.get_ylim(), alpha=.1, color='k')\nax.set_title('Distribution of votes randomized by region', fontsize=20)\nax.legend([ax.lines[0], axfill], ['Actual Difference', 'Confidence Interval'], fontsize=12)\n_ = plt.setp(ax, xlim=[-15, 15])\n\nNow we see a different sort of picture. Randomizing votes by district instead of by individual greatly increased the variability in the outcome. So much so that the ‚Äútrue‚Äù results from the Brexit now fall well within our confidence interval.","type":"content","url":"/blog/2016/2016-07-08-voting-randomness#simulating-a-not-completely-random-population","position":5},{"hierarchy":{"lvl1":"Could Brexit have happened by chance?","lvl2":"Concluding thoughts?"},"type":"lvl2","url":"/blog/2016/2016-07-08-voting-randomness#concluding-thoughts","position":6},{"hierarchy":{"lvl1":"Could Brexit have happened by chance?","lvl2":"Concluding thoughts?"},"content":"So what can we conclude from something like this? The point of this article isn‚Äôt to say that this particular simulation proves anything about the Brexit vote, but it does bring up an important point: we need to account for randomness whenever we aggregate data that we‚Äôve collected.\n\nWhen deciding whether to make a gigantic decision that will affect millions of people, we should be reasonably certain that the people‚Äôs opinion is clear. Choosing a 50/50 split as a cutoff means that we could potentially make such a decision because of random chance. Doesn‚Äôt sound like a great way to conduct national policy to me.\n\nWhat could we do instead? There‚Äôs the hard part. But the short answer is that we could include some idea of random variability in our voting rules. For example, we could require that this kind of ‚Äúshould we deviate from the norm‚Äù decision exceeds the results expected from a totally random vote. Settling on this uncertainty limit is not a simple task, but then again I wouldn‚Äôt want to bet 2 trillion dollars worth of global economy on a coin flip.\n\n","type":"content","url":"/blog/2016/2016-07-08-voting-randomness#concluding-thoughts","position":7},{"hierarchy":{"lvl1":"Could Brexit have happened by chance?","lvl2":"Extra: Adding a short term swing"},"type":"lvl2","url":"/blog/2016/2016-07-08-voting-randomness#extra-adding-a-short-term-swing","position":8},{"hierarchy":{"lvl1":"Could Brexit have happened by chance?","lvl2":"Extra: Adding a short term swing"},"content":"As I mentioned above, deciding how to simulate the votes involves making assumptions about how things will go. I tried to keep the simulation as simple as possible in order to make a point, but you could include extra components as well.\n\nFor example, what if we chose a random subset of districts in the UK and swung their vote several percentage points in one random direction? This might happen if an eye-catching event caused sentiment to momentarily swing in one direction or another. In the long run each district‚Äôs sentiment would probably ease back into it‚Äôs ‚Äúnatural‚Äù split, but since votes happen on one day, these short-term factors can play an important role.\n\nWe can build this into our simulation...\n\n# Here we define a percentage of districts that undergo a sudden swing in voter opinion\nperc_swing_amt = .1\nperc_swing_districts = .2\nn_perc_swing_districts = int(n_areas * perc_swing_districts)\n\n# Now, re-run the simulation including the random swing.\ndiff = np.zeros(n_votes)\nfor ii in tqdm(range(n_votes)):\n    # Define a random split yes vs. no for each area\n    area_splits = np.random.rand(n_areas)\n    \n    # Define a random swing across a subset of random districts\n    swing = perc_swing_amt * np.random.choice([-1, 1])\n    ixs_swing = np.random.choice(range(n_areas), n_perc_swing_districts, replace=False)\n    area_splits[ixs_swing] = np.clip(swing + area_splits[ixs_swing], 0, 100)\n    \n    # Now calculate the opposing side amount and average\n    area_splits = np.vstack([area_splits, 1 - area_splits])\n    yes, no = area_splits\n    \n    # Now, calculate the difference and average these together, weighted by the area size\n    diffs = yes - no\n    diffs = np.average(diffs, weights=populations['percent'])\n    diff[ii] = diffs * 100\n\n# Here we calculate 99% confidence interval on the difference\nclo, chi = np.percentile(diff, [.5, 99.5])\n\n# Let's look at the distribution of differences\nf, ax = plt.subplots(figsize=(10, 5))\n_ = ax.hist(diff, bins=30)\n_ = ax.axvline(actual_diff, color='r', ls='--')\naxfill = ax.fill_between([clo, chi], *ax.get_ylim(), alpha=.1, color='k')\nax.set_title('Distribution of votes randomized by region', fontsize=20)\nax.legend([ax.lines[0], axfill], ['Actual Difference', 'Confidence Interval'], fontsize=12)\n_ = plt.setp(ax, xlim=[-15, 15])\n\nIt looks like now our confidence intervals are even wider than before. This is becase basically any change to our voting system that deviates away from a completely random 50/50 split will increase the variability in the outcome.","type":"content","url":"/blog/2016/2016-07-08-voting-randomness#extra-adding-a-short-term-swing","position":9},{"hierarchy":{"lvl1":"5 things I learned at SciPy"},"type":"lvl1","url":"/blog/2016/2016-11-01-5-things-scipy-2016","position":0},{"hierarchy":{"lvl1":"5 things I learned at SciPy"},"content":"I‚Äôve finally decompressed after my first go-around with Scipy. For those who haven‚Äôt heard of this conference before, Scipy is an annual meeting where members of scientific community get together to discuss their love of Python, scientific programming, and open science. It spans both academics and people from industry, making it a unique place in terms of how software interfaces with scientific research. (if you‚Äôre interested the full set of Scipy conferences, \n\ncheck out here.\n\nIt was an eye-opening experience that I learned a lot from, so here‚Äôs a quick recap of some things that I learned during my first rodeo.","type":"content","url":"/blog/2016/2016-11-01-5-things-scipy-2016","position":1},{"hierarchy":{"lvl1":"5 things I learned at SciPy","lvl2":"1. Scientific conferences can be fun"},"type":"lvl2","url":"/blog/2016/2016-11-01-5-things-scipy-2016#id-1-scientific-conferences-can-be-fun","position":2},{"hierarchy":{"lvl1":"5 things I learned at SciPy","lvl2":"1. Scientific conferences can be fun"},"content":"I know I know, everybody has fun at conferences because they‚Äôre kind of like mini-vacations. But this is one of the only conferences I‚Äôve been to where people seemed legitimately excited to be at the conference itself. Austin is a really cool city, but rooms were packed full of people whether it was morning or evening. Everybody was enthusiastic about one another‚Äôs work, and it was a rare feeling of community over competitiveness.","type":"content","url":"/blog/2016/2016-11-01-5-things-scipy-2016#id-1-scientific-conferences-can-be-fun","position":3},{"hierarchy":{"lvl1":"5 things I learned at SciPy","lvl2":"2. Open science feels great"},"type":"lvl2","url":"/blog/2016/2016-11-01-5-things-scipy-2016#id-2-open-science-feels-great","position":4},{"hierarchy":{"lvl1":"5 things I learned at SciPy","lvl2":"2. Open science feels great"},"content":"Speaking of competition...you know that feeling when you‚Äôre walking around a conference, and you‚Äôre kind of terrified that you‚Äôll run into somebody else‚Äôs poster that covers the same thing you‚Äôre studying? That feeling doesn‚Äôt exist at scipy. In fact, discovering that somebody has already done what you were working on is great. It means that you can start building off of their hard work, instead of laying a bunch of groundwork yourself. That‚Äôs how science is supposed to work, and is really refreshing coming from a field (neuroscience) that tends to be very cagey and protective of both its data and analyses. It makes me want to encourage these open and collaborative practices in my own field.","type":"content","url":"/blog/2016/2016-11-01-5-things-scipy-2016#id-2-open-science-feels-great","position":5},{"hierarchy":{"lvl1":"5 things I learned at SciPy","lvl2":"3. It‚Äôs actually ‚Äúcutting edge‚Äù"},"type":"lvl2","url":"/blog/2016/2016-11-01-5-things-scipy-2016#id-3-its-actually-cutting-edge","position":6},{"hierarchy":{"lvl1":"5 things I learned at SciPy","lvl2":"3. It‚Äôs actually ‚Äúcutting edge‚Äù"},"content":"In the sense that people talk about things that they‚Äôre actively developing right now. A number of times people gave presentations where they explained that they literally just built in XXX feature thirty minutes before the talk began. When I go to scientific conferences I get the feeling that people are mostly talking about projects that are nearly complete or already submitted. Having a conference that is more about where the field is going, rather than just showcasing what it‚Äôs already accomplished, makes for a great experience and focuses on what to do next.","type":"content","url":"/blog/2016/2016-11-01-5-things-scipy-2016#id-3-its-actually-cutting-edge","position":7},{"hierarchy":{"lvl1":"5 things I learned at SciPy","lvl2":"4. Project Jupyter is going to take over the world"},"type":"lvl2","url":"/blog/2016/2016-11-01-5-things-scipy-2016#id-4-project-jupyter-is-going-to-take-over-the-world","position":8},{"hierarchy":{"lvl1":"5 things I learned at SciPy","lvl2":"4. Project Jupyter is going to take over the world"},"content":"What began as a side project (IPython) has clearly evolved into an organization/project that is impressively ambitious and productive. Jupyter unveiled their next iteration, called JupyterLab, that makes their browser-based platform much more flexible and extensible. I‚Äôm excited to see what the community starts to cook up. Jupyter also made its way into a ton of other presentations and projects. For example, Jupyter has been used to \n\nmanage a cluster of nodes for parallel computing, to create \n\none-button reproducible workflows, and to \n\ndiff and merge notebooks. We also showcased our own use of JupyterHub for \n\nteaching data science to undergraduates at UC Berkeley.","type":"content","url":"/blog/2016/2016-11-01-5-things-scipy-2016#id-4-project-jupyter-is-going-to-take-over-the-world","position":9},{"hierarchy":{"lvl1":"5 things I learned at SciPy","lvl2":"5. There‚Äôs a lot of energy and excitement in science"},"type":"lvl2","url":"/blog/2016/2016-11-01-5-things-scipy-2016#id-5-theres-a-lot-of-energy-and-excitement-in-science","position":10},{"hierarchy":{"lvl1":"5 things I learned at SciPy","lvl2":"5. There‚Äôs a lot of energy and excitement in science"},"content":"At least, when people have an outlet to show it. I heard all kinds of people excited to share their analyses, projects, and findings from their scientific work (as opposed to their software development). It didn‚Äôt have the obligatory ‚ÄúI‚Äôm doing this for the purpose of networking‚Äù feel that I‚Äôve gotten from many conferences. Instead, people were looking forward to showing off the cool work that they‚Äôd done and get feedback from others.\n\nAt the end of the day, Scipy is less of a conference and more of a celebration of the principles that guide the open source and open science community. It turns out that when you base a field off of openness, collaboration, iteration, and empowerment, you create a very energetic and exciting culture. I really enjoyed my first go-around with the scipy community, and I‚Äôm looking forward to what comes next.","type":"content","url":"/blog/2016/2016-11-01-5-things-scipy-2016#id-5-theres-a-lot-of-energy-and-excitement-in-science","position":11},{"hierarchy":{"lvl1":"5 things I learned at SciPy","lvl2":"Some quick extras"},"type":"lvl2","url":"/blog/2016/2016-11-01-5-things-scipy-2016#some-quick-extras","position":12},{"hierarchy":{"lvl1":"5 things I learned at SciPy","lvl2":"Some quick extras"},"content":"Scipy still has a \n\nlong way to go when it comes to gender parity at Scipy. And it felt even worse for racial diversity.\n\nPeople seem really excited about Dask for parallel computing. Here are \n\ntwo \n\ntalks that got a lot of buzz.\n\nAustin is \n\nreally, really hot in the summertime. Especially when your AirBnB is a 2 mile walk away.","type":"content","url":"/blog/2016/2016-11-01-5-things-scipy-2016#some-quick-extras","position":13},{"hierarchy":{"lvl1":"1. Scientific conferences can be fun"},"type":"lvl1","url":"/blog/2016/2016-11-01-5-things-scipy-2016-1","position":0},{"hierarchy":{"lvl1":"1. Scientific conferences can be fun"},"content":"I‚Äôve finally decompressed after my first go-around with Scipy. For those who haven‚Äôt heard of this conference before, Scipy is an annual meeting where members of scientific community get together to discuss their love of Python, scientific programming, and open science. It spans both academics and people from industry, making it a unique place in terms of how software interfaces with scientific research. (if you‚Äôre interested the full set of Scipy conferences, \n\ncheck out here.\n\nIt was an eye-opening experience that I learned a lot from, so here‚Äôs a quick recap of some things that I learned during my first rodeo.","type":"content","url":"/blog/2016/2016-11-01-5-things-scipy-2016-1","position":1},{"hierarchy":{"lvl1":"1. Scientific conferences can be fun"},"type":"lvl1","url":"/blog/2016/2016-11-01-5-things-scipy-2016-1#id-1-scientific-conferences-can-be-fun","position":2},{"hierarchy":{"lvl1":"1. Scientific conferences can be fun"},"content":"I know I know, everybody has fun at conferences because they‚Äôre kind of like mini-vacations. But this is one of the only conferences I‚Äôve been to where people seemed legitimately excited to be at the conference itself. Austin is a really cool city, but rooms were packed full of people whether it was morning or evening. Everybody was enthusiastic about one another‚Äôs work, and it was a rare feeling of community over competitiveness.","type":"content","url":"/blog/2016/2016-11-01-5-things-scipy-2016-1#id-1-scientific-conferences-can-be-fun","position":3},{"hierarchy":{"lvl1":"2. Open science feels great"},"type":"lvl1","url":"/blog/2016/2016-11-01-5-things-scipy-2016-1#id-2-open-science-feels-great","position":4},{"hierarchy":{"lvl1":"2. Open science feels great"},"content":"Speaking of competition...you know that feeling when you‚Äôre walking around a conference, and you‚Äôre kind of terrified that you‚Äôll run into somebody else‚Äôs poster that covers the same thing you‚Äôre studying? That feeling doesn‚Äôt exist at scipy. In fact, discovering that somebody has already done what you were working on is great. It means that you can start building off of their hard work, instead of laying a bunch of groundwork yourself. That‚Äôs how science is supposed to work, and is really refreshing coming from a field (neuroscience) that tends to be very cagey and protective of both its data and analyses. It makes me want to encourage these open and collaborative practices in my own field.","type":"content","url":"/blog/2016/2016-11-01-5-things-scipy-2016-1#id-2-open-science-feels-great","position":5},{"hierarchy":{"lvl1":"3. It‚Äôs actually ‚Äúcutting edge‚Äù"},"type":"lvl1","url":"/blog/2016/2016-11-01-5-things-scipy-2016-1#id-3-its-actually-cutting-edge","position":6},{"hierarchy":{"lvl1":"3. It‚Äôs actually ‚Äúcutting edge‚Äù"},"content":"In the sense that people talk about things that they‚Äôre actively developing right now. A number of times people gave presentations where they explained that they literally just built in XXX feature thirty minutes before the talk began. When I go to scientific conferences I get the feeling that people are mostly talking about projects that are nearly complete or already submitted. Having a conference that is more about where the field is going, rather than just showcasing what it‚Äôs already accomplished, makes for a great experience and focuses on what to do next.","type":"content","url":"/blog/2016/2016-11-01-5-things-scipy-2016-1#id-3-its-actually-cutting-edge","position":7},{"hierarchy":{"lvl1":"4. Project Jupyter is going to take over the world"},"type":"lvl1","url":"/blog/2016/2016-11-01-5-things-scipy-2016-1#id-4-project-jupyter-is-going-to-take-over-the-world","position":8},{"hierarchy":{"lvl1":"4. Project Jupyter is going to take over the world"},"content":"What began as a side project (IPython) has clearly evolved into an organization/project that is impressively ambitious and productive. Jupyter unveiled their next iteration, called JupyterLab, that makes their browser-based platform much more flexible and extensible. I‚Äôm excited to see what the community starts to cook up. Jupyter also made its way into a ton of other presentations and projects. For example, Jupyter has been used to \n\nmanage a cluster of nodes for parallel computing, to create \n\none-button reproducible workflows, and to \n\ndiff and merge notebooks. We also showcased our own use of JupyterHub for \n\nteaching data science to undergraduates at UC Berkeley.","type":"content","url":"/blog/2016/2016-11-01-5-things-scipy-2016-1#id-4-project-jupyter-is-going-to-take-over-the-world","position":9},{"hierarchy":{"lvl1":"5. There‚Äôs a lot of energy and excitement in science"},"type":"lvl1","url":"/blog/2016/2016-11-01-5-things-scipy-2016-1#id-5-theres-a-lot-of-energy-and-excitement-in-science","position":10},{"hierarchy":{"lvl1":"5. There‚Äôs a lot of energy and excitement in science"},"content":"At least, when people have an outlet to show it. I heard all kinds of people excited to share their analyses, projects, and findings from their scientific work (as opposed to their software development). It didn‚Äôt have the obligatory ‚ÄúI‚Äôm doing this for the purpose of networking‚Äù feel that I‚Äôve gotten from many conferences. Instead, people were looking forward to showing off the cool work that they‚Äôd done and get feedback from others.\n\nAt the end of the day, Scipy is less of a conference and more of a celebration of the principles that guide the open source and open science community. It turns out that when you base a field off of openness, collaboration, iteration, and empowerment, you create a very energetic and exciting culture. I really enjoyed my first go-around with the scipy community, and I‚Äôm looking forward to what comes next.","type":"content","url":"/blog/2016/2016-11-01-5-things-scipy-2016-1#id-5-theres-a-lot-of-energy-and-excitement-in-science","position":11},{"hierarchy":{"lvl1":"Some quick extras"},"type":"lvl1","url":"/blog/2016/2016-11-01-5-things-scipy-2016-1#some-quick-extras","position":12},{"hierarchy":{"lvl1":"Some quick extras"},"content":"Scipy still has a \n\nlong way to go when it comes to gender parity at Scipy. And it felt even worse for racial diversity.\n\nPeople seem really excited about Dask for parallel computing. Here are \n\ntwo \n\ntalks that got a lot of buzz.\n\nAustin is \n\nreally, really hot in the summertime. Especially when your AirBnB is a 2 mile walk away.","type":"content","url":"/blog/2016/2016-11-01-5-things-scipy-2016-1#some-quick-extras","position":13},{"hierarchy":{"lvl1":"Visualizing publication bias"},"type":"lvl1","url":"/blog/2016/2016-11-30-funnel-plots","position":0},{"hierarchy":{"lvl1":"Visualizing publication bias"},"content":"This article is now interactive! Check out a live Binder instance \n\nhere\n\nIn the next few months, I‚Äôll try to take some time to talk about the things I learn as I make my way through this literature. While it‚Äôs easy to make one-off complaints to one another about how ‚Äúscience is broken‚Äù without really diving into the details, it‚Äôs important learn about how it‚Äôs broken, or at least how we could assess something like this.\n\nFortunately, there are a lot of great researchers out there who are studying these very issues. Whether they dedicate all of their research to these ‚Äúmeta science‚Äù topics, or simply treat this as a part of their scientific duty on top of their domain-specific work, their work represents a crucial step in reforming our scientific culture.\n\nI‚Äôm not really dredging up anything new here. People have spoken at length about scientific principles and how to improve them for quite a long time. However, I‚Äôve found that these insights often come buried within relatively dense papers that are themselves hidden behind subscription journal paywalls. This is an attempt to make these thoughts a little more digestible, discoverable, and useful.\n\nRead on below to learn about why funnel plots are a great way to visualize the problems our publishing system faces...\n\n# Some quick imports we'll use later\nimport numpy as np\nfrom scipy.stats import distributions\nfrom matplotlib import pyplot as plt\nfrom IPython.html.widgets import interact\nfrom IPython.display import Image\n%matplotlib inline\n\n","type":"content","url":"/blog/2016/2016-11-30-funnel-plots","position":1},{"hierarchy":{"lvl1":"Visualizing publication bias","lvl2":"On to Funnel Plots"},"type":"lvl2","url":"/blog/2016/2016-11-30-funnel-plots#on-to-funnel-plots","position":2},{"hierarchy":{"lvl1":"Visualizing publication bias","lvl2":"On to Funnel Plots"},"content":"(note, all of the plots are taken from the excellent paper The Rules of the Game of Psychological Science, though funnel plots date back at least to the book Summing Up by Light and Lillemer)\n\nBefore diving into the guts of funnel plots, we first need to talk about experiments and effect sizes.\n\nThe (theoretical) goal of science is to observe and accurately describe various phenomena in nature. One way to do this is to conduct some experimental manipulation (e.g., drinking variable amounts of coffee), and measuring its effect on a dependent variable (e.g., how many minutes I spend cleaning my kitchen). Many scientists conduct similar experiments, and report the effect size they found in their papers (e.g., when Chris drinks 2 cups of coffee, he cleans his kitchen an average of 1 hour longer).\n\nWe can aggregate the reported effect size across many papers in order to determine an even ‚Äútruer‚Äù effect, one that removes experimenter bias and noise. This is similar to how poll aggregators theoretically remove noise by combining the results of many different polls (unless of course \n\npollsters are systematically biased). The result is a number that is closer to reality.\n\nOr is it?\n\nOne big problem with this is that scientists don‚Äôt report all of their findings. They only report the ones they (or the journal publishers) deem ‚Äúsignificant‚Äù. In practice, this means that the effect has to be non-zero, because nobody wants to hear about null results (even though, you know, that‚Äôs the vast majority of science). As a result, publishing is skewed in the direction of positive findings, and those that argue for more skepticism about whether an effect actually exists are often asked to please go back to the bench until they can come back with some sexier results.\n\nNow, on to funnel plots.\n\nThe result of this whole situation is that the scientific literature probably overestimates effect sizes and their ‚Äúsignificance‚Äù. How much so? Well, with the advent of digital technologies it has become much easier to aggregate information across scientific studies. This means that we can look for patterns in the reported effect sizes, and determine whether there‚Äôs something fishy going on (spoiler alert: there usually is.)\n\nThe funnel plot is one tool for visualizing and determining whether there is a positive skew in the literature for a given scientific finding (e.g., the relationship between coffee and my cleaning habits). Here‚Äôs what it looks like:\n\nIt‚Äôs a bit busy, but the underlying ideas here are pretty simple.\n\nThe x-axis is the size of an effect (here it‚Äôs correlation but it could be any other statistic). 0 in the middle representing ‚Äúno effect‚Äù and the extremes on either end representing the maximum possible effect for correlation values (in this case).\n\nThe right y-axis is the statistical power of the study. That is, the likelihood of concluding that an effect is ‚Äúsignificantly‚Äù different from 0. As power increases and for a fixed effect size, it becomes more likely that we conclude significance.\n\nThis is related to the left y-axis, which is the inverse of the sample size. AKA, smaller samples -> higher standard error -> less power -> smaller y-values. Larger samples -> lower standard error -> more power -> higher y-values.\n\nFinally, the shaded region tells us combinations of effect sizes / sample sizes that would be deemed ‚Äúsignificant‚Äù (and publishable). If we assume a (two-sided) p-value threshold of .05, the area in white wouldn‚Äôt make it into literature, while the area in grey would.\n\nA funnel plot visually shows that as our sample size goes down, our statistical power also goes down. This means that with smaller sample sizes, we need a larger effect in order to conclude that our results are significant (and get them into Nature). Seems reasonable, so where‚Äôs the problem?\n\nThe issue lies in the aforementioned positive effect bias in scientific publishing. Because null effects won‚Äôt ever make it into the literature, the effect size we aggregate across papers will only draw from those that fall outside of the white inner region.\n\nThis is a problem because the whole point of science is to estimate the ‚Äútrue‚Äù underlying distribution of an effect, as opposed to merely determining whether it is ‚Äúdifferent from zero‚Äù. So, let‚Äôs show the ‚Äútrue‚Äù and ‚Äúreported‚Äù distributions at the top and see what happens.\n\nOn the top of the funnel plot we can see the two distributions at play. In green is the ‚Äúnull‚Äù distribution, meaning the set of results we‚Äôd expect to see if there was really no statistical effect. Now we have more explanation for the white region of non-significance in the middle. As we have smaller sample sizes (lower y-values), the noise increases, and we‚Äôd expect more variability under the null distribution. This is why we need a really large effect size to conclude that there‚Äôs really something going on.\n\nNow look at the ‚Äúalternative‚Äù hypothesis in red. This is the ‚Äúexperimental‚Äù distribution of this statistic, as determined from the results combined across many studies that estimate this effect. From these results, it looks like it is quite different from the ‚Äúnull‚Äù distribution. Hooray, science has found an effect!\n\nBut wait a second, there‚Äôs something funny about these results. Notice how the datapoints (the effect sizes in reported studies) seem to follow the boundary between the white and the grey regions? Also note that they don‚Äôt look symmetric around the mean of the ‚Äúexperimental‚Äù distribution. That‚Äôs positive publication bias in action.\n\nThe reason that data points follow the boundary between white / grey isn‚Äôt because that‚Äôs the ‚Äútruth‚Äù, but because our publishing system and scientific incentives suppress findings that lie in the white region. It doesn‚Äôt mean these data points don‚Äôt exist, they just lie in the filing cabinets of labs all of the world who aren‚Äôt able to publish results that aren‚Äôt significant. As a result, we get a skewed idea of what the true effect size is.\n\nThere‚Äôs another problem with this plot. As we‚Äôve noted, small sample sizes means that you can only write papers with really large effect sizes. Seems reasonable, but if you can‚Äôt report non-significant results, it means that studies with a smaller N are the most likely to throw off our belief about the true effect size.","type":"content","url":"/blog/2016/2016-11-30-funnel-plots#on-to-funnel-plots","position":3},{"hierarchy":{"lvl1":"Visualizing publication bias","lvl3":"Getting our hands dirty with some code","lvl2":"On to Funnel Plots"},"type":"lvl3","url":"/blog/2016/2016-11-30-funnel-plots#getting-our-hands-dirty-with-some-code","position":4},{"hierarchy":{"lvl1":"Visualizing publication bias","lvl3":"Getting our hands dirty with some code","lvl2":"On to Funnel Plots"},"content":"But this is all very theoretical...to show how this works, we‚Äôll investigate funnel plots with a quick simulation to drive the point home.\n\nWe‚Äôll simulate 10,000 studies, each with an N ranging from 2 to 50. We‚Äôll ignore all of the ‚Äúquestionable scientific practices‚Äù that the article mentions, and only focus on the problem of not reporting scientific results. Let‚Äôs see what happens:\n\nNote: you can skip reading the code below if you like, as it just defines some functions that will be useful, but feel free to dig into the code if you like\n\n# Helper functions to simulate experiments.\ndef simulate_data(effect, variance, n):\n    \"\"\"Simulate a population of data. We'll sample from this in each study.\n    Note that we're drawing from a normal distribution.\"\"\"\n    data = np.sqrt(true_variance) * np.random.randn(int(n))\n    data += effect\n    return data\n\ndef simulate_experiments(data, n_min=10, n_max=50, prefer_low_n=False,\n                         n_simulations=100):\n    \"\"\"Randomly simulates data collection and analyses of many experiments.\n    \n    On each iteration, it chooses a random sample from data, calculates the\n    mean of that sample, as well as a p-value associated with that mean's\n    difference from 0.\n    \n    data : the full population dataset\n    n_min : the minimum sample size for each study.\n    n_max : the maximum sample size for each study.\n    prefer_low_n : whether lower sample sizes are preferred.\n    \"\"\"\n    effects = np.zeros(n_simulations)\n    n = np.zeros(n_simulations)\n    p = np.zeros(n_simulations)\n    for ii in range(n_simulations):\n        # Take a random sample from the population\n        if prefer_low_n is False:\n            n_sample = np.random.randint(n_min, n_max, 1)[0]\n        else:\n            probabilities = np.logspace(5, 1, n_max - n_min)\n            probabilities /= np.sum(probabilities)\n            n_sample = np.random.choice(range(n_min, n_max),\n                                        p=probabilities)\n        ixs_sample = random_indices[ii][:n_sample]\n        i_data = data[ixs_sample]\n        effects[ii] = np.mean(i_data)\n        n[ii] = n_sample\n        p[ii] = calculate_stat(np.mean(i_data), np.std(i_data), n_sample)\n    return effects, n, p\n\ndef calculate_stat(mean, std, n, h0=0):\n    \"\"\"Calculate a p-value using a t-test.\n    \n    Note that this probably *isn't* the right test to run with data that\n    is bounded on either side (in this case, -1 and 1). However, luckily\n    this is not a statistics tutorial so I'm just going to be blissfully\n    ignorant of this.\n    \"\"\"\n    t = (mean - h0) / (std / np.sqrt(n))\n    p = distributions.t.pdf(t, n-1)\n    return p\n\n\ndef plot_funnel_plot(effects, sample_sizes,\n                     effects_reported, sample_sizes_reported,\n                     p_effects_reported):\n    \"\"\"Creates a funnel plot using a 'full' set of effects, corresponding\n    to the effects we'd report if all results were published, regardless of\n    their 'significance', as well as a 'reported' set of effects which made\n    it through peer review\"\"\"\n    # Create a figure w/ 2 axes\n    fig = plt.figure(figsize=(5, 5))\n    axdist = plt.subplot2grid((4, 4), (0, 0), 1, 4)\n    axmesh = plt.subplot2grid((4, 4), (1, 0), 3, 4)\n\n    # Calculate relevant stats\n    mn_full = effects.mean()\n    std_full = effects.std()\n    mn_pub = effects_reported.mean()\n    std_pub = effects_reported.std()\n    \n    mn_diff = np.abs(mn_full - mn_pub)\n    std_diff = np.abs(std_full - std_pub)\n    \n    # First axis is a histogram of the distribution for true/experimental effects\n    bins = np.arange(-2, 2, .1)\n    _ = axdist.hist(effects, color='k', histtype='stepfilled',\n                    normed=True, bins=bins)\n    _ = axdist.hlines(4.5, mn_full - std_full, mn_full + std_full,\n                      color='.3', lw=2)\n    _ = axdist.hist(effects_reported, color='r', histtype='step', lw=2,\n                    normed=True, bins=bins)\n    _ = axdist.hlines(4.0, mn_pub - std_pub, mn_pub + std_pub,\n                      color='r', lw=2)\n    axdist.set_ylim([0, 5])\n    axdist.set_title('Distribution of effects\\nError in mean: {:.3f}'\n                     '\\nError in std: {:.3f}'.format(mn_diff, std_diff))\n    axdist.set_axis_off()\n\n    # Now make the funnel plot\n    sig = pvals < .05\n    mesh = axmesh.contour(combinations[0], combinations[1], sig, cmap=plt.cm.Greys,\n                          vmin=0, vmax=3, rasterized=True)\n    \n    inv_p_effects = 1 - p_effects_reported\n    axmesh.scatter(effects, sample_sizes,\n                   s=100, c='k', alpha=.1)\n    axmesh.scatter(effects_reported, sample_sizes_reported,\n                   s=100, c=inv_p_effects,\n                   vmin=.95, vmax=1., cmap=plt.cm.viridis)\n    axmesh.axis('tight')\n    axmesh.set_xlabel('Effect Size')\n    axmesh.set_ylabel('Sample Size (or statisical power)')\n\n    _ = plt.setp(axdist, xlim=axmesh.get_xlim())\n    return fig\n\n","type":"content","url":"/blog/2016/2016-11-30-funnel-plots#getting-our-hands-dirty-with-some-code","position":5},{"hierarchy":{"lvl1":"Visualizing publication bias","lvl2":"Simulating the population"},"type":"lvl2","url":"/blog/2016/2016-11-30-funnel-plots#simulating-the-population","position":6},{"hierarchy":{"lvl1":"Visualizing publication bias","lvl2":"Simulating the population"},"content":"Here we‚Äôll create a population of datapoints corresponding to the effect of each person. Experiments are performed by taking a random sample from that population, and calculating the average effect of the sample. For each experiment we‚Äôll choose a random number for the sample size as well. That means that we‚Äôll get a collection of sample sizes, effect sizes, and p-values. One set for each simulated experiment.\n\n# This is the true value and variance of our variable of interest.\n# Remember, it's bounded between -2 and 2\ntrue_value = .5\ntrue_variance = 2\n\n# This creates the contour to show the \"significance edge\" of the plot\nn_simulations = 200\neffect_sizes = np.linspace(-2, 2, 1000)\nns = np.arange(2, 100, .1)\ncombinations = np.meshgrid(effect_sizes, ns)\npvals = calculate_stat(combinations[0], np.sqrt(true_variance),\n                       combinations[1])\n\n# How many simulations will we run, and how large is the full population\ntotal_population = 1e5\nn_min, n_max = 5, 100\n\n# We'll pre-define these because they take a while\npopulation_indices = np.arange(total_population).astype(int)\nrandom_indices = [np.random.permutation(population_indices)\n                  for _ in range(n_simulations)]\n\n# First create our population data\ndata = simulate_data(true_value, true_variance, total_population)\n\n# Simulate a bunch of random effects, along w/ sample size and p-value for each\neffects, n, p = simulate_experiments(data, n_min=n_min, n_max=n_max,\n                                     n_simulations=n_simulations)\n\n# In this case, the reported and actual effects are the same\n_ = plot_funnel_plot(effects, n, effects, n, p)\n\nIn the funnel plot above, each datapoint corresponds to the effect size found in a single study (x-axis), along with its sample size (y-axis).\n\nThe contour lines show us the ‚Äúsignificance cutoffs‚Äù.\n\nThe distributions at the top show us the effect size distribution for all experiments, as well as the distribution for only the reported experiments. In this case, those distributions are the same because all of our scientific experiments reported their results. We have an accurate idea of the effect size.","type":"content","url":"/blog/2016/2016-11-30-funnel-plots#simulating-the-population","position":7},{"hierarchy":{"lvl1":"Visualizing publication bias","lvl2":"Simulate the scientific publishing world"},"type":"lvl2","url":"/blog/2016/2016-11-30-funnel-plots#simulate-the-scientific-publishing-world","position":8},{"hierarchy":{"lvl1":"Visualizing publication bias","lvl2":"Simulate the scientific publishing world"},"content":"Now, let‚Äôs simulate the scientific publishing process and see what happens. We‚Äôll take a relatively generous take on things, and say that studies with a p-value > .05 still have a small chance of being accepted.\n\n# This simulates which datapoints we keep and which we throw out\ndef simulate_publishing(pvals, null_perc=.01, pos_p_perc=.5, super_p_perc=.9):\n    \"\"\"Given a collection of p-vals, randomly choose ones to accept for\n    publication, with the likelihood of acceptance varying with the size\n    of the p-value.\"\"\"\n    keep = np.zeros_like(pvals).astype(bool)\n    for i, ip in enumerate(pvals):\n        flip = np.random.rand()\n        if ip > .05:\n            this_perc = null_perc\n        elif ip > .005 and ip < .05:\n            this_perc = pos_p_perc\n        else:\n            this_perc = super_p_perc\n        keep[i] = True if flip < this_perc else False\n    return keep\n\ndef plot_simulation_results(p_values, mask_reported):\n    \"\"\"A quick way to viz which papers get accepted and which don't\"\"\"\n    fig, ax = plt.subplots()\n    sc = ax.scatter(range(len(p_values)), p_values,\n                      c=mask_reported, s=50, cmap=plt.cm.viridis,\n                      vmin=0, vmax=1)\n    ax.axhline(.05, ls='--')\n    _ = plt.setp(ax, ylabel=\"p-value\", xlabel=\"study number\",\n                 title='Accepted and rejected studies')\n    return ax\n\nmask_reported = simulate_publishing(p, null_perc=.1, pos_p_perc=.5,\n                                    super_p_perc=.9)\neffects_reported = effects[mask_reported]\nn_reported = n[mask_reported]\np_reported = p[mask_reported]\n_ = plot_funnel_plot(effects, n, effects_reported, n_reported, p_reported)\n\nplot_simulation_results(p, mask_reported)\n\nWe can already see that we‚Äôve skewed the distribution of reported findings (in red) further to the right. This is because it is less likely for experiments inside the contour lines to be reported in the literature, making us think that the effect size is larger than it really is.\n\nNow, let‚Äôs take a more cynical look at scientific publishing by reducing the likelihood that studies are published w/o a ‚Äúsignificant‚Äù result:\n\nmask_reported = simulate_publishing(p, null_perc=0, pos_p_perc=.3,\n                                    super_p_perc=.99)\neffects_reported = effects[mask_reported]\nn_reported = n[mask_reported]\np_reported = p[mask_reported]\n_ = plot_funnel_plot(effects, n, effects_reported, n_reported, p_reported)\n\nIt‚Äôs skewed even further to the right. As you can see, the harder it is to publish null results, the more overconfident we will be in the significance of what‚Äôs in the literature. As you can probably tell, this is especially problematic for effect sizes lie near the boundary between publishable / non-publishable.","type":"content","url":"/blog/2016/2016-11-30-funnel-plots#simulate-the-scientific-publishing-world","position":9},{"hierarchy":{"lvl1":"Visualizing publication bias","lvl2":"Adding a low-N bias"},"type":"lvl2","url":"/blog/2016/2016-11-30-funnel-plots#adding-a-low-n-bias","position":10},{"hierarchy":{"lvl1":"Visualizing publication bias","lvl2":"Adding a low-N bias"},"content":"As we mentioned above, there‚Äôs one more factor at play that makes things even worse. Smaller studies take less time and less resources to conduct, and in practice there are far more tiny studies than large, highly-powered ones. Let‚Äôs incorporate that into our data simulation and see how that affects things.\n\n# This simulates data where there is about a 10 times higher chance for a low-n study\neffects, n, p = simulate_experiments(data, n_min=n_min, n_max=n_max,\n                              prefer_low_n=True)  \n\nmask_reported = simulate_publishing(p, null_perc=0., pos_p_perc=.3,\n                                    super_p_perc=.99)\neffects_reported = effects[mask_reported]\nn_reported = n[mask_reported]\np_reported = p[mask_reported]\n\n_ = plot_funnel_plot(effects, n, effects_reported, n_reported, p_reported)\n\nIt‚Äôs even worse. As you can see, both of these factors (studies with a low N, not being able to publish null results) give the scientific community an unrealistic idea of the true effect size. Moreover, we haven‚Äôt even incorporated any experimenter-specific biases, such as defining datapoints that nullify an effect as ‚Äúoutliers‚Äù, not reporting studies that are significant but in the opposite direction of what we‚Äôd expect, and collecting more data until they achieve a significant p-value. All of these practices would serve to enhance the positive bias seen above.\n\nIn many cases, this might cause us to conclude that there is an effect, when in reality there is not. Unfortunately, this often has wide-ranging implications for things like policy decisions, and at the least causes scientists to be ineffective and inefficient at asking questions about the world.\n\nAll of this is not to say that science ‚Äúdoesn‚Äôt work‚Äù, but it‚Äôs important to remember that science is about methodology before anything else, and the tools of empiricism and peer review are in constant evolution as we learn more about the pitfalls of our current approach. This is one way to identify these pitfalls, and hopefully in future years the community will adapt in order to avoid them.\n\n","type":"content","url":"/blog/2016/2016-11-30-funnel-plots#adding-a-low-n-bias","position":11},{"hierarchy":{"lvl1":"Visualizing publication bias","lvl2":"Try it yourself!"},"type":"lvl2","url":"/blog/2016/2016-11-30-funnel-plots#try-it-yourself","position":12},{"hierarchy":{"lvl1":"Visualizing publication bias","lvl2":"Try it yourself!"},"content":"If you‚Äôre curious about how all of these factors (effect size, effect variability, sample size, and publishing practices) interact, here‚Äôs a quick function to let you play around with each one and determine what the effect would look like in the literature. There are particular circumstances in which these issues are most apparent, and most problematic. See if you can figure out what those circumstances are.\n\n# Create datasets with new effects / variances here\neffect = .5\nvariance = 3\nn_population = 1e6\nn_simulations = 100\ndata = simulate_data(effect, variance, n_population)\n\n# We'll pre-define these because they take a while\npopulation_indices = np.arange(len(data)).astype(int)\nrandom_indices = [np.random.permutation(population_indices)\n                  for _ in range(n_simulations)]\n\nsample_min = 4\nsample_max = 100\nprefer_low_n = True\neffects, n, p = simulate_experiments(data, n_min=sample_min, n_max=sample_max,\n                     prefer_low_n=prefer_low_n, n_simulations=n_simulations)\n\ndef plot_simulated_data(null_perc=.05, pos_perc=.5, super_p_perc=1.):\n    \"\"\"\n    null_perc = Chance of accepting paper w/ a null result (p<.05)\n    pos_perc = Chance of accepting a paper w/ a moderate effect size\n    super_p_perc = Chance of accepting a paper w/ a big effect size\n    \"\"\"\n    mask_reported = simulate_publishing(\n        p, null_perc=null_perc, pos_p_perc=pos_perc, super_p_perc=super_p_perc)\n    effects_reported = effects[mask_reported]\n    n_reported = n[mask_reported]\n    p_reported = p[mask_reported]\n    plot_funnel_plot(effects, n, effects_reported, n_reported, p_reported)\n    \ninteract(plot_simulated_data,\n         null_perc=[0., 1., .01],\n         pos_perc=[0., 1., .01],\n         super_p_perc=[0., 1., .01])","type":"content","url":"/blog/2016/2016-11-30-funnel-plots#try-it-yourself","position":13},{"hierarchy":{"lvl1":"The bleeding edge of publishing, Scraping publication amounts at biorxiv"},"type":"lvl1","url":"/blog/2016/2016-12-19-biorxiv-neuro","position":0},{"hierarchy":{"lvl1":"The bleeding edge of publishing, Scraping publication amounts at biorxiv"},"content":"Per a recent request somebody posted on Twitter, I thought it‚Äôd be fun to write a quick scraper for the \n\nbiorxiv, an excellent new tool for posting pre-prints of articles before they‚Äôre locked down with a publisher embargo.\n\nA big benefit of open science is the ability to use modern technologies (like web scraping) to make new use of data that would originally be unavailable to the public. One simple example of this is information and metadata about published articles. While we‚Äôre not going to dive too deeply here, maybe this will serve as inspiration for somebody else interested in scraping the web.\n\nFirst we‚Äôll do a few imports. We‚Äôll rely heavily on the requests and BeautifulSoup packages, which together make an excellent one-two punch for doing web scraping. We coud use something like scrapy, but that seems a little overkill for this small project.\n\nimport requests\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nfrom bs4 import BeautifulSoup as bs\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n%matplotlib inline\n\nFrom a quick look at the biorxiv we can see that its search API works in a pretty simple manner. I tried typing in a simple search query and got something like this:\n\nhttp://biorxiv.org/search/neuroscience%20numresults%3A100%20sort%3Arelevance-rank\n\nHere we can see that the term you search for comes just after /search/, and parameters for the search, like numresults. The keyword/value pairs are separated by a %3A character, which corresponds to : (see \n\nthis site for a reference of url encoding characters), and these key/value pairs are separated by %20, which corresponds to a space.\n\nSo, let‚Äôs do a simple scrape and see what the results look like. We‚Äôll query the biorxiv API to see what kind of structure the result will have.\n\nn_results = 20\nurl = \"http://biorxiv.org/search/neuroscience%20numresults%3A{}\".format(\n    n_results)\nresp = requests.post(url)\n\n# I'm not going to print this because it messes up the HTML rendering\n# But you get the idea...probably better to look in Chrome anyway ;)\n# text = bs(resp.text)\n\nIf we search through the result, you may notice that search results are organized into a list (denoted by li for each item). Inside each item is information about the article‚Äôs title (in a div of class highwire-cite-title) and author information (in a div of calss highwire-cite-authors).\n\nLet‚Äôs use this information to ask three questions:\n\nHow has the rate of publications for a term changed over the years\n\nWho‚Äôs been publishing under that term.\n\nWhat kinds of things are people publishing?\n\nFor each, we‚Äôll simply use the phrase ‚Äúneuroscience‚Äù, although you could use whatever you like.\n\nTo set up this query, we‚Äôll need to use another part of the biorxiv API, the limit_from paramter. This lets us constrain the search to a specific month of the year. That way we can see the monthly submissions going back several years.\n\nWe‚Äôll loop through years / months, and pull out the author and title information. We‚Äôll do this with two dataframes, one for authors, one for articles.\n\n# Define the URL and start/stop years\nstt_year = 2012\nstp_year = 2016\nsearch_term = \"neuroscience\"\nurl_base = \"http://biorxiv.org/search/{}\".format(search_term)\nurl_params = \"%20limit_from%3A{0}-{1}-01%20limit_to%3A{0}-{2}-01%20numresults%3A100%20format_result%3Astandard\"\nurl = url_base + url_params\n\n# Now we'll do the scraping...\nall_articles = []\nall_authors = []\nfor yr in tqdm(range(stt_year, stp_year + 1)):\n    for mn in range(1, 12):\n        # Populate the fields with our current query and post it\n        this_url = url.format(yr, mn, mn + 1) \n        resp = requests.post(this_url)\n        html = bs(resp.text)\n        \n        # Collect the articles in the result in a list\n        articles = html.find_all('li', attrs={'class': 'search-result'})\n        for article in articles:\n            # Pull the title, if it's empty then skip it\n            title = article.find('span', attrs={'class': 'highwire-cite-title'})\n            if title is None:\n                continue\n            title = title.text.strip()\n            \n            # Collect year / month / title information\n            all_articles.append([yr, mn, title])\n            \n            # Now collect author information\n            authors = article.find_all('span', attrs={'class': 'highwire-citation-author'})\n            for author in authors:\n                all_authors.append((author.text, title))\n\n# We'll collect these into DataFrames for subsequent use\nauthors = pd.DataFrame(all_authors, columns=['name', 'title'])\narticles = pd.DataFrame(all_articles, columns=['year', 'month', 'title'])\n\nTo make things easier to cross-reference, we‚Äôll add an id column that‚Äôs unique for each title. This way we can more simply join the dataframes to do cool things:\n\n# Define a dictionary of title: ID mappings\nunique_ids = {title: ii for ii, title in enumerate(articles['title'].unique())}\narticles['id'] = [unique_ids[title] for title in articles['title']]\nauthors['id'] = [unique_ids[title] for title in authors['title']]\n\nNow, we can easily join these two dataframes together if we so wish:\n\npd.merge(articles, authors, on=['id', 'title']).head()\n\n","type":"content","url":"/blog/2016/2016-12-19-biorxiv-neuro","position":1},{"hierarchy":{"lvl1":"The bleeding edge of publishing, Scraping publication amounts at biorxiv","lvl2":"Question 1: How has the published articles rate changed?"},"type":"lvl2","url":"/blog/2016/2016-12-19-biorxiv-neuro#question-1-how-has-the-published-articles-rate-changed","position":2},{"hierarchy":{"lvl1":"The bleeding edge of publishing, Scraping publication amounts at biorxiv","lvl2":"Question 1: How has the published articles rate changed?"},"content":"This one is pretty easy to ask. Since we have both year / month data about each article, we can plot the number or articles for each group of time. To do this, let‚Äôs first turn these numbers into an actual ‚Äúdatetime‚Äù object. This let‚Äôs us do some clever plotting magic with pandas\n\n# Add a \"date\" column\ndates = [pd.datetime(yr, mn, day=1)\n         for yr, mn in articles[['year', 'month']].values]\narticles['date'] = dates\n\n# Now drop the year / month columns because they're redundant\narticles = articles.drop(['year', 'month'], axis=1)\n\nNow, we can simply group by month, sum the number of results, and plot this over time:\n\nmonthly = articles.groupby('date').count()['title'].to_frame()\nax = monthly['title'].plot()\nax.set_title('Articles published per month for term\\n{}'.format(search_term))\n\nWe can also plot the cumulative number of papers published:\n\ncumulative = np.cumsum(monthly.values)\nmonthly['cumulative'] = cumulative\n\n# Now plot cumulative totals\nax = monthly['cumulative'].plot()\nax.set_title('Cumulative number of papers matching term \\n{}'.format(search_term))\nax.set_ylabel('Number of Papers')\n\n","type":"content","url":"/blog/2016/2016-12-19-biorxiv-neuro#question-1-how-has-the-published-articles-rate-changed","position":3},{"hierarchy":{"lvl1":"The bleeding edge of publishing, Scraping publication amounts at biorxiv","lvl2":"Question 2: Which author uses pre-prints the most?"},"type":"lvl2","url":"/blog/2016/2016-12-19-biorxiv-neuro#question-2-which-author-uses-pre-prints-the-most","position":4},{"hierarchy":{"lvl1":"The bleeding edge of publishing, Scraping publication amounts at biorxiv","lvl2":"Question 2: Which author uses pre-prints the most?"},"content":"For this one, we can use the ‚Äúauthors‚Äù dataframe. We‚Äôll group by author name, and count the number of publications per author:\n\n# Group by author and count the number of items\nauthor_counts = authors.groupby('name').count()['title'].to_frame('count')\n\n# We'll take the top 30 authors\nauthor_counts = author_counts.sort_values('count', ascending=False)\nauthor_counts = author_counts.iloc[:30].reset_index()\n\nWe‚Äôll use some pandas magical gugu to get this one done. Who is the greatest pre-print neuroscientist of them all?\n\n# So we can plot w/ pretty colors\ncmap = plt.cm.viridis\ncolors = cmap(author_counts['count'].values / float(author_counts['count'].max()))\n\n# Make the plot\nfig, ax = plt.subplots(figsize=(10, 5))\nax = author_counts.plot.bar('name', 'count', color=colors, ax=ax)\n_ = plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n\nRather than saying congratulations to #1 etc here, I‚Äôll just take this space to say that all of these researchers are awesome for helping push scientific publishing technologies into the 21st century ;)","type":"content","url":"/blog/2016/2016-12-19-biorxiv-neuro#question-2-which-author-uses-pre-prints-the-most","position":5},{"hierarchy":{"lvl1":"The bleeding edge of publishing, Scraping publication amounts at biorxiv","lvl2":"Question 3: What topics are covered in the titles?"},"type":"lvl2","url":"/blog/2016/2016-12-19-biorxiv-neuro#question-3-what-topics-are-covered-in-the-titles","position":6},{"hierarchy":{"lvl1":"The bleeding edge of publishing, Scraping publication amounts at biorxiv","lvl2":"Question 3: What topics are covered in the titles?"},"content":"For this one we‚Äôll use a super floofy answer, but maybe it‚Äôll give us something pretty. We‚Äôll use the wordcloud module, which implements fit and predict methods similar to scikit-learn. We can train it on the words in the titles, and then create a pretty word cloud using these words.\n\nTo do this, we‚Äôll use the wordcloud module along with sklearn‚Äôs stop words (which are also useful for text analysis, incidentally)\n\nimport wordcloud as wc \nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n\n# We'll collect the titles and turn them into one giant string\ntitles = articles['title'].values\ntitles = ' '.join(titles)\n\n# Then define stop words to use...we'll include some \"typical\" brain words\nour_stop_words = list(ENGLISH_STOP_WORDS) + ['brain', 'neural']\n\nNow, generating a word cloud is as easy as a call to generate_from_text. Then we can output in whatever format we like\n\n# This function takes a buch of dummy arguments and returns random colors\ndef color_func(word=None, font_size=None, position=None,\n               orientation=None, font_path=None, random_state=None):\n    rand = np.clip(np.random.rand(), .2, None)\n    cols = np.array(plt.cm.rainbow(rand)[:3])\n    cols = cols * 255\n    return 'rgb({:.0f}, {:.0f}, {:.0f})'.format(*cols)\n\n# Fit the cloud\ncloud = wc.WordCloud(stopwords=our_stop_words,\n                     color_func=color_func)\ncloud.generate_from_text(titles)\n\n# Now make a pretty picture\nim = cloud.to_array()\nfig, ax = plt.subplots()\nax.imshow(im, cmap=plt.cm.viridis)\nax.set_axis_off()\n\nLooks like those cognitive neuroscience folks are leading the charge towards pre-print servers. Hopefully in the coming years we‚Äôll see increased adoption from the systems and cellular fields as well.","type":"content","url":"/blog/2016/2016-12-19-biorxiv-neuro#question-3-what-topics-are-covered-in-the-titles","position":7},{"hierarchy":{"lvl1":"The bleeding edge of publishing, Scraping publication amounts at biorxiv","lvl2":"Wrapup"},"type":"lvl2","url":"/blog/2016/2016-12-19-biorxiv-neuro#wrapup","position":8},{"hierarchy":{"lvl1":"The bleeding edge of publishing, Scraping publication amounts at biorxiv","lvl2":"Wrapup"},"content":"Here we played with just a few questions that you can ask with some simple web scraping and the useful tools in python. There‚Äôs a lot more that you could do with it, but I‚Äôll leave that up to readers to figure out for themselves :)","type":"content","url":"/blog/2016/2016-12-19-biorxiv-neuro#wrapup","position":9},{"hierarchy":{"lvl1":"Brainy Jingle Bells"},"type":"lvl1","url":"/blog/2016/2016-12-23-christmas-ecog-plot","position":0},{"hierarchy":{"lvl1":"Brainy Jingle Bells"},"content":"This is a quick demo of how I created \n\nthis video. Check it out below, or read on to see the code that made it!\n\nfrom IPython.display import YouTubeVideo\nYouTubeVideo('lZS4uaTBrh8')\n\nimport pandas as pd\nimport mne\nimport numpy as np\nimport scipy.io.wavfile as wav\nimport matplotlib.pyplot as plt\nfrom moviepy.editor import VideoClip, ImageClip, AudioFileClip\nfrom moviepy.video.io.bindings import mplfig_to_npimage\nfrom sklearn.preprocessing import MinMaxScaler\nimport colorbabel as cb\n%matplotlib inline\n\n","type":"content","url":"/blog/2016/2016-12-23-christmas-ecog-plot","position":1},{"hierarchy":{"lvl1":"Brainy Jingle Bells","lvl2":"Jingle Bells!"},"type":"lvl2","url":"/blog/2016/2016-12-23-christmas-ecog-plot#jingle-bells","position":2},{"hierarchy":{"lvl1":"Brainy Jingle Bells","lvl2":"Jingle Bells!"},"content":"Here‚Äôs a quick viz to show off some brainy holiday spirit.\n\nWe‚Äôll use matplotlib and MoviePy to read in an audio file and generate a scatterplot that responds to the audio qualities.\n\n# Load the audio clip with MoviePy to save to the movie later\npath_audio = '../../../../data/jinglebells.wav'\naudio_clip = AudioFileClip(path_audio)\n\n# Now load the sound as an array for manipulation\nsfreq, audio = wav.read(path_audio)\naudio = audio.T[0]\n\n# This is the amount of time the audio takes up\ntime_audio = audio.shape[-1] / float(sfreq)\nprint('Total time: {}'.format(time_audio))\n\n# Now read some brain info for plotting\n# NOTE: this is broken, but it's an old post so I'm going to just pretend it isn't broken :-)\n# melec = pd.read_csv('../../../../data/brain/meta_elec.csv')\n# im = plt.imread('../../../../data/brain/brain.png')\n\nWe‚Äôll use the spectral content in the audio to drive activity in the electrodes. Here‚Äôs what I‚Äôm talking about by spectral content:\n\n# A pretty spectrogram of audio\nfig, ax = plt.subplots()\n_ = ax.specgram(audio[100000:200000], Fs=sfreq, cmap=plt.cm.viridis)\nplt.autoscale(tight=True)\nax.set(ylim=[None, 8000])\n\n\nWe‚Äôll extract this information again below so we can make the viz...\n\n# Resample the audio so that it's not so long to process\nsfreq_new = 11025\naudio = mne.filter.resample(audio, sfreq_new, sfreq)\n\n# Now extract a spectrogram of the audio\ndecim = 400\nsfreq_amp = sfreq_new / float(decim)\nfreqs = np.logspace(np.log10(400), np.log10(6000), 10)\nspec = mne.time_frequency.tfr._compute_tfr(\n    audio.reshape([1, 1, -1]), freqs, sfreq=sfreq_new, decim=decim)\nspec = np.abs(spec).squeeze()\n\n# Low-pass filter the spectrogram so it varies more smoothly\nspec = mne.filter.filter_data(spec, sfreq_amp, None, 5)\n\nNow, we‚Äôll assign each electrode to a particular point on the y-axis of the spectrogram. We‚Äôll assign based off of the height of each electrode.\n\n# Now bin the y-point of each electrode and assign it to a specotrogram bin\ny_bins = np.linspace(melec['y_2d'].min(), melec['y_2d'].max(), len(freqs))\nbinned_elecs = np.digitize(melec['y_2d'].values, y_bins)\n\n# Scale the amplitude of each frequency band and assign them to electrodes\nscaler = MinMaxScaler(feature_range=(0, 1.6))\namplitudes = spec[binned_elecs - 1, :]\namplitudes_scaled = np.clip(scaler.fit_transform(amplitudes.T).T, None, 1)\n\n# Scaling etc for the scatterplot\namplitudes_sizes = np.clip(amplitudes_scaled, .01, None) * 100\namplitudes_sizes **= 2\namplitudes_sizes *= 1  # Set to 1 to not change size at all\n\n# Set the sampling frequency for the video so it fills up all the audio time\nn_frames = amplitudes.shape[-1]\nduration = time_audio\nsfreq_video = n_frames / duration\nprint(sfreq_video)\n\n","type":"content","url":"/blog/2016/2016-12-23-christmas-ecog-plot#jingle-bells","position":3},{"hierarchy":{"lvl1":"Brainy Jingle Bells","lvl2":"Making the movie"},"type":"lvl2","url":"/blog/2016/2016-12-23-christmas-ecog-plot#making-the-movie","position":4},{"hierarchy":{"lvl1":"Brainy Jingle Bells","lvl2":"Making the movie"},"content":"\n\n# Here is our colorbar\ntrans = cb.ColorTranslator(['red', 'green'])\ncmap = trans.to_diverging(mid_spread=.8)\ncb.ColorTranslator(cmap).show_colors()\n\n# Here's an example of what the plot looks like\nfig, ax = plt.subplots(figsize=(10, 10))\nax.imshow(im)\nax.set_axis_off()\nscat = ax.scatter(*melec[['x_2d', 'y_2d']].values.T, s=100)\n\n# This function maps a time (in seconds) onto an index\n# It sets the scatterplot sizes from that index\n# Then it returns the image of the figure.\ndef animate_scatterplot(t):\n    ix = int(np.round(t * sfreq_video)) - 1\n    sizes = amplitudes_sizes[:, ix]\n    colors = amplitudes_scaled[:, ix]\n    scat.set_sizes(sizes)\n    scat.set_color(cmap(colors))\n    return mplfig_to_npimage(fig)\n\n# Now we'll create our videoclip using this function, and give it audio\nclip = VideoClip(animate_scatterplot, duration=duration)\nclip.audio = audio_clip\n\n# Finally, write it to disk\nclip.write_videofile('../data/jinglebells.mp4', fps=sfreq_video, audio=True)\n\nAnd now you‚Äôve got a great video!\n\nCredit for the nice brain image goes to the excellent \n\nBenedicte Rossi","type":"content","url":"/blog/2016/2016-12-23-christmas-ecog-plot#making-the-movie","position":5},{"hierarchy":{"lvl1":"Matplotlib Cyclers are Great"},"type":"lvl1","url":"/blog/2017/2017-01-04-matplotlib-cycles","position":0},{"hierarchy":{"lvl1":"Matplotlib Cyclers are Great"},"content":"Every now and then I come across a nifty feature in Matplotlib that I wish I‚Äôd known about earlier. The MPL documentation can be a beast to get through, and as a result you miss some cool stuff sometimes.\n\nThis is a quick demo of one such feature: the cycler.\n\nHave you ever had to loop through a number of plotting parameters in matplotlib? Say you have two datasets and you‚Äôd like to compare them to one another. Maybe something like this:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport mne\n%matplotlib inline\n\n# Create fake data\ndata1 = np.random.randn(500)\ndata1 = mne.filter.filter_data(data1, 100, None, 3)\ndata2 = data1 + np.random.randn(500) * .1\n\n# Plot\nlinewidths = [6, 1]\ncolors = ['k', 'r']\nalphas = [.3, 1]\nfig, ax = plt.subplots()\nfor i_data, color, lw, alpha in zip([data1, data2], colors, linewidths, alphas):\n    ax.plot(i_data[50:450], c=color, lw=lw, alpha=alpha)\n\nThere‚Äôs really a lot of unnecessary code going on above. We‚Äôre defining objects that share the same name as the kwarg that they represent. We can‚Äôt store them as dictionaries, because then we‚Äôd have to do some python-fu in order to get them to iterate properly. This is where cycler is handy:\n\n# Plot the same thing, but now it's more readable and compact\ncycler = plt.cycler(lw=[6, 1], c=['k', 'r'], alpha=[.3, 1])\nfig, ax = plt.subplots()\nfor i_data, kwargs in zip([data1, data2], cycler):\n    ax.plot(i_data[50:450], **kwargs)\n\nYou can even cycle through more complex properties like colormaps. Let‚Äôs create one that cycles through several colormaps for a plot:\n\ncyc = plt.cycler(s=np.linspace(200, 50, 3),\n                 cmap=['viridis', 'magma', 'coolwarm'],\n                 alpha=[.25, .5, .75],\n                 lw=[0, .1, .5])\n\n# You can print the cycler, or use nice jupyter notebook support\nprint(cyc)\ncyc\n\nfig, ax = plt.subplots()\nfor args in cyc:\n    x, y = np.random.randn(2, 100)\n    ax.scatter(x, y, c=x, **args)\n\nSo there you have it - cyclers are pretty neat. Give them a shot, and buy a matplotlib dev a beer next time you see them for making such an awesome and often under-appreciated package!\n\nTip\n\nYou can check out the MPL cycler page \n\nhere","type":"content","url":"/blog/2017/2017-01-04-matplotlib-cycles","position":1},{"hierarchy":{"lvl1":"Dates in python"},"type":"lvl1","url":"/blog/2017/2017-03-16-dates-in-python","position":0},{"hierarchy":{"lvl1":"Dates in python"},"content":"As a part of setting up the website for the \n\nDocathon I‚Äôve had to re-learn all of my date string formatting rules. It‚Äôs one of those little problems you don‚Äôt really think about - turning an arbitrary string into something structured like a date - until you‚Äôve actually got to do it.\n\nThere are a bunch of tools in python for using date-like objects, but it‚Äôs not always easy to figure out how these work. This post is just a couple of pieces of information I‚Äôve picked up along the process.","type":"content","url":"/blog/2017/2017-03-16-dates-in-python","position":1},{"hierarchy":{"lvl1":"Dates in python","lvl3":"Useful links"},"type":"lvl3","url":"/blog/2017/2017-03-16-dates-in-python#useful-links","position":2},{"hierarchy":{"lvl1":"Dates in python","lvl3":"Useful links"},"content":"Here‚Äôs a list of useful links I‚Äôve picked up, which I‚Äôll mention below:\n\nstrftime.org for remembering how date string formatting works.\n\nThe pandas datetime docs\n\nA list of time zone names\n\nISO 8601 format explanation","type":"content","url":"/blog/2017/2017-03-16-dates-in-python#useful-links","position":3},{"hierarchy":{"lvl1":"Dates in python","lvl2":"A quick tour of our tools"},"type":"lvl2","url":"/blog/2017/2017-03-16-dates-in-python#a-quick-tour-of-our-tools","position":4},{"hierarchy":{"lvl1":"Dates in python","lvl2":"A quick tour of our tools"},"content":"In this post we‚Äôll focus on two main tools for date functionality: the datetime module and pandas.","type":"content","url":"/blog/2017/2017-03-16-dates-in-python#a-quick-tour-of-our-tools","position":5},{"hierarchy":{"lvl1":"Dates in python","lvl3":"Datetime","lvl2":"A quick tour of our tools"},"type":"lvl3","url":"/blog/2017/2017-03-16-dates-in-python#datetime","position":6},{"hierarchy":{"lvl1":"Dates in python","lvl3":"Datetime","lvl2":"A quick tour of our tools"},"content":"\n\nimport numpy as np\nfrom datetime import datetime\nimport pandas as pd\n%matplotlib inline\n\nThe datetime module is our core definition of how to represent dates in python. We can call datetime directly with a bunch of integers as input, representing years, months, days, etc:\n\ndate = datetime(2016, 3, 3, 12, 12, 12)\ndate\n\nWe can access components of this date object as attributes:\n\ndate.day\n\nattrs = ['year', 'month', 'day']\nfor ii in attrs:\n    print('\\n---\\n')\n    print(ii, getattr(date, ii))\n\n","type":"content","url":"/blog/2017/2017-03-16-dates-in-python#datetime","position":7},{"hierarchy":{"lvl1":"Dates in python","lvl4":"Extra components of datetime objects","lvl3":"Datetime","lvl2":"A quick tour of our tools"},"type":"lvl4","url":"/blog/2017/2017-03-16-dates-in-python#extra-components-of-datetime-objects","position":8},{"hierarchy":{"lvl1":"Dates in python","lvl4":"Extra components of datetime objects","lvl3":"Datetime","lvl2":"A quick tour of our tools"},"content":"We can also access specific sub-components of the datetime (that is, either the date or the time)\n\ndate.date()\n\ndate.time()\n\nDatetime objects also have timezones. You can check whether our object already has a timezone with the tzinfo attribute:\n\ndate.tzinfo\n\nAs we can see, there‚Äôs nothing in there because we didn‚Äôt specify a timezone when we created this object. We need to add a timezone first.\n\nHowever, once you start using timezones it is useful to use a more powerful package like pandas. Let‚Äôs do a quick intro:","type":"content","url":"/blog/2017/2017-03-16-dates-in-python#extra-components-of-datetime-objects","position":9},{"hierarchy":{"lvl1":"Dates in python","lvl3":"Datetimes in pandas","lvl2":"A quick tour of our tools"},"type":"lvl3","url":"/blog/2017/2017-03-16-dates-in-python#datetimes-in-pandas","position":10},{"hierarchy":{"lvl1":"Dates in python","lvl3":"Datetimes in pandas","lvl2":"A quick tour of our tools"},"content":"Pandas (among many, many other things) adds some extra functionality on top of datetime. The most useful function for this is probably to_datetime. This will try to be clever about whatever the input is, which lets us do things like give it strings:\n\ndate = pd.to_datetime('2017-03-01')\nprint('Class: ', type(date), '|', date)\n\nNote that this isn‚Äôt the same object as the datetime object, but we can usually treat them similarly. It also exposes several new methods that can do useful things, like changing the timezone of our datetime object:\n\ndate = date.tz_localize('UTC')\ndate\n\nprint('timezone: ', date.tzinfo)\n\nWe used a time zone called ‚ÄúUTC‚Äù. That stands for ‚ÄúCoordinated Universal Time‚Äù, which is basically the global standard for the ‚Äúbase‚Äù time. All other timezones are expressed in reference to this base time. For example, let‚Äôs now convert our time to PST:\n\ndate = date.tz_convert('US/Pacific')\ndate\n\nprint('timezone: ', date.tzinfo)\n\nNotice how we now see the -0800 at the end. This tells us what kind of offset the ‚ÄòUS/Pacific‚Äô timezone is from ‚ÄòUTC‚Äô. It‚Äôs expressed as -mmss, where m = minute and s = second\n\nOne of the really nice things about pandas is that we can represent ranges in python. Let‚Äôs generate a range of dates below:\n\n# The `freq` parameter tells us the size of the jump between items\nfreq = 'H'  # H == hours, D == days\ndates = pd.date_range('2017-03-01', '2017-03-05', freq=freq, tz='UTC')\ndates[:5]\n\nThis lets us do several things, such as create masks for any data we have:\n\ndates < \"2017-03-02\"\n\nIt also lets us plot things across dates:\n\ndates = pd.date_range('2017-01-01', '2017-03-06', freq='H', tz='UTC')\nsr = pd.Series(np.random.randn(len(dates)), index=dates)\nsr.rolling(30).mean().plot()  # Give us a 30 hour rolling mean\n\nNote that if we now change the timezone, some of the days have changed because of the time shift:\n\n# Original times\ndates[:5]\n\n# New times\ndates.tz_convert('US/Pacific')[:5]\n\nFor more information about date objects in pandas, check out:\n\nThe pandas datetime docs\n\nAnd this is useful for figuring out how to convert between timezones:\n\nA list of time zone names\n\n","type":"content","url":"/blog/2017/2017-03-16-dates-in-python#datetimes-in-pandas","position":11},{"hierarchy":{"lvl1":"Dates in python","lvl2":"Parsing strings as a date"},"type":"lvl2","url":"/blog/2017/2017-03-16-dates-in-python#parsing-strings-as-a-date","position":12},{"hierarchy":{"lvl1":"Dates in python","lvl2":"Parsing strings as a date"},"content":"","type":"content","url":"/blog/2017/2017-03-16-dates-in-python#parsing-strings-as-a-date","position":13},{"hierarchy":{"lvl1":"Dates in python","lvl3":"ISO 8601","lvl2":"Parsing strings as a date"},"type":"lvl3","url":"/blog/2017/2017-03-16-dates-in-python#iso-8601","position":14},{"hierarchy":{"lvl1":"Dates in python","lvl3":"ISO 8601","lvl2":"Parsing strings as a date"},"content":"So how is pandas doing all of these string auto-conversions? Well, there are a few string structures that are classically defined as date-like structures. For example, the yyyy-mm-dd string I gave above looks fairly straightforward for a date.\n\nWhat if we wanted to do something more complex with strings? For this, we use the \n\nISO 8601. It takes the following form:\n\nYYYY-MM-DDThh:mm:ss.\n\nMost of these are pretty self-explanatory, but of note is the Z at the end. This simply says that there is zero offset for this datetime, AKA it is ‚ÄúUTC‚Äù time.\n\ndate = pd.to_datetime('2017-03-01T12:34:02Z')\ndate\n\n","type":"content","url":"/blog/2017/2017-03-16-dates-in-python#iso-8601","position":15},{"hierarchy":{"lvl1":"Dates in python","lvl4":"Arbitrary strings","lvl3":"ISO 8601","lvl2":"Parsing strings as a date"},"type":"lvl4","url":"/blog/2017/2017-03-16-dates-in-python#arbitrary-strings","position":16},{"hierarchy":{"lvl1":"Dates in python","lvl4":"Arbitrary strings","lvl3":"ISO 8601","lvl2":"Parsing strings as a date"},"content":"We can also tell pandas how to parse an arbitrary date string. For this, we use placeholders representing different components of the date, and then construct a ‚Äúparsing string‚Äù that we give to the function. This tells the function how to turn input strings into datetime objects. For example, let‚Äôs say we have this string:\n\ndate_string = '02/2016/12'\n\nThis is a little tricky, because it‚Äôs unclear which is the month and which is the day. By specifying a date parsing string, we can easily parse this. The placeholders that we use are in the \n\nstrftime website linked above. Here are some useful ones:\n\n%Y = year\n\n%m = month\n\n%d = day\n\ndate = pd.to_datetime(date_string, format='%m/%Y/%d')\ndate\n\nAbove we told pandas to parse the date string as {month}/{year}/{day}. We can do this with any string structure we like:\n\n# Create a complicated date string\ndate_string = '02/2016/12 and the time is 11 hr 04 min and 02 sec'\n\n# And tell `to_datetime` how to parse it\ndate = pd.to_datetime(date_string,\n                      format='%m/%Y/%d and the time is %H hr %M min and %S sec')\ndate\n\n","type":"content","url":"/blog/2017/2017-03-16-dates-in-python#arbitrary-strings","position":17},{"hierarchy":{"lvl1":"Dates in python","lvl4":"Printing strings","lvl3":"ISO 8601","lvl2":"Parsing strings as a date"},"type":"lvl4","url":"/blog/2017/2017-03-16-dates-in-python#printing-strings","position":18},{"hierarchy":{"lvl1":"Dates in python","lvl4":"Printing strings","lvl3":"ISO 8601","lvl2":"Parsing strings as a date"},"content":"The same principle can be applied to printing strings. Any datetime object has a method called strftime. This lets you create a string from time. You can pass a similar date parsing string to what we used above, and it will tell the object what kind of string to create:\n\ndate.strftime('%Y/%m and %d days')\n\nYou can also use this to do some nice formatting, e.g.:\n\ndate.strftime('%A, %B %d, %Y')\n\nYou can also pass these parsing strings to the format method of python strings. For example:\n\nmystring = \"Today's date is...{:%A, %B %d, %Y}\"\nprint(mystring.format(date))\n\n","type":"content","url":"/blog/2017/2017-03-16-dates-in-python#printing-strings","position":19},{"hierarchy":{"lvl1":"Dates in python","lvl3":"And that‚Äôs it...sort of","lvl2":"Parsing strings as a date"},"type":"lvl3","url":"/blog/2017/2017-03-16-dates-in-python#and-thats-it-sort-of","position":20},{"hierarchy":{"lvl1":"Dates in python","lvl3":"And that‚Äôs it...sort of","lvl2":"Parsing strings as a date"},"content":"In reality this is the tip of the iceberg when it comes to using datetime objects, but this should be enough to get you started. Make sure that any time you read in new data, you double check whether it is timezone encoded. If it isn‚Äôt, then figure out what timezone you can assume (often it is UTC), and encoding it yourself to avoid confusion.\n\nDatetime objects are incredibly useful, but can be a little confusing, so I hope this helps clear some things up!","type":"content","url":"/blog/2017/2017-03-16-dates-in-python#and-thats-it-sort-of","position":21},{"hierarchy":{"lvl1":"Combining dates with analysis visualization in python"},"type":"lvl1","url":"/blog/2017/2017-11-02-dates-multiple-plots","position":0},{"hierarchy":{"lvl1":"Combining dates with analysis visualization in python"},"content":"Sometimes you want to do two things:\n\nPlot a timeseries that handles datetimes in a clever way (e.g., with Pandas or Matplotlib)\n\nPlot some kind of analysis on top of that timeseries.\n\nSounds simple right? It‚Äôs not.\n\nThe reason for this is that plotting libraries don‚Äôt really plot human-readable dates, they convert dates to numbers, then change the xtick labels so that they‚Äôre human readable. This means that if you want to plot something on top of dates, it‚Äôs quite confusing.\n\nTo demonstrate this, let‚Äôs grab the latest stock market prices for a couple companies and\nfit regression lines to them...\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas_datareader.data as web\nimport datetime\nplt.ion()\n\nstart = datetime.datetime(2010, 1, 1)\nend = datetime.datetime(2017, 10, 31)\ncompanies = ['AAPL', 'IBM']\ndata = web.DataReader(companies, 'google', start, end)\n\ndef prep_data(data):\n    data = data.sort_index()\n    data = data.resample('D').mean().copy()\n    data = data.reset_index().dropna()\n    data['High'] -= data['High'].min()\n    return data\n\nfig, ax = plt.subplots(figsize=(12, 6))\nfor company in companies:\n    this_data = prep_data(data.loc[:, :, company])\n    this_data.plot.line('Date', 'High', ax=ax, label=company)\n\nLet‚Äôs say we want to fit a regression line to each stock, should be simple, right?\n\nfig, ax = plt.subplots(figsize=(12, 6))\nfor company in companies:\n    this_data = prep_data(data.loc[:, :, company])\n    this_data.plot.line('Date', 'High', ax=ax, label=company)\n    try:\n        sns.regplot('Date', 'High', data=this_data.query('Date > \"2017-04-01\" and Date < \"2017-09-01\"'))\n    except Exception as ee:\n        print('***ERROR: ',  ee, '***')\n\nWe got an error! That‚Äôs because seaborn was treating the Date column as a number, when in fact it was a datetime object.","type":"content","url":"/blog/2017/2017-11-02-dates-multiple-plots","position":1},{"hierarchy":{"lvl1":"Combining dates with analysis visualization in python","lvl2":"How do we fix this?"},"type":"lvl2","url":"/blog/2017/2017-11-02-dates-multiple-plots#how-do-we-fix-this","position":2},{"hierarchy":{"lvl1":"Combining dates with analysis visualization in python","lvl2":"How do we fix this?"},"content":"To fix this, we need to convert the datetime labels to their ordinal (numeric) representation. There are a number of ways to convert dates to numbers. Fortunately, Matplotlib has a convenience function to convert datetime objects to their numeric representation.\n\nfrom matplotlib.dates import date2num\ndate2num(this_data['Date'][0])\n\n# Here's how we convert the Period object to a date:\nsample_ordinal = this_data['Date'].map(lambda a: date2num(a))\nsample_ordinal.head()\n\nBelow we‚Äôll insert this into our plotting code to see what happens.\n\nfig, ax = plt.subplots(figsize=(12, 6))\nfor ii, company in enumerate(companies):\n    this_data = prep_data(data.loc[:, :, company])\n    this_data['Date_Ord'] = this_data['Date'].map(lambda a: date2num(a))\n    this_data.plot.line('Date', 'High', ax=ax, label=company, color='C{}'.format(ii))\n    sns.regplot('Date_Ord', 'High', data=this_data.query('Date > \"2017-04-01\" and Date < \"2017-09-01\"'),\n                truncate=True, color='C{}'.format(ii))\n\nAnd there you have it - mixed datetime visualization across multiple libraries.\n\nNote that this is not the same thing as plotting an actual\ndatetime object, we‚Äôre plotting a datetime period above, which is the only way I‚Äôve figure\nout how to make this work.\n\nOf course, this is all unnecessarily complicated, and hopefully we‚Äôll see some patches\nthat ease this in the future. But in the meantime you can refer to the above method\nfor your datetime viz needs.","type":"content","url":"/blog/2017/2017-11-02-dates-multiple-plots#how-do-we-fix-this","position":3},{"hierarchy":{"lvl1":"Automatically mirror a github repository with CircleCI"},"type":"lvl1","url":"/blog/2018/circlci-github","position":0},{"hierarchy":{"lvl1":"Automatically mirror a github repository with CircleCI"},"content":"tl;dr: you can automatically mirror the contents of one repository to another by\nusing CI/CD services like CircleCI. This post shows you one way to do it using\nsecrets that let you push to a GitHub repository from a CircleCI process.\n\nWe recently ran into an issue with the Data 8 course where we needed to mirror\none GitHub site to another. In short, the textbook is built with a tool called\n\n\njupyter-book, and we use \n\ngithub-pages\nto host the content at \n\ninferentialthinking‚Äã.com.\nFor \n\nweird URL-naming reasons,\nwe had to create \n\na second organization\nto host the actual site. This introduced the complexity that any time the textbook\nhad to be updated, we did so in two different places. The raw textbook content\nis hosted at \n\nhttps://‚Äãgithub‚Äã.com‚Äã/data‚Äã-8‚Äã/textbook, and the version hosted online is\nat \n\nhttps://‚Äãgithub‚Äã.com‚Äã/inferentialthinking‚Äã/inferentialthinking‚Äã.github‚Äã.io.\n\nThis is a pain, because now a person has to take several actions across two repositories\nany time we update the textbook content. But not anymore! Now we use CirleCI to automatically deploy an update to\n\n\ninferentialthinking‚Äã.com any time a change is made to data-8/textbook. Here\nare the steps to do it.\n\nIn these steps, we‚Äôll have two repositories, one with the ‚Äúprimary‚Äù repository\nwe want to keep, and one with the ‚Äúmirror‚Äù repository that should always contain\nexactly the content of the ‚Äúprimary‚Äù repo. I‚Äôll call these ‚Äúprimary‚Äù and ‚Äúmirror‚Äù\nrepos from here on out (no, I won‚Äôt call them master and slave but that‚Äôs a whole\nother conversation).","type":"content","url":"/blog/2018/circlci-github","position":1},{"hierarchy":{"lvl1":"Automatically mirror a github repository with CircleCI","lvl2":"What we want to do"},"type":"lvl2","url":"/blog/2018/circlci-github#what-we-want-to-do","position":2},{"hierarchy":{"lvl1":"Automatically mirror a github repository with CircleCI","lvl2":"What we want to do"},"content":"Ultimately, we‚Äôd like the following thing to happen:\n\nWhenever a change is pushed to the ‚Äúprimary‚Äù repository, CircleCI should push\nthose changes to the ‚Äúmirror‚Äù repository.\n\nSince CircleCI already lets you run semi-arbitrary code, this is relatively\nstraightforward, with one big caveat: permissions. GitHub doesn‚Äôt let anybody\npush to any repository, so we need some way to allow CircleCI to push to\nour mirror repository. That‚Äôs what these steps are all about.","type":"content","url":"/blog/2018/circlci-github#what-we-want-to-do","position":3},{"hierarchy":{"lvl1":"Automatically mirror a github repository with CircleCI","lvl2":"Step 1: Create an SSH key for your mirror github repository"},"type":"lvl2","url":"/blog/2018/circlci-github#step-1-create-an-ssh-key-for-your-mirror-github-repository","position":4},{"hierarchy":{"lvl1":"Automatically mirror a github repository with CircleCI","lvl2":"Step 1: Create an SSH key for your mirror github repository"},"content":"First off, we need to tell the mirror repository ‚Äúyou should let anyone with\nthese credentials push to the repo‚Äù. We‚Äôll do this by creating a ‚Äúdeploy key‚Äù.\nThis is a SSH public/private key pair that, when combined, will allow anybody\nto push to your repository. If anybody has the private key, they have push access\nto your repo, so keep the private key safe!\n\nFirst, create a new public/private key pair with this command (in a *nix system):ssh-keygen -f ~/key.txt\n\nwhen it asks for a passphrase, simply hit enter twice. This makes the passphrase empty.\n\nThis generates two files in your home directory:\n\nkey.txt is your private key. You should not share this w/ others unless you know what you‚Äôre doing.\n\nkey.txt.pub is your public key. You can share this w/ others.","type":"content","url":"/blog/2018/circlci-github#step-1-create-an-ssh-key-for-your-mirror-github-repository","position":5},{"hierarchy":{"lvl1":"Automatically mirror a github repository with CircleCI","lvl2":"Step 2: Add this SSH key as a ‚Äúdeploy key‚Äù to your mirror repo"},"type":"lvl2","url":"/blog/2018/circlci-github#step-2-add-this-ssh-key-as-a-deploy-key-to-your-mirror-repo","position":6},{"hierarchy":{"lvl1":"Automatically mirror a github repository with CircleCI","lvl2":"Step 2: Add this SSH key as a ‚Äúdeploy key‚Äù to your mirror repo"},"content":"In your github repository, go to settings -> deploy keys -> add deploy key. See the diagram below for the steps:\n\nWhen you click add deploy key, it‚Äôll open an interface for you to add the public key for the\ndeployment permissions. Copy the text of your public key (at ~/key.txt.pub) and paste it in the window like so:\n\nRemember, this is the public key for your repository. This means that anyone\nwith the private key will now be able to push to the repo.","type":"content","url":"/blog/2018/circlci-github#step-2-add-this-ssh-key-as-a-deploy-key-to-your-mirror-repo","position":7},{"hierarchy":{"lvl1":"Automatically mirror a github repository with CircleCI","lvl2":"Step 3: Set up CircleCI for your ‚Äúprimary‚Äù repository"},"type":"lvl2","url":"/blog/2018/circlci-github#step-3-set-up-circleci-for-your-primary-repository","position":8},{"hierarchy":{"lvl1":"Automatically mirror a github repository with CircleCI","lvl2":"Step 3: Set up CircleCI for your ‚Äúprimary‚Äù repository"},"content":"Next, we need to set up CircleCI to build our primary repository with continuous integration.\nI recommend following the \n\nCircleCI getting started guide.\nOnce you follow this, CircleCI will automatically generate new builds for your repository following the configuration\nyou specify in .circleci/config.yml.\n\nHere‚Äôs a good start for a config.yml file:version: 2\njobs:\n  build:\n    branches:\n      only:\n        # Tell Circle only to build this branch\n        - gh-pages\n    docker:\n      # Any Docker image should do, since we only need git\n      - image: circleci/python:3.6-stretch\n    steps:\n      # This ensures that the working directory contains the contents of your repo\n      - checkout\n      # We'll add more steps here\n\nNext, we‚Äôll configure that yaml file to do what we want.","type":"content","url":"/blog/2018/circlci-github#step-3-set-up-circleci-for-your-primary-repository","position":9},{"hierarchy":{"lvl1":"Automatically mirror a github repository with CircleCI","lvl2":"Step 4: Configure CircleCI to push the primary repository to the mirror repository"},"type":"lvl2","url":"/blog/2018/circlci-github#step-4-configure-circleci-to-push-the-primary-repository-to-the-mirror-repository","position":10},{"hierarchy":{"lvl1":"Automatically mirror a github repository with CircleCI","lvl2":"Step 4: Configure CircleCI to push the primary repository to the mirror repository"},"content":"Now that CircleCI is building the primary repo, it‚Äôs time to tell it to do what we want.\nWe‚Äôll modify the workflow in the .circleci/config.yml file to do the following:\n\nadd the mirror repository as a git remote\n\npush the latest copy of master from the primary repository to the mirror repository\n(the latest version of the primary repository is already in the CircleCI build because of the - checkout command).\n\nHere‚Äôs the piece of configuration to do do this:version: 2\njobs:\n  build:\n    steps:\n      # Push to the inferentialthinking.github.io repository so it goes live\n      - run: git remote add live_textbook git@github.com:inferentialthinking/inferentialthinking.github.io.git\n      - run:\n          name: Updating inferentialthinking website\n          command: git push live_textbook gh-pages:master\n\nNow we‚Äôre telling CircleCI to push to our mirror repository, but\nyou‚Äôll notice that it won‚Äôt be able to complete this action. This is because CircleCI doesn‚Äôt\ncurrently have the permissions needed to push to the mirror repository.\nTime to use that public/private key from before!","type":"content","url":"/blog/2018/circlci-github#step-4-configure-circleci-to-push-the-primary-repository-to-the-mirror-repository","position":11},{"hierarchy":{"lvl1":"Automatically mirror a github repository with CircleCI","lvl2":"Step 5: Add your private key to the primary repository Circle CI settings"},"type":"lvl2","url":"/blog/2018/circlci-github#step-5-add-your-private-key-to-the-primary-repository-circle-ci-settings","position":12},{"hierarchy":{"lvl1":"Automatically mirror a github repository with CircleCI","lvl2":"Step 5: Add your private key to the primary repository Circle CI settings"},"content":"Next, we need to give CircleCI the ability to push to our mirror\nrepository by using the public/private key that we generated earlier. Remember\nthat anybody with the private key can push to your mirror\nrepository. We can add the private key in a secure fashion to our\nCircleCI builds using their interface. Go to the settings\npage for your repository within CircleCI, then SSH Permissions -> Add SSH Key.\n\nThis brings up a dialog where you can add your private key. This is the text\ninside ~/key.txt. Copy that text and paste it in the Private Key box.\n\nIn the Hostname box, put github.com.\n\nNow that CircleCI has our private key, we need to configure it to\nuse this key during builds.\n\nI‚Äôd recommend now deleting the key.txt and key.txt.pub files from your computer,\njust to make sure they don‚Äôt accidentally fall in the wrong hands. You can always generate\na new public/private pair and follow the steps above if you need to update the CircleCI deploy\nkeys.","type":"content","url":"/blog/2018/circlci-github#step-5-add-your-private-key-to-the-primary-repository-circle-ci-settings","position":13},{"hierarchy":{"lvl1":"Automatically mirror a github repository with CircleCI","lvl2":"Step 6: Modify your CircleCI configuration to use your private key"},"type":"lvl2","url":"/blog/2018/circlci-github#step-6-modify-your-circleci-configuration-to-use-your-private-key","position":14},{"hierarchy":{"lvl1":"Automatically mirror a github repository with CircleCI","lvl2":"Step 6: Modify your CircleCI configuration to use your private key"},"content":"Finally, we need to modify the yaml configuration so that it knows to use this\npublic/private key combination when it does SSH stuff in the build. That‚Äôs accomplished\nwith the following configuration:jobs:\n  build:\n    steps:\n      # Add deployment key fingerprint for CircleCI to use for a push\n      - add_ssh_keys:\n          fingerprints:\n            # The SSH key fingerprint\n            - \"XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX\"\n\nThe SSH key fingerprint can be found by looking at your GitHub repository‚Äôs ‚ÄúDeploy Keys‚Äù\npage. It‚Äôll contain a SHA that is unique and refers to the key you want. Copy and paste\nit using the configuration structure above. Here‚Äôs what one key looks like:","type":"content","url":"/blog/2018/circlci-github#step-6-modify-your-circleci-configuration-to-use-your-private-key","position":15},{"hierarchy":{"lvl1":"Automatically mirror a github repository with CircleCI","lvl2":"Our final configuration"},"type":"lvl2","url":"/blog/2018/circlci-github#our-final-configuration","position":16},{"hierarchy":{"lvl1":"Automatically mirror a github repository with CircleCI","lvl2":"Our final configuration"},"content":"That should be all we need to allow CircleCI to push the contents from the primary repository\nto the mirror repository. Every time a change is made to the master branch of the primary\nrepository, this process will be triggered.\n\nThis is what our final yaml configuration looks like:version: 2\njobs:\n  build:\n    # Only build for changes to the gh-pages branch\n    branches:\n      only:\n        - gh-pages\n    # The base environment we'll use (can be any docker image w/ git)\n    docker:\n      - image: circleci/python:3.6-stretch\n    steps:\n      # Move the repository code to our home directory in the CircleCI build\n      - checkout\n      # Add deployment key fingerprint for CircleCI to use for a push\n      - add_ssh_keys:\n          fingerprints:\n            - \"XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX\"\n\n      # Add the mirror repository as a git remote\n      - run: git remote add live_textbook git@github.com:inferentialthinking/inferentialthinking.github.io.git\n      # Push the repository to the mirror site\n      - run:\n          name: Updating inferentialthinking website\n          command: git push live_textbook gh-pages:master","type":"content","url":"/blog/2018/circlci-github#our-final-configuration","position":17},{"hierarchy":{"lvl1":"Using CircleCI to preview documentation in Pull Requests"},"type":"lvl1","url":"/blog/2018/circle-docs","position":0},{"hierarchy":{"lvl1":"Using CircleCI to preview documentation in Pull Requests"},"content":"Writing documentation is important - it‚Äôs the first point of contact between many users and your\nproject, and can be a pivotal moment in whether they decide to adopt your tech or become a contributor.\n\nHowever, it can be a pain to iterate on documentation, as it is often involves a lot of rapid iteration\nlocally, followed by a push to GitHub where you ‚Äújust trust‚Äù that the author has done a good job of\nwriting content, design, etc.\n\nA really helpful tip here is to use Continuous Integration to build and preview your documentation. This\nallows you to generate a link to the build docs, which is a much better way of reviewing than looking at\nthe raw text.\n\nHere‚Äôs a simple CircleCI configuration that you can use to build documentation with Sphinx and store it\nas an artifact in the build that you can then preview. To set this up, follow these steps:","type":"content","url":"/blog/2018/circle-docs","position":1},{"hierarchy":{"lvl1":"Using CircleCI to preview documentation in Pull Requests","lvl2":"Configure CircleCI"},"type":"lvl2","url":"/blog/2018/circle-docs#configure-circleci","position":2},{"hierarchy":{"lvl1":"Using CircleCI to preview documentation in Pull Requests","lvl2":"Configure CircleCI"},"content":"First off, you need to configure CircleCI to build your page. This involves creating a file called .circle/config.yml\nthat Circle will use to decide what to do each time your page is built. You then need to go to the CircleCI\nwebsite and tell it to build your site.\n\nHere‚Äôs a skeleton configuration that will build the documentation with Sphinx:version: 2\njobs:\n  # Define a \"build_docs\" job to be run with Circle\n  build_docs:\n    # This is the base environment that Circle will use\n    docker:\n      - image: circleci/python:3.6-stretch\n    steps:\n      # Get our data and merge with upstream\n      - run: sudo apt-get update\n      - checkout\n      # Update our path\n      - run: echo \"export PATH=~/.local/bin:$PATH\" >> $BASH_ENV\n      # Restore cached files to speed things up\n      - restore_cache:\n          keys:\n            - cache-pip\n      # Install the packages needed to build our documentation\n      # This will depend on your particular package!\n      - run: pip install --user sphinx_rtd_theme sphinx pytest memory_profiler recommonmark sphinx_copybutton jupyterhub\n      # Cache some files for a speedup in subsequent builds\n      - save_cache:\n          key: cache-pip\n          paths:\n            - ~/.cache/pip\n      # Build the docs\n      - run:\n          name: Build docs to store\n          command: |\n            cd doc\n            make html\n      # Tell Circle to store the documentation output in a folder that we can access later\n      - store_artifacts:\n          path: doc/_build/html/\n          destination: html\n\n# Tell CircleCI to use this workflow when it builds the site\nworkflows:\n  version: 2\n  default:\n    jobs:\n      - build_docs\n\nSee the comments above for what each step does.","type":"content","url":"/blog/2018/circle-docs#configure-circleci","position":3},{"hierarchy":{"lvl1":"Using CircleCI to preview documentation in Pull Requests","lvl2":"Tell CircleCI to build Pull Requests for your repository"},"type":"lvl2","url":"/blog/2018/circle-docs#tell-circleci-to-build-pull-requests-for-your-repository","position":4},{"hierarchy":{"lvl1":"Using CircleCI to preview documentation in Pull Requests","lvl2":"Tell CircleCI to build Pull Requests for your repository"},"content":"Because we‚Äôre doing this in order to preview changes to the documentation in a Pull Request,\nwe now need to tell CircleCI to run builds on PRs to your repo. To do so, go to the CircleCI UI, click on ‚ÄúJobs‚Äù, then click\nyour project name, then click the settings button here:\n\nIn the next page, click on Advanced Settings, and finally switch on Build forked pull requests.\n\nNow, Circle will build against the PRs of your repository.","type":"content","url":"/blog/2018/circle-docs#tell-circleci-to-build-pull-requests-for-your-repository","position":5},{"hierarchy":{"lvl1":"Using CircleCI to preview documentation in Pull Requests","lvl2":"Make a Pull Request"},"type":"lvl2","url":"/blog/2018/circle-docs#make-a-pull-request","position":6},{"hierarchy":{"lvl1":"Using CircleCI to preview documentation in Pull Requests","lvl2":"Make a Pull Request"},"content":"Now it‚Äôs time to test things out. Make a Pull Request for your repository. GitHub should automatically\ndetect a CircleCI configuration, and run the job with the configuration you‚Äôve specified.\n\nOnce the documentation is built (or if it fails) you can click on the CircleCI link from the GitHub UI\nin order to see what happened.","type":"content","url":"/blog/2018/circle-docs#make-a-pull-request","position":7},{"hierarchy":{"lvl1":"Using CircleCI to preview documentation in Pull Requests","lvl2":"View your artifacts"},"type":"lvl2","url":"/blog/2018/circle-docs#view-your-artifacts","position":8},{"hierarchy":{"lvl1":"Using CircleCI to preview documentation in Pull Requests","lvl2":"View your artifacts"},"content":"You should be taken to a page that shows a summary of the recent CircleCI build for this PR.\n\nIf your documentation successfully built (and if you‚Äôve told Sphinx to put the built site in doc/_build/) then\nyou can now click on the Artifacts tab. You should see a drop-down list of artifacts that CircleCI has\nstored for you. Click on index.html and you should see a preview of your built documentation:\n\nAnd that‚Äôs it! Obviously you can configure CircleCI in many more ways, but this is just a barebones example\nto get you started. I hope you‚Äôve found it useful!","type":"content","url":"/blog/2018/circle-docs#view-your-artifacts","position":9},{"hierarchy":{"lvl1":"Summer conference report back"},"type":"lvl1","url":"/blog/2018/conferences-summer-2018","position":0},{"hierarchy":{"lvl1":"Summer conference report back"},"content":"This is a short update on several of the conferences and workshops over the\nsummer of this year. There‚Äôs all kinds of exciting things going on in open\nsource and open communities, so this is a quick way for me to collect my\nthoughts on some things I‚Äôve learned this summer.","type":"content","url":"/blog/2018/conferences-summer-2018","position":1},{"hierarchy":{"lvl1":"Summer conference report back","lvl2":"SciPy"},"type":"lvl2","url":"/blog/2018/conferences-summer-2018#scipy","position":2},{"hierarchy":{"lvl1":"Summer conference report back","lvl2":"SciPy"},"content":"","type":"content","url":"/blog/2018/conferences-summer-2018#scipy","position":3},{"hierarchy":{"lvl1":"Summer conference report back","lvl3":"The Pangeo project demoed their JupyterHub for big-data geoscience","lvl2":"SciPy"},"type":"lvl3","url":"/blog/2018/conferences-summer-2018#the-pangeo-project-demoed-their-jupyterhub-for-big-data-geoscience","position":4},{"hierarchy":{"lvl1":"Summer conference report back","lvl3":"The Pangeo project demoed their JupyterHub for big-data geoscience","lvl2":"SciPy"},"content":"Pangeo is a project that provides\naccess to a gigantic geosciences dataset. They use lots of tools in the\nopen-source community, including Dask for efficient numerical computation,\nthe SciPy stack for a bunch of data analytics, and JupyterHub on\nKubernetes for managing user instances and deploying on remote infrastructure.\nPangeo has a neat demo of their hosted JupyterHub instance that people can use\nto access this otherwise-inaccessible dataset! See their video from SciPy below.","type":"content","url":"/blog/2018/conferences-summer-2018#the-pangeo-project-demoed-their-jupyterhub-for-big-data-geoscience","position":5},{"hierarchy":{"lvl1":"Summer conference report back","lvl3":"Wholetale shared some ideas on getting data to work with reproducible pipelines","lvl2":"SciPy"},"type":"lvl3","url":"/blog/2018/conferences-summer-2018#wholetale-shared-some-ideas-on-getting-data-to-work-with-reproducible-pipelines","position":6},{"hierarchy":{"lvl1":"Summer conference report back","lvl3":"Wholetale shared some ideas on getting data to work with reproducible pipelines","lvl2":"SciPy"},"content":"Wholetale is a collection\nof technology that makes it easier to do reproducible work. It is NSF-funded,\nso tries to be fairly open about how it interfaces with the ecosystem around it.\nThey Wholetale team gave an interesting talk about how to handle data in\nreproducible environments. This is a big unsolved problem in the space, since\ndatasets are often difficult to ship around or copy, and it doesn‚Äôt usually make\nsense to bake them into things like container images.\n\nCheck out their presentation below","type":"content","url":"/blog/2018/conferences-summer-2018#wholetale-shared-some-ideas-on-getting-data-to-work-with-reproducible-pipelines","position":7},{"hierarchy":{"lvl1":"Summer conference report back","lvl3":"Binder 2.0 was unveiled!","lvl2":"SciPy"},"type":"lvl3","url":"/blog/2018/conferences-summer-2018#binder-2-0-was-unveiled","position":8},{"hierarchy":{"lvl1":"Summer conference report back","lvl3":"Binder 2.0 was unveiled!","lvl2":"SciPy"},"content":"Min Ragan-Kelley and I presented Binder 2.0 on\nbehalf of the Binder community. We talked a bit about the philosophy behind Binder,\nhow we connect with the broader open-source ecosystem (hint: BinderHub is 70% jupyterhub),\ndove into the technical pieces a bit, and laid out some ideas for how Binder can\ngrow in the future. We‚Äôd love to see a world where there are BinderHubs all over\nthe scientific landscape (e.g. ‚Äú\n\nbinder.berkeley.edu‚Äù) that users can select based\non their institutional affiliation or address.\n\nIn addition, the Binder team collaborated on¬†\n\na ‚ÄúBinder 2.0‚Äù\npaper,\nwhich will be published in the proceedings from SciPy. Hooray citable\nresearch artifacts!\n\nHere‚Äôs our Binder talk:","type":"content","url":"/blog/2018/conferences-summer-2018#binder-2-0-was-unveiled","position":9},{"hierarchy":{"lvl1":"Summer conference report back","lvl2":"PEARC"},"type":"lvl2","url":"/blog/2018/conferences-summer-2018#pearc","position":10},{"hierarchy":{"lvl1":"Summer conference report back","lvl2":"PEARC"},"content":"PEARC is a scientific and research computing conference held each year. It‚Äôs\ninteresting because it exists at the intersection of a few different communities.\nAs the scientific world has become more ‚Äúdata-intensive‚Äù, new parts of the academy\noverlap with technical infrastructure that was traditionally just for physics/\nastronomy/simulation kinds of folks. PEARC is (sort of) where a lot of ‚Äúold school‚Äù\nand ‚Äúnew school‚Äù parts of research infrastructure intersect.","type":"content","url":"/blog/2018/conferences-summer-2018#pearc","position":11},{"hierarchy":{"lvl1":"Summer conference report back","lvl3":"Lots of high-performance computing centers use JupyterHub","lvl2":"PEARC"},"type":"lvl3","url":"/blog/2018/conferences-summer-2018#lots-of-high-performance-computing-centers-use-jupyterhub","position":12},{"hierarchy":{"lvl1":"Summer conference report back","lvl3":"Lots of high-performance computing centers use JupyterHub","lvl2":"PEARC"},"content":"Perhaps the most exciting thing I noticed was how much chatter is in the HPC\nworld around JupyterHub on high-performance compute. I think this makes sense,\nsince JupyterHub provides an interface people are familiar with, and obviates\nthe need to be a skilled computer scientist just to interact with high-performance\nhardware.  In particular, the \n\nMinnesota Supercomputing Institute\nand the \n\nNational Energy Research Scientific Computing Center\nshowed some promising examples of providing access to large-scale compute via an\ninteractive environment that was hosted by JupyterHub.","type":"content","url":"/blog/2018/conferences-summer-2018#lots-of-high-performance-computing-centers-use-jupyterhub","position":13},{"hierarchy":{"lvl1":"Summer conference report back","lvl3":"HPC centers are curious about Kubernetes","lvl2":"PEARC"},"type":"lvl3","url":"/blog/2018/conferences-summer-2018#hpc-centers-are-curious-about-kubernetes","position":14},{"hierarchy":{"lvl1":"Summer conference report back","lvl3":"HPC centers are curious about Kubernetes","lvl2":"PEARC"},"content":"There was also a lot of talk about Kubernetes at the conference, though it was\nclear that many people simply didn‚Äôt have the experience with it to know whether\nit was a ‚Äúgood‚Äù option or not. Systems administrators tend to be lower-case ‚Äúc‚Äù\nconservative when it comes to technology, and it seems that this community\nis lagging behind the tech world by many years when it comes to the adoption\nof ‚Äúcloud‚Äù technologies. Hopefully we‚Äôll see more experiments with Kubernetes\nin the HPC world, and that people report back their experiences as this happens.","type":"content","url":"/blog/2018/conferences-summer-2018#hpc-centers-are-curious-about-kubernetes","position":15},{"hierarchy":{"lvl1":"Summer conference report back","lvl3":"Research computing has a weird relationship with cloud companies","lvl2":"PEARC"},"type":"lvl3","url":"/blog/2018/conferences-summer-2018#research-computing-has-a-weird-relationship-with-cloud-companies","position":16},{"hierarchy":{"lvl1":"Summer conference report back","lvl3":"Research computing has a weird relationship with cloud companies","lvl2":"PEARC"},"content":"A lot of research computing folks also expressed some combination of excitement\nand hesitation at outsourcing their computing to large-scale cloud companies\n(such as Google, Amazon, or Microsoft). On the one hand, paying Google to\nrun your compute means you can accomplish some things for less money. On the other,\nit‚Äôs hard to avoid building an institutional dependency on a single company‚Äôs\ntechnical infrastructure. Once you‚Äôve got that 100 petabyte dataset hosted with\none company, it‚Äôll probably be staying there for quite a long time. All the cloud\ncompanies are offering ‚Äúfree‚Äù services to cool-sounding research projects right now,\nbut never forget that many of these companies have built business models on converting\n‚Äúfree-tier‚Äù customers into locked-in paying customers. A few HPC people shared\nstories along the lines of ‚Äúwell they said it would be free to host our scientific\ndata, until they started sending us bills for it several years later.‚Äù","type":"content","url":"/blog/2018/conferences-summer-2018#research-computing-has-a-weird-relationship-with-cloud-companies","position":17},{"hierarchy":{"lvl1":"Summer conference report back","lvl3":"We gave a JupyterHub and BinderHub on Kubernetes tutorial","lvl2":"PEARC"},"type":"lvl3","url":"/blog/2018/conferences-summer-2018#we-gave-a-jupyterhub-and-binderhub-on-kubernetes-tutorial","position":18},{"hierarchy":{"lvl1":"Summer conference report back","lvl3":"We gave a JupyterHub and BinderHub on Kubernetes tutorial","lvl2":"PEARC"},"content":"Last week, Aaron Culich, Jessica Forde, Felix-Antoine Fortin, and I presented a\nday-long tutorial on deploying JupyterHub and BinderHub with Kubernetes. You can\n\n\nfind a copy of our slides here. In particular, I‚Äôm\nexcited to see some improvement in the methods for deploying Kubernetes on\nOpenStack. OpenStack is much more common in scientific research, and exists\non pre-existing hardware. Deploying Kubernetes on OpenStack would open up this\ntechnology to the academic community in a way that won‚Äôt happen with cloud\ncompanies any time soon. In particular, Felix shared \n\nEtienne‚Äôs terraform scripts\nto deploy Kubernetes on OpenStack.\nI‚Äôd love to see people try this out and report back their experiences.","type":"content","url":"/blog/2018/conferences-summer-2018#we-gave-a-jupyterhub-and-binderhub-on-kubernetes-tutorial","position":19},{"hierarchy":{"lvl1":"Summer conference report back","lvl2":"Open Source Alliance for Open Scholarship"},"type":"lvl2","url":"/blog/2018/conferences-summer-2018#open-source-alliance-for-open-scholarship","position":20},{"hierarchy":{"lvl1":"Summer conference report back","lvl2":"Open Source Alliance for Open Scholarship"},"content":"Finally, I attended¬†\n\na meeting in New York\nCity¬†organized\nby¬†\n\nOSAOS. What‚Äôs OSAOS? My take is that it‚Äôs an\norganization dedicated to building connections, best-practices, and leadership\nbetween organizations in open scholarship (AKA, organizations that operate on\nopen principles with a goal of creating and sharing knowledge). More than anything,\nI am thrilled to see more interest in the ‚Äúsoft skills‚Äù side of open communities.\nIt‚Äôs easy to treat all of these things as technical projects, but in my experience\nthe biggest challenges are social and systemic, and won‚Äôt be solved with code.\nHowever, leaders of open groups are often isolated, underappreciated, and don‚Äôt\nhave a lot of training in how to lead communities and projects. It‚Äôs great to see\norganizations such as OSAOS trying to improve this!","type":"content","url":"/blog/2018/conferences-summer-2018#open-source-alliance-for-open-scholarship","position":21},{"hierarchy":{"lvl1":"An academic scientist goes to DevOps Days"},"type":"lvl1","url":"/blog/2018/devopsdays-sv-2018","position":0},{"hierarchy":{"lvl1":"An academic scientist goes to DevOps Days"},"content":"Last week I took a few days to attend \n\nDevOpsDays Silicon Valley. My goal\nwas to learn a bit about how the DevOps culture works, what are the things\npeople are excited about and discuss in this community. I‚Äôm also interested in\nlearning a thing or two that could be brought back into the scientific / academic world.\nHere are a couple of thoughts from the experience.\n\ntl;dr: DevOps is more about culture and team process than it is about technology, maybe science should be too...","type":"content","url":"/blog/2018/devopsdays-sv-2018","position":1},{"hierarchy":{"lvl1":"An academic scientist goes to DevOps Days","lvl2":"What is DevOps anyway?"},"type":"lvl2","url":"/blog/2018/devopsdays-sv-2018#what-is-devops-anyway","position":2},{"hierarchy":{"lvl1":"An academic scientist goes to DevOps Days","lvl2":"What is DevOps anyway?"},"content":"This one is going to be hard to define (\n\nthough here‚Äôs one definition),\nas I‚Äôm new to the community as well.\nBut, my take on this is that DevOps is a coming-together of what was once\na bunch of different roles within companies. The process of releasing technology\n(or doing anything really) involves many different steps with different specializations\nneeded at each step. The ‚Äòold way‚Äô of doing things involved teams that‚Äôd build\nprototypes, teams that would adapt those prototypes to a company‚Äôs infrastructure,\nteams that would maintain and service the ‚Äúproduction‚Äù deployments, etc. The whole\nprocess was quite slow, partially because of the lack of communication between these\nvery different kinds of groups.\n\nInstead, DevOps attempts to encapsulate this entire process under one moniker. It is\ngenerally recognized that people do have different skills and roles, but they should\nbe working together in a group to create, mature, and ship new code iteratively\nand as a single continuous process. DevOps is intently focused on ‚Äú\n\nagile processes‚Äù,\nand values being quick and lightweight, focusing on metrics like ‚Äútime between development\nand deployment.‚Äù This is only possible with a large focus on team dynamics and\nhow the relationships between people with different skillsets and responsibilities\nshould work with one another effectively. Perhaps unsurprisingly, this is the\nkind of thing that people talk about a lot at DevOps conferences (well, at least\nat the one I went to).","type":"content","url":"/blog/2018/devopsdays-sv-2018#what-is-devops-anyway","position":3},{"hierarchy":{"lvl1":"An academic scientist goes to DevOps Days","lvl2":"DevOpsDays was more about people than tech"},"type":"lvl2","url":"/blog/2018/devopsdays-sv-2018#devopsdays-was-more-about-people-than-tech","position":4},{"hierarchy":{"lvl1":"An academic scientist goes to DevOps Days","lvl2":"DevOpsDays was more about people than tech"},"content":"More than anything else, what struck me was how little emphasis was paid on\nthe technology itself. There are a billion moving parts in the cloud orchestration\nand container technology space, but they got relatively little discussion time at\nthe conference (with the exception of \n\nJennelle Crothers\ntalking about how Microsoft was trying to make its Windows containers super lightweight,\nwhich was pretty neat). In general the only people consistently talking about\nthe greatness of XXX new software/tool/etc were salespeople trying to\nget you to buy their product. Instead, the vast majority of conversations,\ndiscussions, brainstorms, etc were about people and process, not\ntechnology per-se.\n\nObviously, it‚Äôs difficult to disentangle the tech from the people when\nyou‚Äôre in the tech industry, but it was illustrative to see the\nrelative focus that got placed on the ‚Äúsquishier‚Äù questions. For example,\na few that came up pretty frequently:\n\nHow can we disseminate information across a distributed team most effectively?\n\nHow can we create a team culture that welcomes newcomers?\n\nHow can we avoid alienating members of the team?\n\nHow can we avoid single points of failure in the team?\n\nHow can we do things faster, more efficiently, and more reliably as a team?\n\nIt‚Äôs no coincidence that the word ‚Äúteam‚Äù was in each of the bullet points above.","type":"content","url":"/blog/2018/devopsdays-sv-2018#devopsdays-was-more-about-people-than-tech","position":5},{"hierarchy":{"lvl1":"An academic scientist goes to DevOps Days","lvl2":"Scientific DevOps?"},"type":"lvl2","url":"/blog/2018/devopsdays-sv-2018#scientific-devops","position":6},{"hierarchy":{"lvl1":"An academic scientist goes to DevOps Days","lvl2":"Scientific DevOps?"},"content":"As a member of the scientific / academic community, this is quite interesting\nto me. A whole conference where people talk about creating positive culture and effective\nyeams? Yes please. However, I realize that the incentives and\nproblems associated with DevOps are not the same as those faced by scientists. For\nexample, a big part of DevOps (and SREs more generally) is ensuring that services,\ntools, and sites are reliable and stable over time. You can design tech around this\nidea all you want, but at some point you‚Äôll need a team of people to manage that tech.\nThe DevOps world has seemed to realize that this means the social dynamics of that\nteam are just as important as the technology itself, which is a breath of fresh\nair.\n\nI wonder what scientific DevOps would look like. Scientists are theoretically also\noperating in team-based environments (at least, the ones in scientific labs). The\nincentives of reward and recognition are totally misaligned, but it‚Äôs still the\ncase that successful teams produce more effective work in general. Perhaps it‚Äôs\nworth exploring how the DevOps take on operations and team dynamics ports to\nthe academic scientific community.","type":"content","url":"/blog/2018/devopsdays-sv-2018#scientific-devops","position":7},{"hierarchy":{"lvl1":"An academic scientist goes to DevOps Days","lvl2":"Open and (relatively) diverse culture"},"type":"lvl2","url":"/blog/2018/devopsdays-sv-2018#open-and-relatively-diverse-culture","position":8},{"hierarchy":{"lvl1":"An academic scientist goes to DevOps Days","lvl2":"Open and (relatively) diverse culture"},"content":"A final point that I noticed was that, relative to other tech conferences I‚Äôve\nattended, this one had a general air of positivity and open culture. There were\nall kinds of people there, and while the general makeup of attendees definitely\nstill had a lot of white dudes in it, the room nonetheless never felt like it was\ndominated by this group of people. There was also a great culture of supporting\npeople as they were giving talks - some of the speakers were clearly more nervous\nthan others, and the audience did a good job of trying to disarm their anxiety.\nPerhaps this is the kind of culture that comes with a profession that depends on\n(and focuses on) team dynamics and culture.\n\nUltimately, as with any good conference, I left having more questions than answers.\nHow can scientists improve their own team dynamics using principles from the\nDevOps community? How can the open-source community do the same but for distributed team\nworkloads and responsibility? Where is the balance between ‚Äúsolve this with tech‚Äù and\n‚Äúsolve this with people‚Äù? How can we encourage more cross-talk between the world of\nscientific research and the world of tech? Either way, it was an interesting and\ninformative experience, and I‚Äôm looking forward to learning more about this community.","type":"content","url":"/blog/2018/devopsdays-sv-2018#open-and-relatively-diverse-culture","position":9},{"hierarchy":{"lvl1":"An academic scientist goes to DevOps Days","lvl2":"Highlights and takeaways"},"type":"lvl2","url":"/blog/2018/devopsdays-sv-2018#highlights-and-takeaways","position":10},{"hierarchy":{"lvl1":"An academic scientist goes to DevOps Days","lvl2":"Highlights and takeaways"},"content":"Amy Nguyen and Adam Barber shared their strategies for taking a \n\ndata-driven approach to user interface / experience design.\n\nAlways collect data from people you design things for.\n\nPut in the legwork to interview and gather diverse perspectives before you build tools.\n\nFatema Boxwala (\n\nfatty_box) shared her experience as an intern, and gave some pointers for how to create a welcoming and productive environment for intern positions.\n\nDon‚Äôt forget that interns have lives too. If they just moved to a new city, help them settle in.\n\nAlign your intern‚Äôs project with something a team member (or yourself) will be actively work on.\n\nKaty Farmer (\n\nTheKaterTot) reminded everybody that teams shouldn‚Äôt automatically aspire to use the workflows that gigantic tech companies use.\n\nJust because it works for a big tech company doesn‚Äôt mean it‚Äôll work for you.\n\nBeing a smaller-sized company isn‚Äôt better or worse, it‚Äôs just different. Don‚Äôt treat company size as a reflection of quality, and don‚Äôt assume larger companies do operations more effectively.\n\nFrances Hocutt reassured everybody that it‚Äôs OK if your tests wouldn‚Äôt satisfy a production-level standard all the time.\n\nMake sure your tests reflect the current state of your code.\n\nDon‚Äôt let perfect be an enemy of good. Testing 10% of the code is still better than testing 0%.\n\nJennelle Crothers (\n\njkc137) described how Windows is trying to shrink images that run Windows so that you can run them in containers more easily.\n\nIt turns out that shrinking something from several GB to a few hundred MB makes it much easier to ship around :-)\n\nAs an aside, it‚Äôs fascinating to see Microsoft focus on integrating itself with the container ecosystem (as opposed to trying to replace or compete with it). Maybe they really have learned something from their ‚Äú\n\nlinux is a cancer‚Äù debacle.\n\nAdrian Cockcroft (\n\nadrianco) explained the importance of creating a \n\nculture of reporting and logging incidents, even the small ones!\n\nInstrument and study the ‚Äúnon-events‚Äù - look for ‚Äúnear-misses‚Äù and outliers. Never throw away information just because something catastrophic didn‚Äôt happen.\n\nPlan and practice for chaos! What will your team do if ‚Äúeverything goes wrong‚Äù?","type":"content","url":"/blog/2018/devopsdays-sv-2018#highlights-and-takeaways","position":11},{"hierarchy":{"lvl1":"Open communities need to be partners, not sources of free labor"},"type":"lvl1","url":"/blog/2018/free-labor-partners","position":0},{"hierarchy":{"lvl1":"Open communities need to be partners, not sources of free labor"},"content":"In the last couple of years, we‚Äôve seen an increasing number of organizations start to\nspawn products that take a largely open stack (e.g., the SciPy ecosystem) and wrap\nit in a thin layer of proprietary/custom interface + infrastructure.\nOn the face of it, this isn‚Äôt a problem - I really want people to be able to\nmake money using the open source stack - however, there is a big caveat. When you look\nat the work that those organizations have done over time, you often see a pretty thin trail\nof contributions back to those open source projects.\n\nI‚Äôd argue that using an open community‚Äôs software without contributing back is straight-up\nexploitative (legal, sure, but still exploitative), and we should think about ways to\nsuppress this kind of behavior. This post is a collection of thoughts on that topic.\n\nBut first....","type":"content","url":"/blog/2018/free-labor-partners","position":1},{"hierarchy":{"lvl1":"Open communities need to be partners, not sources of free labor","lvl2":"Why should I care whether I‚Äôm being a ‚Äúgood citizen in the open community‚Äù?"},"type":"lvl2","url":"/blog/2018/free-labor-partners#why-should-i-care-whether-im-being-a-good-citizen-in-the-open-community","position":2},{"hierarchy":{"lvl1":"Open communities need to be partners, not sources of free labor","lvl2":"Why should I care whether I‚Äôm being a ‚Äúgood citizen in the open community‚Äù?"},"content":"Organizations are driven by incentives, and ultimately if a behavior isn‚Äôt conducive\nto the organization‚Äôs bottom-line, then we can‚Äôt expect them to adopt that behavior.\nI‚Äôd argue that interacting with the open community has a number of bottom-line benefits, here are a few:\n\nIt makes you a more attractive place to work. Operating in open communities is\noften where people learn their technical skills, and while people come and go through\norganizations, the open community remains. Employees want to interact in these communities,\nand employers that have a good relationship with open communities are more attractive\nplaces to work.\n\nOpen communities can be a great training resource. Have some improvements to make in\nyour Python or R skills? Need to tool-up on your ability to write CI/CD pipelines? Open\ncommunities are a great place to get feedback and guidance from others with more experience.\nThe open community is a resource for constantly tooling-up your technical skills. Use it!\n\nOpen communities can make better team-members. Making complex technical/social decisions\nwith a diverse community distributed across the world is really difficult! For the communities\nthat do this well, interacting with them can be a fantastic learning experience in team-work,\ncommunication, coalition-building, critical feedback, and leadership.\n\nInteracting with open communities gives you influence. If you‚Äôre using an open\ntool, don‚Äôt you want to have a say in how that tool grows and evolves? Being a passive consumer\nof open technology means you‚Äôre at the whim of that community‚Äôs wishes. If you‚Äôre a part of\nthat community, you can influence its direction to align with your organization‚Äôs goals.\n\nInteracting with open communities means you write less code. I suspect that in the long-term,\nwriting your own code will almost cost more than using somebody else‚Äôs code (in terms of\nperson-hours it requires). If you‚Äôve got a say in an open project, you minimize the chance that\nyou must either maintain an internal fork w/ significant changes, or create your own new thing\nthat you must now maintain on your own.\n\nInteracting with open communities can make them more robust. Finally, let‚Äôs not forget\nthat communities can and do fail all the time. What happens if a project you depend on\nstops making bug fixes? Or doesn‚Äôt have bandwidth to maintain security patches? By contributing\nresources to these communities, we keep them thriving and healthy creators of open-source\ntools. We all benefit from this, and more importantly you don‚Äôt have a dead project that your\ncore product or teams depend on.\n\nThose are just a few benefits, but I‚Äôm sure there are others. For more information and ideas about\nhow open source communities can be a benefit to your organization, I recommend Mozilla‚Äôs\n\n\nOpen Source Archetypes\nresearch paper.\n\nNow that we‚Äôve got that out\nof the way, let‚Äôs move on to deciding whether or not your organization is, in fact, being\na positive actor in the open-source community.","type":"content","url":"/blog/2018/free-labor-partners#why-should-i-care-whether-im-being-a-good-citizen-in-the-open-community","position":3},{"hierarchy":{"lvl1":"Open communities need to be partners, not sources of free labor","lvl2":"How can I tell if my organization is not being a good citizen in the open community?"},"type":"lvl2","url":"/blog/2018/free-labor-partners#how-can-i-tell-if-my-organization-is-not-being-a-good-citizen-in-the-open-community","position":4},{"hierarchy":{"lvl1":"Open communities need to be partners, not sources of free labor","lvl2":"How can I tell if my organization is not being a good citizen in the open community?"},"content":"I suspect that many organizations simply haven‚Äôt put a ton of thought into whether they‚Äôve got a positive-or-negative\nrelationship with the open communities that intersect with their work. Here‚Äôs a quick set of questions\nto ask yourself:\n\nDo your employees routinely use open-source software in their work?\n\nDoes your organization create a product that depends on open-source software?\n\nDoes your organization depend on proprietary software that depends on open-source software?\n\nIf you answered ‚Äúyes‚Äù to any of those questions, then ask yourself the following questions:\n\nDo you have explicit policies that encourage employees to contribute back to open projects?\n\nDo you have explicit funding mechanisms to give resources to open projects?\n\nDo you explicitly call out and acknowledge the importance of those open tools in your presentations and (some kinds of) marketing?\n\nDo you have regular, open channels of communication with those open communities?\n\nDo these behaviors scale with the amount of value you derive from open communities?\n\nIf you answered ‚Äúno‚Äù to these questions, then you‚Äôre probably exploiting the open community.\nYou should stop that!\n\nHowever, avoiding this kind of one-way relationship with the open source community is complex\nand requires some new efforts and thinking from both sides of the equation.\nThe rest of this post includes a few ideas on how open projects, as well as organizations,\ncan improve this relationship.","type":"content","url":"/blog/2018/free-labor-partners#how-can-i-tell-if-my-organization-is-not-being-a-good-citizen-in-the-open-community","position":5},{"hierarchy":{"lvl1":"Open communities need to be partners, not sources of free labor","lvl2":"How can open communities encourage organizations to be good citizens"},"type":"lvl2","url":"/blog/2018/free-labor-partners#how-can-open-communities-encourage-organizations-to-be-good-citizens","position":6},{"hierarchy":{"lvl1":"Open communities need to be partners, not sources of free labor","lvl2":"How can open communities encourage organizations to be good citizens"},"content":"How can the open community encourage better interactions\nwith organizations that depend on our tools?\n\nFirst off, I think these are not problems that we should have to solve with licensing [1]\n. These are social, moral, and incentives problems.\nThere are plenty of organizations that make money from open-source, but that still\nmanage to spend time being part of the communities that they draw value from. How do we encourage this\nkind of behavior? I have a few ideas, but we should spend more time thinking about this:\n\nOpen communities need more ways to signal ‚Äúthank you‚Äù to organization that behave well. For\nexample, the Kubernetes community runs a \n\nservice called stackalytics\nthat lists contributions broken down by company. It would be great to adapt this kind of visualization\nfor the broader OSS community. More generally, \"thank-you\"s should be for specific behavior, and it should be clear whether that behavior is a one-off or is on-going. Organizations are incentive-driven, so we should create more\npositive incentives for them to interact.\n\nOpen communities need a better vocabulary to describe bad behavior when it exists. I‚Äôve had\na number of conversations with folks that are frustrated when an organization uses their tool without\ngiving back. These people often feel like there is no medium through which they can ‚Äúcall-out‚Äù the\noffending organization without sounding ‚Äúwhiny‚Äù, and so they internalize this stress. If there were a way to\nquickly describe what this behavior is, it could be easier for people to call it when they see it.\n\nOpen communities need to make it clear how organizations can get involved. This is a two-way street.\nWhile we need more participation from organizations making products or services around open-source, we also\nneed to position our open communities so that there are clear pathways for interaction and contribution.\nThings like roadmaps, clear community governance, and well-tuned practices around encouraging outsiders to\njoin the community would go a long way.\n\nFunding bodies should consider open-source participation when giving out money. Perhaps this one only\napplies to some sub-fields, but private philanthropies or public funding bodies (like the NSF) should consider\na project‚Äôs intersection with the pre-existing open community when deciding whether to fund a project.\nIf you propose creating a new tool, you should be required to do due-diligence on other tools in that space,\nand explain why you won‚Äôt just contribute to those communities instead of building your own. If you propose\nbuilding a product off of open-source software, you should have a plan for how you‚Äôll contribute back to those\ncommunities. Saying that you‚Äôll open-source your code and throw it over the wall is not enough.","type":"content","url":"/blog/2018/free-labor-partners#how-can-open-communities-encourage-organizations-to-be-good-citizens","position":7},{"hierarchy":{"lvl1":"Open communities need to be partners, not sources of free labor","lvl2":"What could organizations do now to support open communities?"},"type":"lvl2","url":"/blog/2018/free-labor-partners#what-could-organizations-do-now-to-support-open-communities","position":8},{"hierarchy":{"lvl1":"Open communities need to be partners, not sources of free labor","lvl2":"What could organizations do now to support open communities?"},"content":"All of the steps above are fairly long-term solutions to a problem that exists right now. What are some things\nthat organizations can do now in order to make sure they‚Äôre perceived as more positive actors in open communities?\nHere‚Äôs a short ‚Äúoff the top of my head‚Äù list.\n\nIf you‚Äôre an employee, find allies and pressure management to support open-source software. Often, organizations\ncontribute to open source only because employees make a case for it.\n\nIf you‚Äôre at management-level, create space for your employees to contribute to open-source software. They‚Äôll\nbe happier employees (OSS is a gratifying experience), you‚Äôll have more say in the tools you use, and you‚Äôll be\na more attractive place to work for developers.\n\nIf you‚Äôre at the executive / strategic level, incorporate an open-source strategy in your business plan. Open source\ncan be an incredible resource if harnessed properly. Your organization will be better off in the long term if you treat\nopen communities as partners rather than sources of free labor.\n\nIf you‚Äôre creating a new open-source tool, write a justification for why you‚Äôre creating a new thing, instead of contributing to an existing thing.\nSometimes there‚Äôs a great reason not to jump on-board with a pre-existing tool. However, you need to signal that you‚Äôve\nthought hard about this, and recognize the downside to creating yet-another open source tool when alternatives exist.\n\nIf you‚Äôve got a product that directly uses open-source, have an open-source plan for giving back. Signal to the\ncommunity what you‚Äôre doing to say ‚Äúthank you‚Äù for all of the value you‚Äôre getting from the open community. Maybe it‚Äôs\npeople‚Äôs time to contribute back, maybe it‚Äôs money, maybe it‚Äôs marketing for OSS communities.\n\nWrite an annual ‚Äúcontributing back to open source‚Äù report that details the ways you‚Äôve contributed back. It‚Äôll highlight your organization‚Äôs role as a leader in this space, and signal the value that these tools provide for you.\n\nThat‚Äôs it for now, though as you can probably tell, this is a complex topic with a lot of nuance. If you‚Äôve got any thoughts of your own, feel free to leave a comment below, or \n\nreach out to me on twitter\n\n[1] A note on licenses\n\nWhen I talk about this stuff, people often mention copyleft licenses as an option. Basically,\nthis means releasing something under a permissive license, with the caveat that nobody else can\nchange the license to be less-permissive. I think this is a reasonable step to take if you really want to\ncurb this kind of bad behavior. However, it‚Äôs also a blunt instrument. I want organizations to be\nable to make money using open-source software, and I think copyleft licenses may reduce a lot of\nthe fluid, open practices that make open-source such a powerful force in our society.\n\nMany thanks to \n\nYuvi Panda, \n\nTim Head, and \n\nJoe Hamman for comments on iterations of this post","type":"content","url":"/blog/2018/free-labor-partners#what-could-organizations-do-now-to-support-open-communities","position":9},{"hierarchy":{"lvl1":"Blogging with Jupyter Notebooks and Jekyll using nbconvert templates"},"type":"lvl1","url":"/blog/2018/jekyllmarkdown","position":0},{"hierarchy":{"lvl1":"Blogging with Jupyter Notebooks and Jekyll using nbconvert templates"},"content":"Here‚Äôs a quick (and hopefully helpful) post for those wishing to blog in\nJekyll using Jupyter notebooks. As some of you may know, nbconvert can\neasily convert your .ipynb files to markdown, which Jekyll can easily\nturn into blog posts for you.nbconvert --to markdown myfile.ipynb\n\nHowever, an annoying part of this is that Markdown doesn‚Äôt include classes\nfor input and outputs, which means they each get treated the same in the\noutput. Not ideal.\n\nFortunately, \n\nyou can customize nbconvert extensively.\nFirst, it‚Äôs possible to \n\ncreate your own exporter class, but this is a bit heavy for what we want to do. In our case, we‚Äôd\nsimply like to extend the markdown exporter so that it outputs Jekyll-friendly\nmarkdown.","type":"content","url":"/blog/2018/jekyllmarkdown","position":1},{"hierarchy":{"lvl1":"Blogging with Jupyter Notebooks and Jekyll using nbconvert templates","lvl2":"Extending nbconvert's markdown template"},"type":"lvl2","url":"/blog/2018/jekyllmarkdown#extending-nbconverts-markdown-template","position":2},{"hierarchy":{"lvl1":"Blogging with Jupyter Notebooks and Jekyll using nbconvert templates","lvl2":"Extending nbconvert's markdown template"},"content":"Because nbconvert uses Liquid Templates for its exporters, this is\nrelatively easy! For example,\n\n\nhere is nbconvert‚Äôs markdown template.\nYou can see how it extends another template, then adds some modifications of\nits own. What we need to do is create a new template that slightly modifies\nthe functionality of nbconvert‚Äôs markdown template. Then we can use the same\nmarkdown exporter, but with our custom template defining how the markdown is\ncreated.\n\nTo treat input and output text differently, we‚Äôll extend nbconvert‚Äôs base\nmarkdown template by creating a template file of our own. Simply write the\nfollowing lines into a file called mytemplate.tpl.\n\n{% highlight html %}\n{% raw %}\n{% extends ‚Äòmarkdown.tpl‚Äô %} Add Div for input area \n\n{% block input %}\n\n{{ super() }}\n\n{% endblock input %} Remove indentations for output text and add div classes  \n\n{% block stream %}\n{:.output_stream}{{ output.text }}\n\n{% endblock stream %}\n\n{% block data_text %}\n{:.output_data_text}{{ output.data['text/plain'] }}\n\n{% endblock data_text %}\n\n{% block traceback_line  %}\n{:.output_traceback_line}{{ line | strip_ansi }}\n\n{% endblock traceback_line  %} Tell Jekyll not to render HTML output blocks as markdown \n\n{% block data_html %}\n\n{{ output.data['text/html'] }}\n\n{% endblock data_html %} {% endraw %} {% endhighlight %}\n\nAbove, we‚Äôre doing two things:\n\nOverriding the input area block so that it is now wrapped in <div> tags.\nNote that we can set a custom class, and set markdown=\"1\" so that the\nmarkdown conversion occurs within the div.\n\nOverriding various output text blocks so that we remove the indentation\nthat was used to denote a ‚Äúcode‚Äù cell. Instead, we‚Äôll wrap the output text\nin more common ``` characters, and use a trick to add a class to\ncode blocks: {:.class_name} syntax.\n\nWe can then directly reference this template when we call nbconvert:nbconvert --to markdown --template path/to/mytemplate.tpl myfile.ipynb\n\nAs a result, we now have classes around each of these divs that we can style\nhowever we like. For example, here are the CSS rules I added to remove the\ntheme‚Äôs ‚Äúcode box‚Äù around each of the output areas:.input_area div.highlighter-rouge {\n  background-color: #f7f7f7  !important;\n}\n\n.output_stream, .output_data_text, .output_traceback_line {\n  margin-left: 2% !important;\n  border: none !important;\n  border-radius: 4px !important;\n  background-color: #fafafa !important;\n  box-shadow: none !important;\n}\n\n.output_stream:before, .output_data_text:before, .output_traceback_line:before{\n  content: none !important;\n}\n\nIt took me a while to figure out this pattern, so hopefully other people find\nit useful as well!","type":"content","url":"/blog/2018/jekyllmarkdown#extending-nbconverts-markdown-template","position":3},{"hierarchy":{"lvl1":"How do projects signal how ‚Äúopen‚Äù they are?"},"type":"lvl1","url":"/blog/2018/kinds-of-openness","position":0},{"hierarchy":{"lvl1":"How do projects signal how ‚Äúopen‚Äù they are?"},"content":"How do open projects signal their ‚Äúopenness‚Äù to the outside community? This is\na really hard question, particularly because nowadays ‚Äúopen‚Äù has become a buzzword\nthat doesn‚Äôt just signal a project‚Äôs position to the community, but is also used\nas a marketing term to increase support, users, or resources.\n\nI was thinking about this the other day, so decided to take to twitter:\n\nhttps://‚Äãtwitter‚Äã.com‚Äã/choldgraf‚Äã/status‚Äã/1054478362209480704\n\nI was surprised at how much this question resonated with people. Here are a few\nhighlights from the (very interesting) conversation that came out of that question.","type":"content","url":"/blog/2018/kinds-of-openness","position":1},{"hierarchy":{"lvl1":"How do projects signal how ‚Äúopen‚Äù they are?","lvl2":"Some discussion threads"},"type":"lvl2","url":"/blog/2018/kinds-of-openness#some-discussion-threads","position":2},{"hierarchy":{"lvl1":"How do projects signal how ‚Äúopen‚Äù they are?","lvl2":"Some discussion threads"},"content":"","type":"content","url":"/blog/2018/kinds-of-openness#some-discussion-threads","position":3},{"hierarchy":{"lvl1":"How do projects signal how ‚Äúopen‚Äù they are?","lvl3":"Wishes vs. reality","lvl2":"Some discussion threads"},"type":"lvl3","url":"/blog/2018/kinds-of-openness#wishes-vs-reality","position":4},{"hierarchy":{"lvl1":"How do projects signal how ‚Äúopen‚Äù they are?","lvl3":"Wishes vs. reality","lvl2":"Some discussion threads"},"content":"Tal immediately brought up a really important point: many projects want to be\ninclusive and welcoming to others, but they don‚Äôt have time to do so.\n\nhttps://‚Äãtwitter‚Äã.com‚Äã/talyarkoni‚Äã/status‚Äã/1054484496769314818\n\nI think this is an important distinction, and something that should be signaled\nclearly. One the one hand, if a person generally wants others to contribute to\nthe project, then they‚Äôre some degree of openness higher than a project that\nactively discourages this.\n\nOn the other hand, running open projects does take work,\nand a project that says ‚Äúwell I‚Äôd like to be open but can‚Äôt commit the time to do it‚Äù\nalso isn‚Äôt that open in practice. No hard feelings there, but I think that\nthe goal of defining a ‚Äúdegree of openness‚Äù isn‚Äôt to signal a value judgment on the\npeople related to the project, but on the project itself. If you really want to\ngrow an open community around a project, you need to dedicate time and resources to\nthe community itself, not just the technical pieces of the tool.","type":"content","url":"/blog/2018/kinds-of-openness#wishes-vs-reality","position":5},{"hierarchy":{"lvl1":"How do projects signal how ‚Äúopen‚Äù they are?","lvl2":"Metrics of openness"},"type":"lvl2","url":"/blog/2018/kinds-of-openness#metrics-of-openness","position":6},{"hierarchy":{"lvl1":"How do projects signal how ‚Äúopen‚Äù they are?","lvl2":"Metrics of openness"},"content":"That leaves open the question: ‚Äúhow do we measure the practical openness of a project,\nrather than just what it says?‚Äù. A few folks mentioned that the CHAOSS project\ndoes a lot of work in this gneeral space:\n\nhttps://‚Äãtwitter‚Äã.com‚Äã/abbycabs‚Äã/status‚Äã/1054492219808403457\n\nCHAOSS defines standards for metrics to collect about communities. They don‚Äôt necessarily\nsay what others should do with those metrics, so perhaps that‚Äôs on the open community\nto define for themselves.\n\nPersonally, I‚Äôd love to see more tooling that makes it possible to scrape activity\nstatistics from open repositories. Tal and others suggested a few things:\n\ntime to initial response to new issues (maybe separated by new vs. old contributors)\n\ninequality coefficient for contributor commits\n\nnumber of unique organizations/email domains in contrbutors\n\nuse of positive/welcoming language\n\nexplicit roles defined, and pathways towards working more with the community\n\nI‚Äôd love to see more thoughts along these lines. If we could define a collection of\nmetrics around openness, it‚Äôd paint a much more rich picture than simply ‚Äúdoes this\nproject have a permissive license.‚Äù\n\nThere was also a specific metric around governance that‚Äôs worth highlighting:\n\nhttps://‚Äãtwitter‚Äã.com‚Äã/GeorgLink‚Äã/status‚Äã/1054621070945329152\n\nThe paper linked above is a study that investigated ‚Äúopen governance‚Äù in a number of\nopen-source mobile projects. It‚Äôs an interesting exploration of the ways that\ndecision-making is made (and signaled) in several projects. Perhaps unsurprisingly, they\nconclude that ‚Äúmore open‚Äù projects are most-likely to be successful in the long term\n(with a few exceptions).\n\nFinally, apparently there‚Äôs also a ‚Äúbadge‚Äù to signal the status of a repository (is it\nactive, vaporware, abandoned, etc):\n\nhttps://‚Äãtwitter‚Äã.com‚Äã/parente‚Äã/status‚Äã/1055053470808580098\n\nI‚Äôd love to see more of these semi-automated signals to help guide the open source community\nin deciding what projects to adopt and contribute to. As more and more people do\ntheir work online and in the open, it also creates a challenge of sifting through the noise\nto make the most of your (limited) time and energy. Having better metrics like these will\nmake these decisions easier.","type":"content","url":"/blog/2018/kinds-of-openness#metrics-of-openness","position":7},{"hierarchy":{"lvl1":"How do projects signal how ‚Äúopen‚Äù they are?","lvl3":"Mozilla‚Äôs archetypes of open projects","lvl2":"Metrics of openness"},"type":"lvl3","url":"/blog/2018/kinds-of-openness#mozillas-archetypes-of-open-projects","position":8},{"hierarchy":{"lvl1":"How do projects signal how ‚Äúopen‚Äù they are?","lvl3":"Mozilla‚Äôs archetypes of open projects","lvl2":"Metrics of openness"},"content":"One of the most fascinating links I found was Mozilla‚Äôs ‚Äúarchetypes of open projects‚Äù\ndocument:\n\nhttps://‚Äãtwitter‚Äã.com‚Äã/neuromusic‚Äã/status‚Äã/1054517145436975104\n\nBriefly, this is an internal document that Mozilla made public. It attempts to define\nthe different kinds of open projects that exist. Importantly, it also explains the\nvalue propositions of each, how it can be used strategically within an organization, and\nhow it supports (or doesn‚Äôt) an open community around it.\n\nI added some thoughts about how Project Jupyter fits into these archetypes on the\n\n\nJupyter governance research issue\nand I‚Äôd love to think more about how these archetypes fit into the pre-existing open communities\nthat are out there. If anybody wants to brainstorm how these archetypes fit into the scientific\nopen community, I‚Äôd love to chat :-)\n\nOn that note, I want to give a brief shout-out to Mozilla in general, which has\neither conducted or sponsored a bunch of interesting work in open projects.\nFor example, they have a whole wiki dedicated to working openly:\n\nhttps://‚Äãtwitter‚Äã.com‚Äã/alex‚Äã_‚Äã_morley‚Äã/status‚Äã/1054483982040121344\n\nand they also run lots of training and community programs such as the\n\n\nMozilla Open Leaders program.\nProject Jupyter is in this year‚Äôs cohort and \n\nkeeping track of its progress here.","type":"content","url":"/blog/2018/kinds-of-openness#mozillas-archetypes-of-open-projects","position":9},{"hierarchy":{"lvl1":"How do projects signal how ‚Äúopen‚Äù they are?","lvl3":"Importance of ethnography:","lvl2":"Metrics of openness"},"type":"lvl3","url":"/blog/2018/kinds-of-openness#importance-of-ethnography","position":10},{"hierarchy":{"lvl1":"How do projects signal how ‚Äúopen‚Äù they are?","lvl3":"Importance of ethnography:","lvl2":"Metrics of openness"},"content":"A final note on the importance of ethnography:\n\nhttps://‚Äãtwitter‚Äã.com‚Äã/mmmpork‚Äã/status‚Äã/1054745690897711104\n\nFor all of my talk about metrics above, I‚Äôve come to appreciate that numbers\nare never sufficient to describe the complexities of a community or group.\nOver the last several years at the \n\nBerkeley Institute for Data Science,\nI‚Äôve had the pleasure of working with several ethnographers who have shared their\nperspective on how to study communities. Semi-automatically-calculated numbers can\nbe a great way to see relatively coarse-level view of a community, but if you really\nwany to understand what‚Äôs going on, you need to dig in there, conduct qualitative interviews,\noperate in the community, and create some stories that back up (or not) the quantitative\ndata that you collect. We‚Äôd all be better off if there were more ethnographers in our\nrespective communities <3.\n\nOK, that‚Äôs enough for now - I hope these links are useful and I‚Äôll try to\nupdate them over time if I hear of some new projects along these lines.\nIf you have any suggestions, feel free to leave 'em in the comments!","type":"content","url":"/blog/2018/kinds-of-openness#importance-of-ethnography","position":11},{"hierarchy":{"lvl1":"Introducing makeitpop, a tool to perceptually warp your data!\""},"type":"lvl1","url":"/blog/2018/makeitpop","position":0},{"hierarchy":{"lvl1":"Introducing makeitpop, a tool to perceptually warp your data!\""},"content":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom makeitpop import makeitpop, update_derivatives, cmaps\nimport seaborn as sns\nsns.set(font_scale=2, style='white')\n\n# Get the perceptual derivatives we use for popping so we can visualize\nderivatives, derivatives_scaled = update_derivatives(cmaps)\n\n\n\ncategory: til\ndate: ‚Äò2018-06-04‚Äô\nfeatured_image: 3\nkernelspec:\ndisplay_name: Python 3 (ipykernel)\nlanguage: python\nname: python3\nlanguage_info:\ncodemirror_mode:\nname: ipython\nversion: 3\nfile_extension: .py\nmimetype: text/x-python\nname: python\nnbconvert_exporter: python\npygments_lexer: ipython3\nversion: 3.9.7\nredirect: makeitpop-intro\ntags:\n\npython colormaps\ntitle: Introducing makeitpop, a tool to perceptually warp your data!","type":"content","url":"/blog/2018/makeitpop","position":1},{"hierarchy":{"lvl1":"Introducing makeitpop, a tool to perceptually warp your data!\""},"type":"lvl1","url":"/blog/2018/makeitpop#introducing-makeitpop-a-tool-to-perceptually-warp-your-data","position":2},{"hierarchy":{"lvl1":"Introducing makeitpop, a tool to perceptually warp your data!\""},"content":"Note\n\nIt should go without saying, but you should never do the stuff that you‚Äôre about to read about here. Data is meant to speak for itself, and our visualizations should accurately reflect the data above all else.*\n\nWhen I was in graduate school, I tended to get on my soapbox and tell everybody\nwhy they should \n\nstop using Jet\nand adopt a ‚Äúperceptually-flat‚Äù colormap like \n\nviridis, magma, or inferno.\n\nSurprisingly (ok, maybe not so surprisingly) I got a lot of pushback from people. Folks would say ‚ÄúBut I like jet, it really highlights my data, it makes the images ‚Äòpop‚Äô more effectively than viridis!‚Äù.\n\nUnfortunately it turns out that when a colormap ‚Äúmakes your data pop‚Äù, it really just means ‚Äú\n\nwarps your perception of the visualized data so that you see non-linearities when there are none‚Äù. AKA, a colormap like Jet actually mis-represents the data.\n\nBut what does this really mean? It‚Äôs difficult to talk and think about coor - especially when it comes to relating color with objective relationships\nbetween data. Rather than talking about colormaps in the abstract, what if we could visualize the warping that is performed by colormaps like Jet?\n\nIn this post I‚Äôll show that this is possible! Introducing makeitpop.","type":"content","url":"/blog/2018/makeitpop#introducing-makeitpop-a-tool-to-perceptually-warp-your-data","position":3},{"hierarchy":{"lvl1":"Introducing makeitpop, a tool to perceptually warp your data!\"","lvl2":"What does makeitpop do?"},"type":"lvl2","url":"/blog/2018/makeitpop#what-does-makeitpop-do","position":4},{"hierarchy":{"lvl1":"Introducing makeitpop, a tool to perceptually warp your data!\"","lvl2":"What does makeitpop do?"},"content":"Makeitpop lets you apply the same perceptual warping that would normally be accomplished\nwith a colormap like Jet, but applies this warping to the data itself! This lets us\nget the same effect with a nice linear colormap like viridis!\n\nFor example, let‚Äôs take a look at the \n\nimage demo in matplotlib. In it, we create two blobs that are meant to be visualized as an image. We‚Äôll visualize this with our old friend viridis.\n\n# Create a mesh grid with two gaussian blobs\ndelta = 0.025\nx = y = np.arange(-3.0, 3.0, delta)\nX, Y = np.meshgrid(x, y)\nZ1 = np.exp(-X**2 - Y**2)\nZ2 = np.exp(-(X - 1)**2 - (Y - 1)**2)\nZ = (Z1 - Z2) * 2\n\n# Visualize it\nfig, ax = plt.subplots(figsize=(5, 5))\nax.imshow(Z, cmap=plt.cm.viridis,\n          origin='lower', extent=[-3, 3, -3, 3],\n          vmax=abs(Z).max(), vmin=-abs(Z).max())\nax.set(title=\"Original data\\nlinear colormap\")\nplt.tight_layout()\n\nHmmm, not too bad...but it‚Äôs a bit boring, no? Why can‚Äôt we make it snazzier? I know, let‚Äôs use Jet!\n\n# Visualize our data\nfig, ax = plt.subplots(figsize=(5, 5))\nax.imshow(Z, cmap=plt.cm.jet,\n          origin='lower', extent=[-3, 3, -3, 3],\n          vmax=abs(Z).max(), vmin=-abs(Z).max())\nax.set(title=\"Original data\\nnon-linear colormap\")\nplt.tight_layout()\n\nOooh now that‚Äôs what I‚Äôm talking about. You can clearly see two peaks of significant results\nat the center of each circle. Truly this is fit for publishing in Nature.\n\nBut...as you all know, this data only looks better because we‚Äôve used a colormap that distorts\nour perception of the underlying data.\n\nLet‚Äôs illustrate this by making it pop!\n\n# Pop the data!\nZ_popped = makeitpop(Z, 'jet', scaling_factor=20)\n\n# Visualize the warped data\nfig, ax = plt.subplots(figsize=(5, 5))\nax.imshow(Z_popped, cmap=plt.cm.viridis,\n          origin='lower', extent=[-3, 3, -3, 3],\n          vmax=abs(Z).max(), vmin=-abs(Z).max())\nax.set(title=\"Warped data\\nlinear colormap\")\nplt.tight_layout()\n\nExcellent! We‚Äôre using a nice, perceptually-flat colormap like viridis, but we‚Äôve attained\nan effect similar to the one that Jet would have created!\n\nNow let‚Äôs visualize all three next to each other so that we can see the total effect:\n\nfig, axs = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n\nkws_img = dict(extent=[-3, 3, -3, 3], origin='lower',\n               vmax=abs(Z).max(), vmin=-abs(Z).max())\n\n# Raw data with a perceptually-flat colormap\naxs[0].imshow(Z, cmap=plt.cm.viridis, **kws_img)\naxs[0].set(title=\"Original data\\nlinear colormap\")\n\n# Raw data with a perceptually-distorted colormap\naxs[1].imshow(Z, cmap=plt.cm.jet, **kws_img)\naxs[1].set(title=\"Original data\\nnon-linear colormap\")\n\n# Distorted data with a perceptually-flat colormap\naxs[2].imshow(Z_popped, cmap=plt.cm.viridis, **kws_img)\naxs[2].set(title=\"Warped data\\nlinear colormap\")\n\nplt.tight_layout()\n\n","type":"content","url":"/blog/2018/makeitpop#what-does-makeitpop-do","position":5},{"hierarchy":{"lvl1":"Introducing makeitpop, a tool to perceptually warp your data!\"","lvl2":"Let‚Äôs see it in the real world"},"type":"lvl2","url":"/blog/2018/makeitpop#lets-see-it-in-the-real-world","position":6},{"hierarchy":{"lvl1":"Introducing makeitpop, a tool to perceptually warp your data!\"","lvl2":"Let‚Äôs see it in the real world"},"content":"Thus far I‚Äôve been using toy examples to illustrate how makeitpop works. Let‚Äôs see how\nthings look on an actual dataset collected in the wild.\n\nFor this, we‚Äôll use the excellent \n\nnilearn package. This has a\nfew datasets we can download to demonstrate our point. First we‚Äôll load the data and prep it:\n\nfrom nilearn import datasets\nfrom nilearn import plotting\nimport nibabel as nb\n\n# Load a sample dataset\ntmap_filenames = datasets.fetch_localizer_button_task()['tmaps']\ntmap_filename = tmap_filenames[0]\n\n# Threshold our data for viz\nbrain = nb.load(tmap_filename)\nbrain_data = brain.get_fdata()\nmask = np.logical_or(brain_data < -.01, brain_data > .01)\n\nNext, we‚Äôll create a ‚Äúpopped‚Äù version of the data, where we apply the non-linear warping\nproperties of Jet to our data, so that we can visualize the same effect in linear space.\n\n# Create a copy of the data, then pop it\n# We'll use a scaling factor to highlight the effect\nbrain_popped = brain_data.copy()\nbrain_popped[mask] = makeitpop(brain_popped[mask], colormap='jet', scaling_factor=75)\nbrain_popped = nb.Nifti1Image(brain_popped, brain.affine)\n\nNow, I‚Äôll plot the results for each.\n\nFirst, we‚Äôll see the raw data on a linear colormap. This is the way we‚Äôd display the data to show the true underlying relationships between datapoints.\n\nNext, we‚Äôll show the same data plotted with Jet. See how many more significant voxels there are! (/s)\n\nFinally, we‚Äôll plot the ‚Äúpopped‚Äù data using a linear colormap (viridis). This accurately represents the underlying data, but the data itself has been distorted!\n\nfor i_brain, name in [(brain, 'original'), (brain, 'original (jet)'), (brain_popped, 'popped brain')]:\n    cmap = 'jet' if name == 'original (jet)' else 'viridis'\n    plotting.plot_stat_map(i_brain, cmap=cmap, vmax=7, display_mode='x',\n                           title=name, cut_coords=np.linspace(20, 50, 5, dtype=int))\n\nAs you can see, different kinds of results show up when your perception of the data is affected by the colormap.\n\n","type":"content","url":"/blog/2018/makeitpop#lets-see-it-in-the-real-world","position":7},{"hierarchy":{"lvl1":"Introducing makeitpop, a tool to perceptually warp your data!\"","lvl2":"How does this work?"},"type":"lvl2","url":"/blog/2018/makeitpop#how-does-this-work","position":8},{"hierarchy":{"lvl1":"Introducing makeitpop, a tool to perceptually warp your data!\"","lvl2":"How does this work?"},"content":"OK, so what is the black voodoo magic that makes it possible to ‚Äúmake your data pop‚Äù?\n\nIt all comes down to your visual system. I won‚Äôt go into a ton of detail\nbecause Nathaniel Smith and Stefan van der Walt \n\nalready gave a great talk about this, however here is a lay-person‚Äôs take:\n\nWhen we use color to represent data, we are mapping a range of data values onto\na range of color values. Usually this means defining a min / max for our data, then mapping data\nvalues linearly from 0 to 1, and finally mapping those values onto RGB values in a colormap.\n\nImplicit in this process is the idea that stepping across our space in the data equates to\nan equal step in our perception of the color that is then chosen. We want a one-to-one mapping between the two.\n\nUnfortunately, this isn‚Äôt how our visual system works.\n\nIn reality, our brains do all kinds of strange things when interpreting color. They are biased to detect changes between particular kinds of colors, and biased to miss the transition between others.\n\nJet uses a range of colors that highlight this fact. It transitions through colors such that linear changes in our data are perceived as nonlinear changes when we look at the visualization. That‚Äôs what makes the data ‚Äúpop‚Äù.","type":"content","url":"/blog/2018/makeitpop#how-does-this-work","position":9},{"hierarchy":{"lvl1":"Introducing makeitpop, a tool to perceptually warp your data!\"","lvl2":"Perceptual ‚Äúdelta‚Äù curves"},"type":"lvl2","url":"/blog/2018/makeitpop#perceptual-delta-curves","position":10},{"hierarchy":{"lvl1":"Introducing makeitpop, a tool to perceptually warp your data!\"","lvl2":"Perceptual ‚Äúdelta‚Äù curves"},"content":"You can determine the extent to which a colormap ‚Äúwarps‚Äù your perception of the data by calculating the ‚Äúperceptual deltas‚Äù as you move across the values of a colormap (e.g. as you move from 0 to 1, and their corresponding colors).\n\nThese deltas essentially mean ‚Äúhow much is the next color in the colormap perceived as different from the current color?‚Äù If your colormap is perceptually flat, the delta will be the same no matter where you are on the range from 0 to 1.\n\nLet‚Äôs see what the deltas look like for Jet:\n\ndef plot_colormap_deltas(deltas, cmap, ax=None):\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(10, 5))\n    xrange = np.arange(len(derivatives))\n    sc = ax.scatter(xrange, deltas, c=xrange, vmin=xrange.min(), vmax=xrange.max(),\n                    cmap=plt.cm.get_cmap(cmap), s=20)\n    ax.plot(xrange, deltas, c='k', alpha=.1)\n    return ax\n\nax = plot_colormap_deltas(derivatives['jet'].values, 'jet')\nylim = ax.get_ylim()  # So we can compare with other colormaps\nax.set(title=\"Perceptual deltas with Jet\")\n\nOops.\n\nAs you can see, Jet does not have a flat line for perceptual deltas. Each ‚Äújump‚Äù you see above is a moment where Jet is actually mis-representing differences in the data. For shame, Jet.\n\nNow let‚Äôs see what this looks like for viridis:\n\nax = plot_colormap_deltas(derivatives['viridis'].values, 'viridis')\nax.set(ylim=ylim, title=\"Perceptual deltas with viridis\");\n\nAhhh, sweet, sweet linear representation of data.\n\nIn case you‚Äôre curious, here are the ‚Äúperceptual deltas‚Äù for several colormaps.\nIn this case, I‚Äôve centered them and scaled each by the variance of the largest colormap,\nso that they are easier to compare.\n\nfig, ax = plt.subplots(figsize=(10, 5))\nfor cmap, deltas in derivatives_scaled.items():\n    if cmap == 'linear':\n        continue\n    ax = plot_colormap_deltas(deltas.values, cmap, ax=ax)\nax.set(title=\"Scaled perceptual deltas for a bunch of colormaps\");\n\n","type":"content","url":"/blog/2018/makeitpop#perceptual-delta-curves","position":11},{"hierarchy":{"lvl1":"Introducing makeitpop, a tool to perceptually warp your data!\"","lvl2":"We can even warp 1-dimensional data!"},"type":"lvl2","url":"/blog/2018/makeitpop#we-can-even-warp-1-dimensional-data","position":12},{"hierarchy":{"lvl1":"Introducing makeitpop, a tool to perceptually warp your data!\"","lvl2":"We can even warp 1-dimensional data!"},"content":"Let‚Äôs see how this principle affects our perception with a different kind of visual\nencoding. Now that we know these perceptual warping functions, we can get all the\ndata-warping properties of jet, but in one dimension!\n\nHere‚Äôs a line.\n\nfig, ax = plt.subplots(figsize=(5, 5))\nx = np.linspace(0, 1, 100)\nax.plot(x, x, 'k-', lw=8, alpha=.4, label='True Data')\nax.set_title('Totally boring line.\\nNothing to see here.');\n\nEw. Boring.\n\nNow, let‚Äôs make it pop! We‚Äôll loop through a few colormaps, applying its color\nwarping function to the y-axis of our line as we step through it.\n\nnames = ['jet', 'viridis', 'rainbow', 'spring', 'hsv']\nfig, ax = plt.subplots(figsize=(10, 10))\n\nx = np.linspace(0, 1, 1000)\nax.plot(x, x, 'k-', lw=12, alpha=.4, label='True Data')\nfor nm in names:\n    ax.plot(x, makeitpop(x, colormap=nm, scaling_factor=40), label=nm, lw=4)\n    ax.legend(loc=(1.05, .6))\nax.set_title('Making data \"pop\" is fun!')\n\nAs you can see, data looks much more interesting when it‚Äôs been non-linearly warped!\nIt looks particularly striking when you see it on a 1-D plot. This is effectively\nwhat colormaps such as Jet are doing in 2 dimensions! We‚Äôre simply bringing the fun\nback to 1-D space.\n\nLet‚Äôs see how it looks on some scatterplots. We‚Äôll plot the raw data in the background in grey,\nand the ‚Äúpopped‚Äù data in front in color. Notice how some colormaps distort the y-values more\nthan others.\n\nnames = ['viridis', 'jet', 'rainbow', 'spring', 'hsv']\nfig, axs = plt.subplots(1, len(names), figsize=(20, 4), sharex=True, sharey=True)\nx = np.linspace(0, 1, 100)\ny = x + np.random.randn(len(x)) * .2\nfor name, ax in zip(names, axs):    \n    ax.scatter(x, y, c='k', s=80, alpha=.2)\n    ax.scatter(x, makeitpop(y, name, 40), c=y, s=40, alpha=.8, cmap=plt.get_cmap(name))\n    ax.set(title=name)\n\n\n","type":"content","url":"/blog/2018/makeitpop#we-can-even-warp-1-dimensional-data","position":13},{"hierarchy":{"lvl1":"Introducing makeitpop, a tool to perceptually warp your data!\"","lvl2":"So what should we do?"},"type":"lvl2","url":"/blog/2018/makeitpop#so-what-should-we-do","position":14},{"hierarchy":{"lvl1":"Introducing makeitpop, a tool to perceptually warp your data!\"","lvl2":"So what should we do?"},"content":"The reason that I wrote this blog post (and made this silly package) is to illustrate what we‚Äôre really\ndoing when we use a colormap like Jet, and to highlight the importance of using a perceptually-flat colormap.  Sure, we want to choose the visualization that best-makes our point,\nbut a colormap like Jet is actively mis-representing your data. You‚Äôd never consider changing the raw\ndata values so that an effect popped out, and you‚Äôd never alter the y-values of a scatterplot so that something shows up. Well, this is perceptually what you‚Äôre doing when you visualize 2-D data with Jet.\n\nHere are a few things to keep in mind moving forward:\n\nDon‚Äôt use Jet\n\nIf you review a paper or are an editor for a journal, consider asking authors to use a perceptually flat colormap (this is usually just a matter of changing cmap='viridis'!)\n\nBe aware of the effects that color has on the point you‚Äôre trying to make. Perceptual warping is\njust one of many potential issues with choosing the right color.","type":"content","url":"/blog/2018/makeitpop#so-what-should-we-do","position":15},{"hierarchy":{"lvl1":"Introducing makeitpop, a tool to perceptually warp your data!\"","lvl2":"Wrapping up"},"type":"lvl2","url":"/blog/2018/makeitpop#wrapping-up","position":16},{"hierarchy":{"lvl1":"Introducing makeitpop, a tool to perceptually warp your data!\"","lvl2":"Wrapping up"},"content":"I hope that this post has been a fun and slightly informative take on the nuances of colormaps, and the unintended effects that they might have.\n\nSo, tl;dr:\n\nJet (and many other colormaps) mis-represent your perception of the data\n\nPerceptually flat colormaps like Viridis, Magma, Inferno, or Parula minimize this effect\n\nYou can calculate the extent to which this mis-representation happens as you move along the colormap\n\nWe can then use this function to distort data so that the data itself contains this mis-representation\n\nBut doing so would be super unethical, so in the end you should stop using jet and use a perceptually-flat colormap like viridis.\n\nIf you‚Äôd like to check out the makeitpop package, see the \n\nGitHub repo here. In addition, all of the examples in this post are runnable\non Binder! You can launch an interactive session with this code by clicking on the Binder button\nat the top of this page!\n\n","type":"content","url":"/blog/2018/makeitpop#wrapping-up","position":17},{"hierarchy":{"lvl1":"Introducing makeitpop, a tool to perceptually warp your data!\"","lvl2":"Addendum: Ok but how does makeitpop actually work?"},"type":"lvl2","url":"/blog/2018/makeitpop#addendum-ok-but-how-does-makeitpop-actually-work","position":18},{"hierarchy":{"lvl1":"Introducing makeitpop, a tool to perceptually warp your data!\"","lvl2":"Addendum: Ok but how does makeitpop actually work?"},"content":"In this section I‚Äôll describe the (admittedly hacky) way that I‚Äôve written makeitpop.\nAs I mentioned before, \n\nall the makeitpop code is on GitHub and\nPull Requests are more than welcome to improve the process (I‚Äôm looking at you, ‚Äúhistogram matching‚Äù people!)\n\nHere‚Äôs what makeitpop does:\n\nCollects a list of the ‚Äúperceptual deltas‚Äù. These are calculated from the equations given in \n\nviscm, which was released as a part of the original work that created viridis.\n\nCenters each colormap‚Äôs deltas at 0.\n\nScales each colormap‚Äôs deltas by the largest variance across all colormaps. This is to make sure that warping the data is done with the relative differences of each colormap in mind.\n\nWhen makeitpop is called, the function then:\n\nScales the input data linearly between 0 and 1\n\nCalculates the point-by-point derivative for linearly spaced points between 0 and 1 (the derivative is the same for all points here).\n\nMultiplies each derivative by the scaled perceptual deltas function, plus an extra scaling factor that accentuates the effect.\n\nAdds each value to the scaled input data.\n\nUn-scales the altered input data so that it has the same min/max as before.\n\nThere are plenty of ways you could do this more effectively (for example, by matching empirical CDFs and\nusing linear interpolation to map the delta function of one colormap onto the delta function for another\ncolormap). If you‚Äôd like to contribute or suggest something, feel free to do so! However, I‚Äôm just creating\nthis package to highlight an idea, and think this approach gets close enough with relatively little complexity.\n\n","type":"content","url":"/blog/2018/makeitpop#addendum-ok-but-how-does-makeitpop-actually-work","position":19},{"hierarchy":{"lvl1":"Some extras"},"type":"lvl1","url":"/blog/2018/makeitpop#some-extras","position":20},{"hierarchy":{"lvl1":"Some extras"},"content":"\n\nHere‚Äôs a viz that will let you visualize how different colormaps distort data. We‚Äôll show a\ngradient of linearly-spaced values, both using a warping colormap such as ‚Äújet‚Äù and a linear\ncolormap like ‚Äúvidiris‚Äù. Then, we‚Äôll ‚Äúpop‚Äù the data and re-visualize with viridis.\n\n# Create a gradient of datapoints\ndata = np.linspace(0, 1, 256)\ndata = np.tile(data, [100, 1])\n\n# Pop the data\ncmap_warping = 'jet'\nscaling_factor = 50\ndata_popped = makeitpop(data, cmap_warping, scaling_factor)\n\n# Visualize\nfig, axs = plt.subplots(6, 1, figsize=(10, 15), sharex=True)\naxs[0].pcolormesh(derivatives_scaled.index, range(len(data)), data, vmin=0, vmax=1, cmap=cmap_warping)\naxs[0].set(title=\"Raw data with colormap {}\".format(cmap_warping))\naxs[1].plot(derivatives_scaled.index, derivatives_scaled[cmap_warping].values)\naxs[1].set(xlim=[0, 1], ylim=[-1.1, 1.1], title=\"First row of image\")\n\naxs[2].pcolormesh(derivatives_scaled.index, range(len(data)), data, vmin=0, vmax=1, cmap='viridis')\naxs[2].set(title=\"Raw data with colormap {}\".format('viridis'))\naxs[3].plot(derivatives_scaled.index, data[0])\naxs[3].set_xlim([0, 1])\n\naxs[4].pcolormesh(derivatives_scaled.index, range(len(data)), data_popped, vmin=0, vmax=1, cmap='viridis')\naxs[4].set(title=\"Data warped with colormap {}\".format(cmap_warping))\naxs[5].plot(derivatives_scaled.index, data_popped[0])\naxs[5].set_xlim([0, 1])\nplt.tight_layout()","type":"content","url":"/blog/2018/makeitpop#some-extras","position":21},{"hierarchy":{"lvl1":"My weekly workflow"},"type":"lvl1","url":"/blog/2018/my-workflow","position":0},{"hierarchy":{"lvl1":"My weekly workflow"},"content":"I‚Äôve had a bunch of conversations with friends who were interested in how to\nkeep track of the various projects they‚Äôre working on, and to prioritize their\ntime over the course of a week. I thought it might be helpful to post my own\napproach to planning time throughout the week in case it‚Äôs useful for others to\nriff off of.","type":"content","url":"/blog/2018/my-workflow","position":1},{"hierarchy":{"lvl1":"My weekly workflow","lvl2":"General principles"},"type":"lvl2","url":"/blog/2018/my-workflow#general-principles","position":2},{"hierarchy":{"lvl1":"My weekly workflow","lvl2":"General principles"},"content":"First off, a few general principles that I use to guide my thinking on planning\nout the week.\n\nBe intentional. This seems obvious, but I find that if I don‚Äôt explicitly define\nwhat I want to work on, I have more of those ‚Äúwhere the heck did the day go‚Äù kinds\nof weeks.\n\nBe strategic. There are only so many hours in the day, which means that I need to be\ncareful about what I commit to spending time on. I try to figure out what are the most\nimportant things to do, and ensure I make space for this work before other stuff.\n\nBe (conservatively) realistic. I often get too optimistic with my time and energy levels.\nSometimes you‚Äôre tired, sometimes you get distracted, sometimes new work pops up you didn‚Äôt\naccount for. For this reason, I only budget about 20 hours of diligent work a week.\n\nBe flexible. Finally, don‚Äôt worry about following the plan obsessively. Use it as a guide,\nnot a set of chains. If it‚Äôs worth re-prioritizing, that‚Äôs fine!\n\nBe diligent. That said, whenever the plans do change, make sure that this is written down\nsomewhere so that I know I‚Äôm being intentional about it.","type":"content","url":"/blog/2018/my-workflow#general-principles","position":3},{"hierarchy":{"lvl1":"My weekly workflow","lvl2":"What tools do I use?"},"type":"lvl2","url":"/blog/2018/my-workflow#what-tools-do-i-use","position":4},{"hierarchy":{"lvl1":"My weekly workflow","lvl2":"What tools do I use?"},"content":"OK so what does this process actually look like? Here‚Äôs a general approach:\n\nI use \n\nTrello for managing most of the projects that I work on each week. For those who aren‚Äôt familiar,\nTrello is a web app that lets you organize ‚Äúcards‚Äù into vertical lists. It‚Äôs useful for Kanban-style\nproject management, and is super popular in technical circles.\n\nI use two kinds of Trello boards, ‚Äúproject‚Äù boards, and my ‚Äúweekly‚Äù board.","type":"content","url":"/blog/2018/my-workflow#what-tools-do-i-use","position":5},{"hierarchy":{"lvl1":"My weekly workflow","lvl2":"Project boards"},"type":"lvl2","url":"/blog/2018/my-workflow#project-boards","position":6},{"hierarchy":{"lvl1":"My weekly workflow","lvl2":"Project boards"},"content":"Project boards are project-specific. They‚Äôre organized in four primary columns.\n\nInfo has cards with information about that project. For example, links, notes, attachments, etc.\n\nToDo has cards representing tasks that I‚Äôll do eventually. These tend to be more abstract and\nwill evolve over time.\n\nNext has cards representing actionable tasks that I‚Äôll do soon.\n\nDoing is a special list that I‚Äôll explain later. It‚Äôs where cards go when I‚Äôm working on them currently.\n\nThe other major component of the projcet board is a collection of ‚ÄúDone‚Äù lists. At the end of each\nweek, and cards that are finished get moved into a list named like so: Done - Week NN where ‚ÄúNN‚Äù is the\nnumber of the current week.\n\nI keep a short-list of currently-active projects that I‚Äôm working on. These are ‚Äústarred‚Äù and show up at\nthe top of the Trello board list to the left.\n\nHere‚Äôs an example of a project board:","type":"content","url":"/blog/2018/my-workflow#project-boards","position":7},{"hierarchy":{"lvl1":"My weekly workflow","lvl2":"Weekly board"},"type":"lvl2","url":"/blog/2018/my-workflow#weekly-board","position":8},{"hierarchy":{"lvl1":"My weekly workflow","lvl2":"Weekly board"},"content":"The other major Trello board that I use is my ‚Äúweekly‚Äù board. This is basically a to-do list for my week.\nIt‚Äôs got one list per project, and each card represents a to-do item for that project. There is one-to-one\nmapping between lists in this board and project boards that the cards come from.\n\nAs I work on cards throughout the week, I move them into another list called ‚ÄúDone‚Äù. Once the card is there,\nit means that I‚Äôve finished that task for the week.\n\nAn additional component is that each card is assigned a number of hours needed to complete the card.\nI try to keep the total number of hours on this board to <= 20. I‚Äôve found that this is a realistic\nbalance between ‚Äúenough hours to actually get stuff done‚Äù and ‚Äúnot so many hours that I won‚Äôt finish them‚Äù.\n\nTo assign and keep track of the hours with each card, I use a chrome extension called \n\nPlus for Trello. This\nadds fields for ‚Äúestimated‚Äù and ‚Äúspent‚Äù hours that I update over the week.\n\nHere‚Äôs an example of a weekly board:","type":"content","url":"/blog/2018/my-workflow#weekly-board","position":9},{"hierarchy":{"lvl1":"My weekly workflow","lvl2":"How does this work in practice?"},"type":"lvl2","url":"/blog/2018/my-workflow#how-does-this-work-in-practice","position":10},{"hierarchy":{"lvl1":"My weekly workflow","lvl2":"How does this work in practice?"},"content":"I use a tool called \n\nButler for Trello (BFT) to manage how cards flow through this system.\nLets you automate various tasks in Trello by writing messages that it parses as instructions.\n\nRemember that ‚ÄúDoing‚Äù list that I mentioned in each project board? Any time I move a card into\na ‚ÄúDoing‚Äù list on a project board, the following happens:\n\nBFT adds a custom field to the card that has the Project Board ID that it came from\n\nBFT moves the card to the ‚Äúweekly‚Äù board, to a list with the same name as the project board\n\nMoreover, remember that there was a ‚ÄúDone‚Äù list in my weekly board? That also does something\nspecial. I have a BFT link that, whenever clicked, does the following:\n\nLoop through every card in the ‚ÄúDone‚Äù list\n\nFor each card, find the Board ID that represents the board the card came from\n\nMove the card to that board, put it in a ‚ÄúDone‚Äù list that is automatically created using the\nnumber of the current active week.\n\nSo, the general flow of cards through this system looks like this:\n\nCreate cards in a project board that represent things to be done.\n\nWhen it‚Äôs time to work on a task, move it to the ‚ÄúDoing‚Äù list\n\nBFT moves that card to my ‚ÄúWeekly‚Äù board, to a list w/ the project name\n\nThis happens at the beginning of the week, along with an estimation of the time needed for\neach card.\n\nAs the week progresses, I mark off the hours that I‚Äôve spent on each card.\n\nWhen the card‚Äôs hours remaining reaches 0, I move it to the ‚ÄúDone‚Äù list\n\nAt the end of the week, I click my ‚Äúboard cleanup‚Äù link, and it returns each card to its\noriginal project board.\n\nNext week, I repeat this cycle.\n\nAnd that‚Äôs about it - I‚Äôve found that this process is a nice way to keep both a high-level\nview of what needs to be done for my various projects (on each project board). It also allows\nmy to dig into specific to-do items throughout the week (on my weekly board).\n\nI‚Äôd love to hear feedback and thoughts about how other people manage their time throughout\nthe week! Hopefully some of you all find this useful.","type":"content","url":"/blog/2018/my-workflow#how-does-this-work-in-practice","position":11},{"hierarchy":{"lvl1":"My weekly workflow","lvl2":"Extras"},"type":"lvl2","url":"/blog/2018/my-workflow#extras","position":12},{"hierarchy":{"lvl1":"My weekly workflow","lvl2":"Extras"},"content":"Butler for Trello commands can be a little bit difficult to compose. Below I‚Äôll post the\ntwo main commands that I use to handle all the card-moving described above.","type":"content","url":"/blog/2018/my-workflow#extras","position":13},{"hierarchy":{"lvl1":"My weekly workflow","lvl3":"The weekly board","lvl2":"Extras"},"type":"lvl3","url":"/blog/2018/my-workflow#the-weekly-board","position":14},{"hierarchy":{"lvl1":"My weekly workflow","lvl3":"The weekly board","lvl2":"Extras"},"content":"The weekly board has a ‚Äúbutler‚Äù list where the command is defined in a card (that‚Äôs how\nyou define board-specific BFT commands). Here‚Äôs the command:\n\n{% raw %}script: for each card in list \"Return\", remove all labels and move the card to list \"Returned | Week {weeknumber}\" on board \"{{%homeboard}}\" and move list \"Returned | Week {weeknumber}\" on board \"{{%homeboard}}\" to position 1 and clear custom field \"homeboard\"\n\n{% endraw %}","type":"content","url":"/blog/2018/my-workflow#the-weekly-board","position":15},{"hierarchy":{"lvl1":"My weekly workflow","lvl3":"A global command for project boards","lvl2":"Extras"},"type":"lvl3","url":"/blog/2018/my-workflow#a-global-command-for-project-boards","position":16},{"hierarchy":{"lvl1":"My weekly workflow","lvl3":"A global command for project boards","lvl2":"Extras"},"content":"I also use a global BFT command that moves the cards from each project board to the weekly\nboard when the card is moved to the ‚ÄúDoing‚Äù category.when a card is added to list \"Doing\" by me, set custom field \"homeboard\" to \"{boardlink}\" and move the card to the top of list \"{boardname}\" on board \"Active\" and move list \"{boardname}\" on board \"Active\" to position 2","type":"content","url":"/blog/2018/my-workflow#a-global-command-for-project-boards","position":17},{"hierarchy":{"lvl1":"I like Rust‚Äôs governance structure"},"type":"lvl1","url":"/blog/2018/rust-governance","position":0},{"hierarchy":{"lvl1":"I like Rust‚Äôs governance structure"},"content":"Recently I‚Äôve been reading up on governance models for several large-ish open\nsource projects. This is partially because I‚Äôm involved in a bunch of\nthese projects myself, and partially because it‚Äôs fascinating to see distributed groups\nof people organizing themselves in effective (or not) ways on the internet.","type":"content","url":"/blog/2018/rust-governance","position":1},{"hierarchy":{"lvl1":"I like Rust‚Äôs governance structure","lvl2":"Why is governance in open projects important?"},"type":"lvl2","url":"/blog/2018/rust-governance#why-is-governance-in-open-projects-important","position":2},{"hierarchy":{"lvl1":"I like Rust‚Äôs governance structure","lvl2":"Why is governance in open projects important?"},"content":"Governance is tricky, because there is an inherent tension between:\n\nBeing able to make important, complex, or sensitive decisions quickly\n\nBeing transparent and inclusive in the decision-making process\n\nFor most companies and organizations, the above is (sort-of) solved with a relatively\nhierarchical decision-making structure. The ‚ÄúChief Executive Officer‚Äù can\ndecide high-level directions for the whole company. The team manager can\ndefine the priorities for the group.\n\nThis generally isn‚Äôt the case in open-source, where nobody is beholden to the\nopinion of anybody else. In this case, leading and decision-making are done\nby persuading others and building coalitions. The effects of this difference often\naren‚Äôt felt in the early days of an open-source project, when the team is\nsmall, everybody knows one another, and developers often have the same perspective.\nHowever, as a project grows in its size and complexity, it becomes more important\nto create an organizational structure that recognizes, manages, and leverages that complexity.\n\nSo, this is why I like Rust‚Äôs governance model.\n\nI won‚Äôt go into detail about what ‚ÄúRust‚Äù is, except to say that it‚Äôs an open-source\nlanguage that has had a lot of support from the Mozilla foundation. In this case,\nI‚Äôm less interested in the specific technical pieces of that project, and want\nto focus on the people and the organizations in it.\n\nHere‚Äôs the challenge that the Rust community faces:\n\nBecause Rust is an open-source language, it has a lot of technical pieces to it that\nare very diverse in the kinds of demands they have. People working on low-level kernel\nimplementation will have a different perspective from those designing libraries for\nthe language. Moreover, because this language is used by many organizations, there‚Äôs\na strong diversity in the type of user that make up the Rust community. A open-source\nlead in a company has a different incentive structure than a researcher at a university.\n\nThis means that a single decision-making body would\n\nhave most of its members unable\nto make strong technical decisions about most of the sub-communities within Rust (e.g.,\na libraries and APIs person making decisions about kernel implementations).\n\nwould be susceptible to inertia in decision-making because of the size needed to represent\nall of the Rust community with a single group of people.\n\nwould probably be skewed towards one set of decisions over another (since whoever was\nmost powerful within this group would set the ‚Äúagenda‚Äù for the whole project)\n\nSo, here are two ways that Rust tries to address this problem:\n\nDivide the governance structure into sub-teams and a ‚Äúcore‚Äù team.\n\nUse an explicit ‚ÄúRequest for Comments‚Äù process to handle all non-trivial decisions.\n\nI‚Äôll describe each of these as I understand them so far:","type":"content","url":"/blog/2018/rust-governance#why-is-governance-in-open-projects-important","position":3},{"hierarchy":{"lvl1":"I like Rust‚Äôs governance structure","lvl2":"The Rust Governance structure - sub-teams and communities"},"type":"lvl2","url":"/blog/2018/rust-governance#the-rust-governance-structure-sub-teams-and-communities","position":4},{"hierarchy":{"lvl1":"I like Rust‚Äôs governance structure","lvl2":"The Rust Governance structure - sub-teams and communities"},"content":"The Rust governance structure is based on the idea that most decisions should\nnot need to be escalated to the highest decision-making authority in the community.\nMoreover, these decisions need to be made by people with a keen understanding of the\ndetails of the problem. Finally, these problems are not just technical in nature, but\nalso span community operations, organization, communication, etc.\n\nSo, Rust is divided into sub-teams that are broken down by topic. By my count, there are\n15 teams in total. Each team is tasked with a specific responsibility to\noversee in the Rust community. For example, the language team is responsible for\ndesigning new language features. The release team is responsible for\ntracking regressions, stabilizations, and producing Rust releases. The community team\nis responsible for coordinating events, outreach, commercial users, teaching materials, and exposure.\n\nHere is a page with all the Rust teams (and their\nmembers).\n\nMy favorite thing about this structure is that roles within the Rust community are\nexplicitly stated and people performing those roles are explicitly credited with\nthat work. You may notice that a lot of the teams that are listed involve work that is\nnot releasing features of Rust, or writing code. This kind of work is crucial for a\ncommunity to grow, but is often unrecognized or underappreciated (which has all kinds of\nimplications for diversity and inclusion, but that‚Äôs another conversation).\n\nOK, so these teams exist, but do they actually do? That takes us to the second part:","type":"content","url":"/blog/2018/rust-governance#the-rust-governance-structure-sub-teams-and-communities","position":5},{"hierarchy":{"lvl1":"I like Rust‚Äôs governance structure","lvl2":"The Rust ‚ÄúRequest for Comments‚Äù process"},"type":"lvl2","url":"/blog/2018/rust-governance#the-rust-request-for-comments-process","position":6},{"hierarchy":{"lvl1":"I like Rust‚Äôs governance structure","lvl2":"The Rust ‚ÄúRequest for Comments‚Äù process"},"content":"All significant design changes in the Rust community are not be submitted directly\nas a PR to the codebase. Instead, Rust has \n\na separate repository called rfcs. This\nrepository manages the process by which Rust sub-teams decide whether to support the high-level\ndesign of a feature. It‚Äôs a process to make a decision about whether something is worth doing (note here that\nwhen I say ‚Äúfeature‚Äù I don‚Äôt just mean code. In fact, the rfcs process itself was\nan rfc at one point). Here‚Äôs how it works:\n\nIf someone wants to make a change within the Rust community, they must make a Pull Request\nto the rfcs repository that proposes this change. They \n\nfill out a template (in markdown)\nthat covers things like ‚Äúwhy should this change be made?‚Äù, ‚Äúwhat is this change?‚Äù, ‚Äúwhat\nare the alternatives?‚Äù, ‚Äúwhat happens if we do nothing?‚Äù etc.\n\nThis person fills out the form and submits a Pull Request. At this point, one of the members\nof the sub-team associated with the topic of the PR is assigned to be the shephard of the RFC.\nThis simply means that their job is to ensure the conversation moves forward in a\ntransparent and inclusive manner. They are not tasked with deciding or implementing the feature.\n\nOnce the PR is made, the pull request enters an ‚Äúopen comments‚Äù period where people can\ndiscuss the proposal. Often this results in modifications to the PR as new ideas come up\nand old ideas get refined. Throughout this process, the shephard‚Äôs responsibility is to keep\nthings moving forward productively.\n\nOnce a member of the sub-team (usually the shephard) believes that enough discussion has\nhappened, they call for a ‚Äúfinal comments‚Äù\nperiod. This is their formal statement that ‚Äúwe‚Äôre ready to make a decision, so speak now or\nforever hold your peace‚Äù. If no major new concerns are brought up, the sub-team associated\nwith the RFC then must reach a consensus about whether to merge or close the PR. If the\nPR is merged, then the Rust community has now officially ‚Äúsupported the idea‚Äù in theory. Often\nthis is just the beginning of the hard work, and specific implementations get hashed out in the\nPR to the codebase.\n\nThis process is about giving more power to the Rust sub-teams. In fact,\n\n\nsub-teams can also modify this RFC process for their own purposes.","type":"content","url":"/blog/2018/rust-governance#the-rust-request-for-comments-process","position":7},{"hierarchy":{"lvl1":"I like Rust‚Äôs governance structure","lvl2":"How do these teams stay on the same page?"},"type":"lvl2","url":"/blog/2018/rust-governance#how-do-these-teams-stay-on-the-same-page","position":8},{"hierarchy":{"lvl1":"I like Rust‚Äôs governance structure","lvl2":"How do these teams stay on the same page?"},"content":"One question you may have from all of this is ‚Äúhow does the Rust community stay cohesive\nwhen all the teams are making decisions on their own?‚Äù. That‚Äôs what the core team is\nfor. In short, the core team is at least made up of leaders from each of the sub-teams\nwithin Rust. The job of the core team is to have a global perspective on the Rust\ncommunity. They make decisions about values that Rust uses in making decisions, and high-level goals that the community should pursue. They also perform project-wide decision making such\nas creating (or shutting down) specific sub-teams.","type":"content","url":"/blog/2018/rust-governance#how-do-these-teams-stay-on-the-same-page","position":9},{"hierarchy":{"lvl1":"I like Rust‚Äôs governance structure","lvl2":"Wrapping up"},"type":"lvl2","url":"/blog/2018/rust-governance#wrapping-up","position":10},{"hierarchy":{"lvl1":"I like Rust‚Äôs governance structure","lvl2":"Wrapping up"},"content":"I like the Rust community governance structure because it is flexible, transparent, and explicit.\n\nIt‚Äôs flexible because this structure treats the complexity of Rust as a feature, not a bug.\nBy giving decision-making power to the sub-teams within the community, they‚Äôre recognizing\nthe unique perspective those teams bring to the table, and credit them with the ability to\nmake the right decision over their domains.\n\nThe governance structure is explicit in that it formally defines roles in the Rust\ncommunity so it‚Äôs clear ‚Äúwho is responsible for what‚Äù. Note that many of these roles\nare of a non-technical nature. These are often ‚Äúglossed over‚Äù in other projects, but they\nare a crucial part of building an open community.\n\nFinally, the RFC process is transparent in that all discussion happens in the open (on the Pull Request).\nMoreover, it is also explicit because there‚Äôs a clearly-stated process for how these\ndecisions happen. This curbs the possibility that decisions will be seen as made in ‚Äúback-room‚Äù\nconversations and builds trust in the process.\n\nOver the next few weeks I‚Äôll keep exploring the Rust community‚Äôs structure because I think\nit‚Äôs fascinating. Some questions that I‚Äôve still got are:\n\nWhat does it look like when sub-teams make decisions that span the whole community?\n\nIs there tension between the core team and individual community members (because there‚Äôs\nan extra layer of bureaucracy in the sub-teams)\n\nHow do they ensure that these teams are comprised of stakeholders from different perspectives?\n\nWhat‚Äôs going wrong with this structure? What are the downsides?\n\nWho decides the membership of the teams? How do the grow / shrink / disappear?\n\nIf anybody has thoughts or comments on the above, I‚Äôd love to hear them!","type":"content","url":"/blog/2018/rust-governance#wrapping-up","position":11},{"hierarchy":{"lvl1":"Adding copy buttons to code blocks in Sphinx"},"type":"lvl1","url":"/blog/2018/sphinx-copy-buttons","position":0},{"hierarchy":{"lvl1":"Adding copy buttons to code blocks in Sphinx"},"content":"NOTE: This is now a sphinx extension! Thanks to some friendly suggestions, I‚Äôve written\nthis up as a super tiny sphinx extension. Check it out here: \n\nhttps://‚Äãgithub‚Äã.com‚Äã/choldgraf‚Äã/sphinx‚Äã-copybutton\n\nSphinx is a fantastic way to build\ndocumentation for your Python package. On the Jupyter project, we use it\nfor almost all of our repositories.\n\nA common use for Sphinx is to step people through a chunk of code. For example,\nin the \n\nZero to JupyterHub for Kubernetes\nguide we step users through a number of installation and configuration steps.\n\nA common annoyance is that there is a lot of copy/pasting involved. Sometimes\nyou accidentally miss a character or some whitespace. So, I spent a bit of time\nfiguring out how to automatically embed a copy button into code blocks. It\nturns out this is pretty easy!\n\nHere‚Äôs what the final result will look like (just hover the code block below)wow = this_text\nis_so = much*more*copyable","type":"content","url":"/blog/2018/sphinx-copy-buttons","position":1},{"hierarchy":{"lvl1":"Adding copy buttons to code blocks in Sphinx","lvl2":"Adding a copy button to your Sphinx code blocks"},"type":"lvl2","url":"/blog/2018/sphinx-copy-buttons#adding-a-copy-button-to-your-sphinx-code-blocks","position":2},{"hierarchy":{"lvl1":"Adding copy buttons to code blocks in Sphinx","lvl2":"Adding a copy button to your Sphinx code blocks"},"content":"To accomplish this we‚Äôll use the excellent \n\nclipboard.js\nwhich provides the machinery for copying the contents of an HTML element as well\nas \n\njquery for modifying our built documentation on-demand.\n\nThe result will be a Sphinx site with code blocks that display a copy button\nwhen you hover over them. You can see how it looks on this very page, which uses a\nsimilar method (but is built with Jekyll).\n\nHere‚Äôs what you should do:\n\nAdd the clipboard.js javascript. Create a javascript\nscript called doc/_static/custom.js. In the file, put the following\ncode (see comments for explanation):function addCopyButtonToCode(){\n// get all code elements\nvar allCodeBlocksElements = $( \"div.highlight pre\" );\n\n// For each element, do the following steps\nallCodeBlocksElements.each(function(ii) {\n// define a unique id for this element and add it\nvar currentId = \"codeblock\" + (ii + 1);\n$(this).attr('id', currentId);\n\n// create a button that's configured for clipboard.js\n// point it to the text that's in this code block\n// add the button just after the text in the code block w/ jquery\nvar clipButton = '<button class=\"btn copybtn\" data-clipboard-target=\"#' + currentId + '\"><img src=\"https://clipboardjs.com/assets/images/clippy.svg\" width=\"13\" alt=\"Copy to clipboard\"></button>';\n   $(this).after(clipButton);\n});\n\n// tell clipboard.js to look for clicks that match this query\nnew Clipboard('.btn');\n}\n\n$(document).ready(function () {\n// Once the DOM is loaded for the page, attach clipboard buttons\naddCopyButtonToCode();\n});\n\nAdd some CSS to make it pretty. Create a custom CSS file\ncalled doc/_static/custom.css (or add to one you‚Äôve\nalready got). In the file, put these lines:/* Copy buttons */\nbutton.copybtn {\n  webkit-transition: opacity .3s ease-in-out;\n  -o-transition: opacity .3s ease-in-out;\n  transition: opacity .3s ease-in-out;\n  opacity: 0;\n  padding: 2px 6px;\n  position: absolute;\n  right: 4px;\n  top: 4px;\n}\ndiv.highlight:hover .copybtn, div.highlight .copybtn:focus {\n    opacity: .3;\n}\ndiv.highlight .copybtn:hover {\n    opacity: 1;\n}\ndiv.highlight {\n    position: relative;\n}\n3. **Link these scripts in your configuration.** You need to link your\n custom JS and CSS scripts, as well as the clipboard.js script so it ships with\n your site. In your `conf.py` file, add the following function/lines (or add to one you've already\n got defined).\n\n ```python\n def setup(app):\n     app.add_stylesheet('custom.css')\n     app.add_javascript(\"custom.js\")\n     app.add_javascript(\"https://cdn.jsdelivr.net/npm/clipboard@1/dist/clipboard.min.js\")\n\nAnd that‚Äôs it! Once you clear your Sphinx cache and re-build your site, you should\nnow have buttons that appear when you hover over them, and that copy\nthe text inside when you click them.","type":"content","url":"/blog/2018/sphinx-copy-buttons#adding-a-copy-button-to-your-sphinx-code-blocks","position":3},{"hierarchy":{"lvl1":"Adding copy buttons to code blocks in Sphinx","lvl2":"Thanks"},"type":"lvl2","url":"/blog/2018/sphinx-copy-buttons#thanks","position":4},{"hierarchy":{"lvl1":"Adding copy buttons to code blocks in Sphinx","lvl2":"Thanks"},"content":"Many thanks to \n\nthis StackOverflow post\nfor the majority of the code that led to this hack!","type":"content","url":"/blog/2018/sphinx-copy-buttons#thanks","position":5},{"hierarchy":{"lvl1":"Three things I love about CircleCI"},"type":"lvl1","url":"/blog/2019/2019-01-29-three-things-circleci","position":0},{"hierarchy":{"lvl1":"Three things I love about CircleCI"},"content":"I recently had to beef up the continuous deployment of Jupyter Book, and used\nit as an opportunity to learn a bit more about CircleCI‚Äôs features. It turns out,\nthey‚Äôre pretty cool! Here are a few of the things that I learned this time around.\n\nFor those who aren‚Äôt familiar with CircleCI, it is a service that runs Continuous\nIntegration and Continuous Deployment (CI/CD) workflows for projects. This basically\nmeans that they manage many kinds of infrastructure that can launch jobs that run\ntest suites, deploy applications, and test on many different environments.\n\nHere are some cool things that I now have a much better appreciation for:","type":"content","url":"/blog/2019/2019-01-29-three-things-circleci","position":1},{"hierarchy":{"lvl1":"Three things I love about CircleCI","lvl2":"Re-run a job with SSH access"},"type":"lvl2","url":"/blog/2019/2019-01-29-three-things-circleci#re-run-a-job-with-ssh-access","position":2},{"hierarchy":{"lvl1":"Three things I love about CircleCI","lvl2":"Re-run a job with SSH access"},"content":"Often when tests don‚Äôt pass or a build otherwise fails, it‚Äôs really helpful to be\nable to get into the machine itself and just start poking around. It turns out that\nCircleCI makes this really easy! If a build has failed, then you can use the drop-down\nmenu next to the ‚ÄúRestart Job‚Äù button to select ‚ÄúRestart Job with SSH‚Äù. The next time\nthe job fails (which it probably will, since you‚Äôve just restarted a job that already\nfailed once), CircleCI will print the IP address and SSH command to connect to that\nmachine remotely.","type":"content","url":"/blog/2019/2019-01-29-three-things-circleci#re-run-a-job-with-ssh-access","position":3},{"hierarchy":{"lvl1":"Three things I love about CircleCI","lvl2":"Persisting files between jobs with Workspaces"},"type":"lvl2","url":"/blog/2019/2019-01-29-three-things-circleci#persisting-files-between-jobs-with-workspaces","position":4},{"hierarchy":{"lvl1":"Three things I love about CircleCI","lvl2":"Persisting files between jobs with Workspaces"},"content":"Everything CircleCI does is based around containers - each job has a Docker image\nenvironment specified (and CircleCI curates a large list of containers for testing).\nOne challenge this introduces is that it can be more complex to use jobs that have\nmultiple languages or tools installed. You can always manually configure this, but\nI‚Äôve found that another easy solution is to split your task across multiple jobs,\nand persist some of the files between them with CircleCI workspaces.\n\nTo set up a CircleCI workspace, you first need two jobs, thenjobs:\n  build_files:\n    docker:\n      # We use a Python image to test our files and run the test suite\n      - image: circleci/python:3.6-stretch\n    steps:\n      # Some steps to build files we need in another job\n      # Assume it places the built files into a folder called `_build/`\n      - run: build_stuff_with_python\n\n      # Persist the specified paths (see https://circleci.com/docs/2.0/workflows/#using-workspaces-to-share-data-among-jobs)\n      - persist_to_workspace:\n          # The root of the workspace, here just the CWD\n          root: .\n          # The sub-paths of the workspace to persist\n          paths:\n            - _build/\n\n  deploy_files:\n    docker:\n      # We'll use a Ruby image to deploy our files\n      - image: circleci/ruby:2.6\n    steps:\n      # Connect the files from the last job to this job\n      - attach_workspace:\n          # Must be absolute path or relative path from working_directory\n          at: /tmp/workspace\n\n      # Our final deployment steps\n      - run: deploy_files_with_ruby\n\nFinally, we‚Äôll set up a CircleCI workflow that runs the deployment job only after the\nbuild job as finished, since the deploy job depends on files that are created by the\nbuild job.workflows:\n  version: 2\n  default:\n    jobs:\n      - build_files\n      #\n      - deploy_files:\n          requires:\n            - build_files\n\nIn this way, we‚Äôve split our task (build a bunch of files, then deploy them online)\ninto two different jobs. One that builds files with a Python container, and another that\ndeploys them with a Ruby container.","type":"content","url":"/blog/2019/2019-01-29-three-things-circleci#persisting-files-between-jobs-with-workspaces","position":5},{"hierarchy":{"lvl1":"Three things I love about CircleCI","lvl2":"Re-use code snippets with Commands"},"type":"lvl2","url":"/blog/2019/2019-01-29-three-things-circleci#re-use-code-snippets-with-commands","position":6},{"hierarchy":{"lvl1":"Three things I love about CircleCI","lvl2":"Re-use code snippets with Commands"},"content":"Finally, many times you‚Äôd like to re-use the same set of snippets across multiple\npoints of your CircleCI jobs. In 2.1, CircleCI added a new feature called Commands\nthat does this fairly simply. Commands are kind of like functions in that they\nwrap up a collection of steps that can be re-used and parameterized. This means you\ncan define a ‚Äútemplate‚Äù of a collection of steps, then fill-in missing fields in that\ntemplate in order to modify its behavior.\n\nFor example, here‚Äôs a Command template to build a site with Jekyll:commands:\n  build_site:\n    description: \"Build the site with Jekyll\"\n    parameters:\n      # We'll define one parameter that lets us pass build arguments to Jekyll build\n      build_args:\n        type: string\n        default: \"\"\n    steps:\n      - run:\n          name: Build the website\n          # Note the << parameters.param >> syntax that lets you define your own inputs\n          command: bundle exec jekyll build << parameters.build_args >>\n\nNow, we can re-use this command throughout our build steps. Here are two jobs that use\nthis command in different ways:jobs:\n  # Build the site to store artifacts\n  build_with_params:\n    steps:\n      # Build the site's HTML w/ the base_url for CircleCI artifacts\n      - build_site:\n          build_args: --baseurl /0/html/\n  # Build the site to store artifacts\n  build_without_params:\n    steps:\n      # Build the site's HTML w/ defaults for Jekyll\n      - build_site:\n          build_args: \"\"\n\nEach of these jobs uses the command specified above in build_site, but\nthey use the build_args parameter to modify its behavior each time.","type":"content","url":"/blog/2019/2019-01-29-three-things-circleci#re-use-code-snippets-with-commands","position":7},{"hierarchy":{"lvl1":"Thoughts from the Jupyter team meeting 2019"},"type":"lvl1","url":"/blog/2019/2019-03-16-jupyter-dev","position":0},{"hierarchy":{"lvl1":"Thoughts from the Jupyter team meeting 2019"},"content":"I just got back from a week-long Jupyter team meeting that was somehow both\nvery tiring and energizing at the same time. In the spirit of openness, I‚Äôd\nlike to share some of my experience. While it‚Äôs still fresh in my mind,\nhere are a few takeaways that occurred to me throughout the week.\n\nNote that these are my personal (rough) impressions, but they shouldn‚Äôt be taken as a\nstatement from the project/community itself.","type":"content","url":"/blog/2019/2019-03-16-jupyter-dev","position":1},{"hierarchy":{"lvl1":"Thoughts from the Jupyter team meeting 2019","lvl2":"Jupyter has a huge and diverse set of users"},"type":"lvl2","url":"/blog/2019/2019-03-16-jupyter-dev#jupyter-has-a-huge-and-diverse-set-of-users","position":2},{"hierarchy":{"lvl1":"Thoughts from the Jupyter team meeting 2019","lvl2":"Jupyter has a huge and diverse set of users"},"content":"The first thing is probably unsurprising to many people, but was really driven\nhome at this meeting, is that there are so many Jupyter users our there. These\npeople come from all different walks of life - some are at huge tech companies,\nsome are scientists, some are educators, some are students. Some are from western\ncountries but many are not, some have wealth, some do not. Jupyter\n(notebooks, anyway...more on that in a second) has really caught fire across\na wide slice of society.\n\nThis is both a great thing and a challenge. Appreciating the size of the Jupyter\nuser community also made me realize that many of these groups have different\nmotives and goals. Jupyter was originally born out of a mission to serve\nscientists and educators, to create public goods,\nand to be a democratizing technology that empowers\nmany different kinds of people in the world. I think Jupyter is still serving\nthis role, but that as the Jupyter user community has grown, the voices of\nscience and education may be getting smaller relative to the gigantic and\nwell-resourced community of ‚Äúenterprise users‚Äù. I hope that we can find ways\nto balance these interests in the project in a way that keeps Jupyter a\nproject for all.","type":"content","url":"/blog/2019/2019-03-16-jupyter-dev#jupyter-has-a-huge-and-diverse-set-of-users","position":3},{"hierarchy":{"lvl1":"Thoughts from the Jupyter team meeting 2019","lvl2":"Jupyter needs to grow its contributor community"},"type":"lvl2","url":"/blog/2019/2019-03-16-jupyter-dev#jupyter-needs-to-grow-its-contributor-community","position":4},{"hierarchy":{"lvl1":"Thoughts from the Jupyter team meeting 2019","lvl2":"Jupyter needs to grow its contributor community"},"content":"While the user-base of Jupyter is fairly large and complex, the community\nof contributors (people that help in issues, help grow the community, help others\nuse Jupyter, or contribute code to Jupyter tools) needs to grow. We have had a\nrelatively stable group of contributors in the Jupyter ecosystem, but I think\nwe need to foster more ‚Äúorganic‚Äù growth with others who jump in and become core parts\nof the team. As an open project, we depend on the good-will and volunteer\ntime of others who want to join the community and participate. The fact that we\nhaven‚Äôt seen a steady growth in contributors (particularly from a pool of people\nmore diverse than the current contributors), tells me that we have a lot of work\nto do in creating obvious pathways to connect with, and grow within, the Jupyter\ncommunity.\n\nWe spent a morning session discussing diversity and inclusion, reading\n\n\nan excellent slideshow on 10 actionable steps to increase D+I.\nIt was a good reminder that recognizing systemic biases against certain groups\nof people does not mean abdicating responsibility as an individual to personally\ncreate a more inclusive environment. A couple of particular points that I hope\nwe can make progress on in the coming months:\n\nHave a moderator and an agenda for meetings. Conduct meetings (in-person or remote)\nwith a moderator who builds a queue of\nspeakers and gives the floor to each of them in turn. While it‚Äôs easy to\ntreat an unstructured meeting as ‚Äúinformal and fun‚Äù, it also makes them significantly\nless-productive and harder to participate for many. I really enjoyed reading\nthe article \n\nhow to make remote meetings not suck\n(SPOILER: the answer is to make all meetings not suck by providing structure\nand moderation). Taking small steps towards team processes that make meetings more\nparticipatory and predictable would go a long way.\n\nMake more active efforts to bring new+diverse members into\nthe community. You can‚Äôt expect organic contributor growth from populations that\nhave very little representation on the project already. We need to continue\nmaking active efforts at engaging these communities and bringing in new people.\nFor example, we had a new team member join the meeting as part of an\n\n\nOutreachy internship, and I really appreciated their\nperspective on many of the issues we discussed. Had we not taken these active steps\nto bringing a new Jovyan into the community, those perspectives would never have been\nshared at the meeting.\n\nCreate explicit roles and ways to contribute. We also spoke at length about\nthe many different things that must be done to have not only cycles of code\ndevelopment, but also a healthy community around that code. Many times this work\nis done in an unstructured and ad-hoc way. This is stressful for the people doing\nthe work (I often have no idea how much time I‚Äôve sunk into responding to issues,\nfor example), and it also makes the project team more opaque to others who might\nwish to join. If I am vaguely interested in contributing to JupyterHub, where\ndo I start? Some people have a clear path for how they could contribute, but I\nsuspect that there are many other ways that we can tell people ‚Äúit would be\nhelpful if you do XXX‚Äù.","type":"content","url":"/blog/2019/2019-03-16-jupyter-dev#jupyter-needs-to-grow-its-contributor-community","position":5},{"hierarchy":{"lvl1":"Thoughts from the Jupyter team meeting 2019","lvl2":"The Jupyter ecosystem needs better explaining"},"type":"lvl2","url":"/blog/2019/2019-03-16-jupyter-dev#the-jupyter-ecosystem-needs-better-explaining","position":6},{"hierarchy":{"lvl1":"Thoughts from the Jupyter team meeting 2019","lvl2":"The Jupyter ecosystem needs better explaining"},"content":"Another topic we discussed was the fact that Jupyter hasn‚Äôt clearly\nexplained its technology stack, how everything fits together, what problems\nit‚Äôs meant to solve, and how it interfaces with the outside community. The\nmajority of people think of ‚ÄúJupyter Notebooks‚Äù when they think of Jupyter,\nbut often don‚Äôt recognize that there are a lot of pieces under the hood as\nwell (e.g. the Notebook application is both a kernel / server architecture, an\nunderlying notebook document specification / format, and a particular notebook UI).\n\nJupyter still has challenges in making other major projects more discoverable\n(e.g. JupyterHub for sharing Jupyter environments on shared infrastructure,\nor other user interfaces like Jupyter Lab or Nteract). Moreover, the project is\nalso starting to be picked up by companies and projects that\nare fairly liberal with the use of the ‚ÄúJupyter‚Äù name. Is your tool still\n‚Äúa Jupyter Notebook interface‚Äù if it only has the ability to export to a\nJupyter Notebook, but uses no other Jupyter tech? I‚Äôm not sure - but either way,\nthere should be a clear answer to that question otherwise the project will\nstart to be defined by other people rather than itself.","type":"content","url":"/blog/2019/2019-03-16-jupyter-dev#the-jupyter-ecosystem-needs-better-explaining","position":7},{"hierarchy":{"lvl1":"Thoughts from the Jupyter team meeting 2019","lvl2":"Governing open projects is really hard"},"type":"lvl2","url":"/blog/2019/2019-03-16-jupyter-dev#governing-open-projects-is-really-hard","position":8},{"hierarchy":{"lvl1":"Thoughts from the Jupyter team meeting 2019","lvl2":"Governing open projects is really hard"},"content":"Finally, something I‚Äôve grown to appreciate more over the last year\nis how difficult it is to balance decision-making, power, and participatory\ncommunity dynamics in a large, multi-stakeholder, open project like Jupyter.\nThere were a lot of conversations around the current governance model of\nthe project, and how this wasn‚Äôt currently serving the community in a satisfying\nway. As the number of stakeholders in the project grows, their\nneeds may start to move in opposing directions. Keeping a project functional\nand productive, while still balancing between these needs, is a massive task.\n\nThis becomes particularly challenging when the stakeholders in the project have\ndiffering levels of resources. For example, Jupyter has always been dedicated\nto building tools for scientists and educators. However, these individuals are\noften part of organizations with vastly fewer resources than tech companies.\nHow can we ensure that the voices of these two groups have a balanced weight?\nIf company X decides they want to contribute a new feature the Jupyter\nNotebook interface, and they put a team of 10 people on it, how does this team\ninteract with the decision-making processes of the Jupyter project? What if the\ncore maintainers are volunteers with limited time to review PRs? What if there are\ndisagreements between the company team‚Äôs internal mandate, and what is best for the\nJupyter community? Finally, what if there aren‚Äôt good channels of communication\nand processes of decision-making that encourage nuanced, in-depth discussion to\nfacilitate the above points?\n\nFrom a company‚Äôs perspective, there‚Äôs always the option of going off and doing your\nown thing. But Jupyter doesn‚Äôt have this option. In its current state, Jupyter‚Äôs resources\nare contrained to the groups that decide to participate. To that extent, a few\nthings that we need to improve:\n\nMake it easier for others to open new topics of discussion with\nthe Jupyter community in a ‚Äúformal‚Äù decision-making process. I think\n\n\nrecent efforts to improve the Jupyter Enhancement Proposal\nprocess are a great start. This should make it easier for stakeholders to voice\ntheir concerns and needs in an open way that allows many in the community to\nparticipate.\n\nFind a way to avoid the ‚Äúgovernance by resources‚Äù trap. Many open projects in the\ntech community adopt a model like ‚Äúyou have decision-making power\nthat scales with the resources you devote to the project‚Äù. That‚Äôs fine if everybody\nhas a similar amount of resources, but many of Jupyter‚Äôs stakeholders don‚Äôt.\nIf we want members of the\neducational and scientific/academic community to participate, or people that aren‚Äôt\nrepresented in the current tech and data industry to participate, we need to find a\nway that encourages the contribution of resources from organizations that have\nthem, but that normalizes decision-making power so that resources don‚Äôt guarantee\nyou a larger voice than others. Ultimately, the goal of the Jupyter project is\nto create public goods that benefit everybody, and I fear we‚Äôll lose sight of this\ngoal if you need to be able to fund a team of developers in order to participate\nin the project.\n\nVest power in more systems and processes, rather than in individual people. Part of\nthe challenges currently facing Jupyter is that it began as a relatively small project\nwith a tight-knit team of developers that all knew each other. In that case, it made\nsense to adopt a traditional BDFL+governing council kind of model. It‚Äôs now clear that\nthis model is inadequate at balancing the nuanced issues described above. Given the\ncomplexity of Jupyter‚Äôs community, I think that we need to move away from thinking about\nindividual people as the sources of power, and instead think about a system that\ndivides power in intentional ways, as well as a process for how individuals can\nmove through that system in a way that addresses some of the concerns above.\n\nSo there are a few thoughts of my own, and I look forward to seeing how others feel\nmoving forward. There‚Äôs a lot happening in the Jupyter ecosystem,\nand I‚Äôm excited to be a part of it.","type":"content","url":"/blog/2019/2019-03-16-jupyter-dev#governing-open-projects-is-really-hard","position":9},{"hierarchy":{"lvl1":"A few recent talks"},"type":"lvl1","url":"/blog/2019/2019-06-25-a-few-talks","position":0},{"hierarchy":{"lvl1":"A few recent talks"},"content":"Lately I‚Äôve given quite a number of talks about the Jupyter and Binder\necosystems for various purposes. Before each of the talks, I make the\nslides available at a public address in case others are interested in\nfollowing up with the material. For those who missed the talks (or the\nsubsequent tweets about them), here are a few of the more recent ones.\n\nA word of warning: there‚Äôs a lot of overlap between these talks - I‚Äôm not\ncrazy enough to re-invent the wheel each time I have to speak. However, maybe\nfolks will find some value in the different angles taken in each case.","type":"content","url":"/blog/2019/2019-06-25-a-few-talks","position":1},{"hierarchy":{"lvl1":"A few recent talks","lvl2":"The Berkeley Data 8 Stack (60 min)"},"type":"lvl2","url":"/blog/2019/2019-06-25-a-few-talks#the-berkeley-data-8-stack-60-min","position":2},{"hierarchy":{"lvl1":"A few recent talks","lvl2":"The Berkeley Data 8 Stack (60 min)"},"content":"This talk covers some of the technical infrastructure behind the pedagogical\nefforts here at UC Berkeley. It‚Äôs a brief dive into JupyterHub distributions\nand how they fit into an institution like UC Berkeley.\n\nhttps://‚Äãbit‚Äã.ly‚Äã/2019‚Äã-data8‚Äã-jupyter","type":"content","url":"/blog/2019/2019-06-25-a-few-talks#the-berkeley-data-8-stack-60-min","position":3},{"hierarchy":{"lvl1":"A few recent talks","lvl2":"Open infrastructure for open science (5 min)"},"type":"lvl2","url":"/blog/2019/2019-06-25-a-few-talks#open-infrastructure-for-open-science-5-min","position":4},{"hierarchy":{"lvl1":"A few recent talks","lvl2":"Open infrastructure for open science (5 min)"},"content":"This one was a quick overview of the Binder ecosystem for a community focused\nmostly around reproducibility and publishing. Lots of action-items in here :-)\n\nhttps://‚Äãbit‚Äã.ly‚Äã/2019‚Äã-elife‚Äã-cc‚Äã-holdgraf","type":"content","url":"/blog/2019/2019-06-25-a-few-talks#open-infrastructure-for-open-science-5-min","position":5},{"hierarchy":{"lvl1":"A few recent talks","lvl2":"Reproducibility with Binder @ ASM"},"type":"lvl2","url":"/blog/2019/2019-06-25-a-few-talks#reproducibility-with-binder-asm","position":6},{"hierarchy":{"lvl1":"A few recent talks","lvl2":"Reproducibility with Binder @ ASM"},"content":"This covers the Binder Project and the tools that it creates for open, reproducible\nscience. It was geared towards a less-technical audience than many of the\nconferences I normally speak at. It covers more of a users‚Äô perspective of \n\nmybinder.org,\nand how this might fit into reproducible publishing.\n\nhttps://‚Äãbit‚Äã.ly‚Äã/2019‚Äã-ASM‚Äã-jupyter","type":"content","url":"/blog/2019/2019-06-25-a-few-talks#reproducibility-with-binder-asm","position":7},{"hierarchy":{"lvl1":"A few recent talks","lvl2":"Reproducibility with Binder @ UW Reproducibility workshop"},"type":"lvl2","url":"/blog/2019/2019-06-25-a-few-talks#reproducibility-with-binder-uw-reproducibility-workshop","position":8},{"hierarchy":{"lvl1":"A few recent talks","lvl2":"Reproducibility with Binder @ UW Reproducibility workshop"},"content":"This talk goes into more depth on the technical side of the reproducibility efforts\nwith Jupyter and Binder. It was given in the context of a \n\ntwo-day workshop on\nreproducible environments and publishing.\n\nhttps://‚Äãbit‚Äã.ly‚Äã/2019‚Äã-uw‚Äã-reproducibility‚Äã-jupyter","type":"content","url":"/blog/2019/2019-06-25-a-few-talks#reproducibility-with-binder-uw-reproducibility-workshop","position":9},{"hierarchy":{"lvl1":"A few recent talks","lvl2":"Binder in the cloud @ csvconf"},"type":"lvl2","url":"/blog/2019/2019-06-25-a-few-talks#binder-in-the-cloud-csvconf","position":10},{"hierarchy":{"lvl1":"A few recent talks","lvl2":"Binder in the cloud @ csvconf"},"content":"A broad overview of the Binder ecosystem and the technical stack that lies\nunderneath it, as well as a short aside on the composable, modular approach\nthat Jupyter takes towards building these tools\n\nhttps://‚Äãbit‚Äã.ly‚Äã/2019‚Äã-binder‚Äã-csvconf","type":"content","url":"/blog/2019/2019-06-25-a-few-talks#binder-in-the-cloud-csvconf","position":11},{"hierarchy":{"lvl1":"A few recent talks","lvl2":"Jupyter Book @ Strada"},"type":"lvl2","url":"/blog/2019/2019-06-25-a-few-talks#jupyter-book-strada","position":12},{"hierarchy":{"lvl1":"A few recent talks","lvl2":"Jupyter Book @ Strada"},"content":"An overview of the \n\njupyter book project. This\ncovers the technical stack behind the tool that converts collections of\nJupyter Noteoboks into an HTML website book.\n\nhttps://‚Äãbit‚Äã.ly‚Äã/2019‚Äã-strada‚Äã-jupyter‚Äã-book","type":"content","url":"/blog/2019/2019-06-25-a-few-talks#jupyter-book-strada","position":13},{"hierarchy":{"lvl1":"Automating Jupyter Book deployments with CI/CD"},"type":"lvl1","url":"/blog/2019/2019-10-11-automating-jb","position":0},{"hierarchy":{"lvl1":"Automating Jupyter Book deployments with CI/CD"},"content":"Lately I‚Äôve spent a lot of time trying to reduce the friction involved\nin deploying Jupyter Book as well as contributing to the project.\nFeatures are a great carrot, but ultimately getting engagement is also\nabout lowering barriers to entry and showing people a path forward.\nJupyter Book is a relatively straightforward project, but it involves\na few technical pieces that can be painful to use (thanks Jekyll).\n\nRecently I experimented with whether we can automate deploying a Jupyter Book online.\nUsing continuous integration / deployment services seems like a natural place\nto try this out. One can upload a barebones set of code to a GitHub repository,\nthen configure a build system to create a book and deploy it online from there.\nThis blog post is a place to keep track of the current state of affairs for this workflow.\n\nI‚Äôll publish the latest configuration files for this at \n\nthis repository.","type":"content","url":"/blog/2019/2019-10-11-automating-jb","position":1},{"hierarchy":{"lvl1":"Automating Jupyter Book deployments with CI/CD","lvl2":"The general set of steps involved"},"type":"lvl2","url":"/blog/2019/2019-10-11-automating-jb#the-general-set-of-steps-involved","position":2},{"hierarchy":{"lvl1":"Automating Jupyter Book deployments with CI/CD","lvl2":"The general set of steps involved"},"content":"We‚Äôll start with the simplest possible Jupyter Book configuration:\nto have a single folder with a collection of content inside. The folder looks\nlike this:.\n‚îú‚îÄ‚îÄ content\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 01\n|   ‚îÇ   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ notebook1.ipynb\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ notebook2.ipynb\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 02\n|   ‚îÇ   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ notebook3.ipynb\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ mdfile4.md\n...\n|\n‚îî‚îÄ‚îÄ configuration_files_for_cicd/\n\nThere‚Äôs no table of contents, and no configuration file (though we could add\nthese if we wish). If Jupyter Book is used to create a new book with some\ncontent, but no TOC is given, it‚Äôll automatically generate one.\n\nOur goal is to do the following in an automated fashion:\n\nBuild a new Jupyter Book template from this content folder (with jupyter-book create)\n\nBuild page HTML for the book (with jupyter-book build)\n\nGenerate the book‚Äôs site with Jekyll (with bundle exec jekyll build)\n\nHost the results somewhere online\n\nBelow are attempts to do this with CI/CD. I‚Äôll update this post\nas new options become available (and hopefully push some stuff to\nthe Jupyter Book documentation).","type":"content","url":"/blog/2019/2019-10-11-automating-jb#the-general-set-of-steps-involved","position":3},{"hierarchy":{"lvl1":"Automating Jupyter Book deployments with CI/CD","lvl2":"Netlify"},"type":"lvl2","url":"/blog/2019/2019-10-11-automating-jb#netlify","position":4},{"hierarchy":{"lvl1":"Automating Jupyter Book deployments with CI/CD","lvl2":"Netlify"},"content":"By far the easiest way to accomplish the above is with the online\nwebsite provider \n\nNetlify. This was my\nfirst experience with the Netlify service, and I must say that I was\nreally pleased (thanks to \n\nElizabeth DuPre\nfor the recommendation and \n\nNetlify tutorial).\n\nNetlify has automatic deployment built into its service, since that‚Äôs\nthe whole point of the site - to deploy websites from online repositories.\nTo do this, I simply had to connect Netlify to my \n\nbook content repository\nand tell it to start building a site from that repository‚Äôs contents.\n\nI modified the build instructions using a custom ‚Äúbuild‚Äù command.\nNetlify runs this command every time it tries to build your site.\nYou can configure this by creating a netlify.toml file and putting it\nin the root of your repository. \n\nHere‚Äôs a link to my configuration file.\n\nThe full text of that TOML file looks like this:[build]\n  command = \"\"\"\n    gem install bundler -v '2.0.2'\n    pip install -U git+https://github.com/jupyter/jupyter-book\n    jupyter-book create mybook --content-folder content\n    cd mybook\n    jupyter-book build ./ --overwrite\n    make install\n    bundle exec jekyll build\n    \"\"\"\n\n  publish = \"mybook/_site\"\n\nThere are two pieces to this: the command section is the command to\nrun first, when a new commit is pushed to a branch. The publish section\ndefines the location where Netlify will look for the finished HTML (AKA, my book website).\n\nNote that in the jupyter-book create command above, I used --content-folder to\ntell Jupyter Book to use some pre-existing content when it generated my book template.\nIn addition, note that I could immediately install both Python and Ruby packages - that‚Äôs\nbecause Netlify‚Äôs base build environment has both languages installed already! #region {\"tags\": [\"popout\"]} \n\nOne gotcha on getting Netlify to work was configuring it to use a Python 3.X environment.\nThat‚Äôs accomplished with the runtime.txt file \n\nat this location. #endregion \n\nBy adding this configuration to my site, Netlify immediately started building\nand hosting the book. You can find \n\nthat book deployment here.","type":"content","url":"/blog/2019/2019-10-11-automating-jb#netlify","position":5},{"hierarchy":{"lvl1":"Automating Jupyter Book deployments with CI/CD","lvl2":"CircleCI"},"type":"lvl2","url":"/blog/2019/2019-10-11-automating-jb#circleci","position":6},{"hierarchy":{"lvl1":"Automating Jupyter Book deployments with CI/CD","lvl2":"CircleCI"},"content":"CircleCI is a website most-commonly used for running\ntest suites and deploying things into production once those tests pass.\nFortunately, deploying an HTML book is pretty similar!\n\nGetting Jupyter Book to build on CircleCI was a little bit trickier for two\nreasons:\n\nCircleCI has more specific environments in its build system. You can have a Python\nenvironment, or a Ruby environment, but not both.\n\nCircleCI has no concept of natively ‚Äúhosting‚Äù HTML content, so we had to piggy-back\non top of GitHub pages.\n\nLuckily, working around both of these issues was relatively straightforward.\nYou can find the CircleCI configuration that ended up working \n\nin the github repository.\n\nThere were two gotchas in there:\n\nI started off with using a Ruby environment rather than a Python environment.\nThat‚Äôs becuase I‚Äôve found Python to be much easier to install than Ruby. In fact,\ninstalling python was as easy as including sudo apt-get install python3-pip in\nmy commands.\n\nI had to use a GitHub SSH deploy key to be able to deploy my built HTML to GitHub\npages. You can find \n\ninstructions for how to do so in this post.\n\nOnce that was accomplished, this is the configuration that got the job done:version: 2.1\njobs:\n  build_book:\n    docker:\n      - image: circleci/ruby:2.6\n    steps:\n      - checkout\n      - run:\n          name: Install Python and dependencies to build page HTML\n          command: |\n            sudo apt-get install python3-pip\n            pip3 install --user -r requirements.txt\n            pip3 install --user -U git+https://github.com/jupyter/jupyter-book.git\n\n      - run:\n          name: Create book template and build page HTML\n          command: |\n            jupyter-book create mybook --content-folder content/\n            jupyter-book build ./mybook\n\n      - run:\n          name: Install ruby dependencies and build the book's website\n          command: |\n            cd mybook\n            make install\n            bundle exec jekyll build\n\n      # If we're on master, push to a gh-pages branch\n      - add_ssh_keys:\n          fingerprints:\n            - \"<my-public-fingerprint>\"\n      - run:\n          name: Push to gh-pages (if on master)\n          command: |\n            if [ $CIRCLE_BRANCH\t== \"master\" ]; then\n              pip3 install ghp-import\n              ghp-import -n -f -p mybook/_site;\n            else\n              echo \"Skipping deploy because we aren't on master\"\n            fi\n\n\nworkflows:\n  version: 2\n  default:\n    jobs:\n      - build_book\n\nYou can find the deployed site \n\nat this location.","type":"content","url":"/blog/2019/2019-10-11-automating-jb#circleci","position":7},{"hierarchy":{"lvl1":"Automating Jupyter Book deployments with CI/CD","lvl2":"Wrapping up"},"type":"lvl2","url":"/blog/2019/2019-10-11-automating-jb#wrapping-up","position":8},{"hierarchy":{"lvl1":"Automating Jupyter Book deployments with CI/CD","lvl2":"Wrapping up"},"content":"Ultimately, it was simpler than I expected to deploy a Jupyter Book with CI/CD.\nThere are lots of other services to explore (in particular, TravisCI and GitHub actions),\nbut I find it hard to believe anything would be more straightforward than Netlify.\n\nThat said, the process also made it clear that some pieces of the Jupyter Book API\nare a bit confusing. It felt natural to have my content in a single folder, and to\nbuild a book from that content, but this isn‚Äôt the ‚Äúdefault‚Äù way that the documentation\nrecommends. I‚Äôll let these ideas simmer a little bit and we‚Äôll see what comes out of it.","type":"content","url":"/blog/2019/2019-10-11-automating-jb#wrapping-up","position":9},{"hierarchy":{"lvl1":"What would Rust-style governance look like in Jupyter?"},"type":"lvl1","url":"/blog/2019/2019-10-13-rust-jupyter-governance","position":0},{"hierarchy":{"lvl1":"What would Rust-style governance look like in Jupyter?"},"content":"As I‚Äôve written about before, I \n\nlike Rust‚Äôs governance structure.\nI mean, who can‚Äôt get behind a community that\n\n\nlists governance as a top-level page on its website?\n\nJupyter is currently in the middle of\n\n\nfiguring out the next phase of its governance structure,\nand so I have been thinking about\nwhat this might look like. This post is a quick thought-experiment to explore what it‚Äôd mean\nto port over Rust‚Äôs governance directly into the Jupyter community.\n\nNote: I‚Äôm not an expert in Rust governance, so there are some assumptions made about its model\nbased on my outside perspective. Apologies if I miss any important details about the Rust model,\nbut this is mostly meant as as inspiration, not a report on Rust‚Äôs governance :-)","type":"content","url":"/blog/2019/2019-10-13-rust-jupyter-governance","position":1},{"hierarchy":{"lvl1":"What would Rust-style governance look like in Jupyter?","lvl2":"A quick recap of Rust‚Äôs governance structure"},"type":"lvl2","url":"/blog/2019/2019-10-13-rust-jupyter-governance#a-quick-recap-of-rusts-governance-structure","position":2},{"hierarchy":{"lvl1":"What would Rust-style governance look like in Jupyter?","lvl2":"A quick recap of Rust‚Äôs governance structure"},"content":"First off, how does Rust govern and organize itself? There are a few few\nkey pieces:\n\nAny significant changes to the codebase are proposed and discussed with a\n\n\nRequest for Comments process.\n\nThe Rust community is broken down into topic-specific teams. Each has a particular\ndomain over which they make decisions. For a list of several teams, check out\nthe \n\nRust governance page.\n\nThere is also a core-team that cuts across topic teams and has representatives\nfrom each topic team, they discuss project-wide matters (but rarely).\n\nA new RFC is assigned a team, as well as a ‚Äúshepherd‚Äù from that team. This person‚Äôs\njob is to move the RFC process forward, not to comment on or implement the RFC.\n\nAfter a discussion period, all members of the sub-team must vote to enter\na Final Comment Period. This should happen when ‚Äúenough information is presented in\nthe RFC to make a decision‚Äù. It triggers a week-long review window.\n\nAt the end of this window, a decision is made about what to do with the RFC. This\nis made by the members of the sub-team, who (I don‚Äôt think) have any strict decision-making\nrules, they can organize themselves in terms of decision-making.\n\nIf it is accepted, the RFC becomes ‚Äúactive‚Äù which is an invitation for people\nto work on implementing it.","type":"content","url":"/blog/2019/2019-10-13-rust-jupyter-governance#a-quick-recap-of-rusts-governance-structure","position":3},{"hierarchy":{"lvl1":"What would Rust-style governance look like in Jupyter?","lvl2":"What would this look like in Jupyter?"},"type":"lvl2","url":"/blog/2019/2019-10-13-rust-jupyter-governance#what-would-this-look-like-in-jupyter","position":4},{"hierarchy":{"lvl1":"What would Rust-style governance look like in Jupyter?","lvl2":"What would this look like in Jupyter?"},"content":"Jupyter is a complex and multi-faceted community, but so is Rust, so let‚Äôs see\nwhat this decision-making structure would look like in the Jupyter community.","type":"content","url":"/blog/2019/2019-10-13-rust-jupyter-governance#what-would-this-look-like-in-jupyter","position":5},{"hierarchy":{"lvl1":"What would Rust-style governance look like in Jupyter?","lvl3":"General decision-making principles and goals","lvl2":"What would this look like in Jupyter?"},"type":"lvl3","url":"/blog/2019/2019-10-13-rust-jupyter-governance#general-decision-making-principles-and-goals","position":6},{"hierarchy":{"lvl1":"What would Rust-style governance look like in Jupyter?","lvl3":"General decision-making principles and goals","lvl2":"What would this look like in Jupyter?"},"content":"First off, we would adopt many of the same decision-making goals of the RFC\nprocess. Here are a few key ones as I understand it:\n\nBe transparent - information about decision-making should be publicly available and\neasy to discover at any moment in time.\n\nBe inclusive - decision-making should strive to include many diverse voices in\nthe conversation.\n\nBe informative - the goal of the RFC process is to surface relevant information\nand perspectives for making a decision.\n\nBe productive - the goal of the RFC process is to move ideas forward in the community.\nIt should achieve a net-positive in ‚Äúenergy spent‚Äù vs. ‚Äúgenerated value to the community‚Äù.\n\nBe impactful - don‚Äôt use RFCs to bike-shed minor details, or implementation details for a PR.\nThey should be used for significant changes in a repository that require discussion at a high level.","type":"content","url":"/blog/2019/2019-10-13-rust-jupyter-governance#general-decision-making-principles-and-goals","position":7},{"hierarchy":{"lvl1":"What would Rust-style governance look like in Jupyter?","lvl3":"An RFC in the Jupyter Community","lvl2":"What would this look like in Jupyter?"},"type":"lvl3","url":"/blog/2019/2019-10-13-rust-jupyter-governance#an-rfc-in-the-jupyter-community","position":8},{"hierarchy":{"lvl1":"What would Rust-style governance look like in Jupyter?","lvl3":"An RFC in the Jupyter Community","lvl2":"What would this look like in Jupyter?"},"content":"First off, the mechanism for proposing, iterating on, and making decisions. In\nRust this is an RFC. in Jupyter, such a mechanism has already been proposed!\n\nThe \n\nJupyter Enhancement Proposals\nhave been around for quite some time, though have never been codified into law official\ndecision-making and have become a bit stale. I suspect this is partially because it‚Äôs\nunclear what kind of ‚Äúpower‚Äù the JEP process has.\n\nRecently, \n\nSafia kick-started a process to\n\n\nrevitalize the JEP process, and\nthe proposed process is quite close to what the Rust community uses. I generally think\nthat this PR is a huge improvement, though for the sake of this thought experiment, I‚Äôm\njust going to directly port over my understanding of Rust into this blog post.\n\nHere‚Äôs how a Rust-like process could work in Jupyter. Since already have JEPs, I‚Äôll\nreplace ‚ÄúRFC‚Äù with ‚ÄúJEP‚Äù.\n\nWe have a single ‚Äúenhancement-proposals‚Äù repository where JEP discussion happens\n\nThis repository has a template for new JEPs help people get started.\n\nNew JEPs begin with general conversations in the community. People get informal\nbuy-in and feedback through discussing in the \n\ncommunity forum\nor in GitHub repositories.\n\nIf a person wants to make their JEP ‚Äúofficial‚Äù, they fill in the JEP template and\nmake a pull-request to the repository.\n\nAfter an initial overview, a Jupyter team is assigned to the JEP.\n\nThat team then picks a shepherd (how the teams do this is up to them).\n\nThe shepherd oversees a process of feedback, asks for input from others in the\ncommunity, and directs attention to the JEP on the listservs, community forum, etc.\n\nWhen the shepherd thinks that the JEP is ready for a decision, they ask their\nteam to vote on whether it should enter a ‚Äúfinal review‚Äù phase. No more modifications\nshould be made to the JEP at this point.\n\nThis triggers a 7-day window for team members to review the JEP. At the\nend of the 7 days, the team votes (say, by a lazy consensus with 50% quorum)\non whether to accept the JEP.\n\nIf accepted, the JEP enters an ‚Äúactive‚Äù state and pull-requests are welcome to\nimplement it.","type":"content","url":"/blog/2019/2019-10-13-rust-jupyter-governance#an-rfc-in-the-jupyter-community","position":9},{"hierarchy":{"lvl1":"What would Rust-style governance look like in Jupyter?","lvl3":"The teams and peronnel needed to manage this process","lvl2":"What would this look like in Jupyter?"},"type":"lvl3","url":"/blog/2019/2019-10-13-rust-jupyter-governance#the-teams-and-peronnel-needed-to-manage-this-process","position":10},{"hierarchy":{"lvl1":"What would Rust-style governance look like in Jupyter?","lvl3":"The teams and peronnel needed to manage this process","lvl2":"What would this look like in Jupyter?"},"content":"The JEP process is the mechanism by which decisions get made, but what\nare the groups that oversee this mechanism? In the Rust community,\nthese teams are broken down by either technical or community topics\n(e.g., ‚Äúcompilers‚Äù, ‚Äúcommunity‚Äù, or ‚Äúpackaging‚Äù). The Jupyter community\nsimilarly has several focus-groups that touch different parts of the\ninteractive computing stack. Here are a few core ones that basically already exist:\n\nJupyterLab core\n\nJupyterHub core\n\nInfrastructure\n\nCommunity\n\nEvents\n\nOne could imagine beginning with this subset of teams, and adding others organically\nover time. Each of the teams listed above would manage JEP processes for their respective\ndomain. They would be given a list of repositories (and maybe a GitHub organization)\nto oversee, and when a new JEP came in, one of the the team members would be\nchosen to shepherd the process. The team would be the definitive source of\ndecision-making for topics in that domain.\n\nHere are a few others that come to mind - they‚Äôre a bit less well-defined and might\nbe good candidates for team growth in the future.\n\nThe notebook specification\n\nKernels and communication protocols\n\nVisualizations and widgets\n\nPublishing and document formats\n\nData specifications\n\nFinance and accounting\n\nTechnical accessibility\n\nDocumentation\n\nThere are a few other roles that would need to be created to facilitate this process:\n\nA JEP communicator - someone would need to manage the JEP infrastructure and process\nat a generic level. This doesn‚Äôt mean getting involved in individual JEPs, but making sure\nthe process as a whole is functioning, and potentially managing infrastructure around it\n(for example, maintaining a website that lists currently-active JEPs).\n\nA core team - would need to be created that cuts across the topic-specific teams.\nThis team would exercise large-scale decisions within the community but generally\nrarely exercise their power. Similar to a BDFL.\n\nA shepherd role - we‚Äôd need to formalize what a ‚Äúshepherd‚Äù is in the Jupyter community.\nPotentially sub-teams would modify this slightly to fit their own needs.","type":"content","url":"/blog/2019/2019-10-13-rust-jupyter-governance#the-teams-and-peronnel-needed-to-manage-this-process","position":11},{"hierarchy":{"lvl1":"What would Rust-style governance look like in Jupyter?","lvl2":"What‚Äôs the difference between current Jupyter and JEP Jupyter?"},"type":"lvl2","url":"/blog/2019/2019-10-13-rust-jupyter-governance#whats-the-difference-between-current-jupyter-and-jep-jupyter","position":12},{"hierarchy":{"lvl1":"What would Rust-style governance look like in Jupyter?","lvl2":"What‚Äôs the difference between current Jupyter and JEP Jupyter?"},"content":"Thinking through the above scenario, I don‚Äôt see too much distance between\nour current situation and a JEP-like process. We:\n\nAlready have informal topic groups, in the form of GitHub organizations,\nforum channels, and meetings (e.g. JupyterLab and JupyterHub come to mind).\n\nAlready have a JEP repository with some past proposals in it.\n\nAlready have the skeleton of a modernized JEP process thanks to Safia‚Äôs awesome work.\n\nWhat we‚Äôd need to do:\n\nmore officially codify the JEP process\n\nmake some topic-based teams official, and get people to accept roles on those teams\n\nbuild some infrastructure to support the JEP process (e.g. a website to make them searchable\nand discoverable)\n\nre-work team processes to encourage them to follow the JEP process over time (I suspect this would\nbe the hardest thing to do)","type":"content","url":"/blog/2019/2019-10-13-rust-jupyter-governance#whats-the-difference-between-current-jupyter-and-jep-jupyter","position":13},{"hierarchy":{"lvl1":"What would Rust-style governance look like in Jupyter?","lvl2":"What‚Äôs the difference between Jupyter and Rust?"},"type":"lvl2","url":"/blog/2019/2019-10-13-rust-jupyter-governance#whats-the-difference-between-jupyter-and-rust","position":14},{"hierarchy":{"lvl1":"What would Rust-style governance look like in Jupyter?","lvl2":"What‚Äôs the difference between Jupyter and Rust?"},"content":"Finally, while it‚Äôs interesting to port one community‚Äôs governance model directly onto another,\nthere are difference between the Jupyter and Rust projects, both technical and social ones.\nHere are a few differences I can think of that might have an impact on how this model would work.\n\nJupyter evolves fairly quickly - Whether it is JupyterLab development, the growth of new\nprotocols or deployments in the JupyterHub stack, or extensions of Jupyter tools for new\nuse-cases, Jupyter seems to move fairly quickly. The RFC (or JEP) model is one that intentionally\nslows things down, so perhaps we‚Äôd need to be more picky about when to follow such a model\nvs. when to allow codebases to grow more quickly.\n\nJupyter doesn‚Äôt have a process like this already - The recent JEP updates notwithstanding,\nJupyter doesn‚Äôt have a culture of following the JEP process already. This makes me think that\nadopting this process would need to be rolled out slowly over time, and in smaller increments\nin order to make sure teams buy-in to the process.\n\nJupyter doesn‚Äôt have many official roles/titles - Adding extra complexity to governance\nalso adds extra responsibility and labor needed to manage that complexity. In order to\nensure that the work gets done, and credit is given to those doing the work, we‚Äôd need to grow\na culture of creating specific roles and responsibility for those roles.","type":"content","url":"/blog/2019/2019-10-13-rust-jupyter-governance#whats-the-difference-between-jupyter-and-rust","position":15},{"hierarchy":{"lvl1":"What would Rust-style governance look like in Jupyter?","lvl2":"Wrapping up"},"type":"lvl2","url":"/blog/2019/2019-10-13-rust-jupyter-governance#wrapping-up","position":16},{"hierarchy":{"lvl1":"What would Rust-style governance look like in Jupyter?","lvl2":"Wrapping up"},"content":"I‚Äôm probably missing some things, but this seems like a reasonable plan! As I mentioned\nabove, this post has mostly been a thought-experiment, but if anybody has thoughts on bringing\nthis into the governance refactoring process, I‚Äôd be happy to talk more.","type":"content","url":"/blog/2019/2019-10-13-rust-jupyter-governance#wrapping-up","position":17},{"hierarchy":{"lvl1":"Analyzing intracranial electrophysiology data with xarray"},"type":"lvl1","url":"/blog/2019/2019-10-22-xarray-neuro","position":0},{"hierarchy":{"lvl1":"Analyzing intracranial electrophysiology data with xarray"},"content":"Over the last few years, it has been exciting to see the xarray project evolve,\nadd new functionality, and mature. This post is an attempt at\ngiving xarray another visit to see how it could integrate into electrophysiology\nworkflows.","type":"content","url":"/blog/2019/2019-10-22-xarray-neuro","position":1},{"hierarchy":{"lvl1":"Analyzing intracranial electrophysiology data with xarray","lvl3":"A quick background on our data"},"type":"lvl3","url":"/blog/2019/2019-10-22-xarray-neuro#a-quick-background-on-our-data","position":2},{"hierarchy":{"lvl1":"Analyzing intracranial electrophysiology data with xarray","lvl3":"A quick background on our data"},"content":"It is common in neuroscience to ask individuals to perform a task over and over again. You record\nthe activity in the brain each time they perform the task (called an ‚Äúepoch‚Äù or a ‚Äútrial‚Äù).\nTime is recorded relative to some onset when the task begins. That is t==0. The result\nis usually a matrix of epochs x channejupyls x time. You can do a lot of stuff with this\ndata, but our task in this paper is to detect changes in neural activity at trial onset (t==0).\n\nIn our case, we‚Äôve got a small dataset from \n\nan old paper of mine.\nThe repository contains\nseveral tutorial notebooks and sample data to describe predictive modeling\nin cognitive neuroscience. \n\nYou can find the repository here. The task that individuals were performing was passively\nlistening to spoken sentences through a speaker. While they did this, we recorded electrical\nactivity at the surface of their brain (these were surgical patients, and had implanted electrodes\nunder their scalp).\n\nIn the \n\nFeature Extraction notebook,\nI covered how to do some simple data manipulation and feature extraction with\ntimeseries analysis. Let‚Äôs try to re-create some of the main steps in that tutorial,\nbut now using xarray as an in-memory structure for our data.\n\nNote: The goal here is to learn a bit about xarray moreso than to discuss\necog modeling, so I‚Äôll spend more time talking about my thoughts on the various\nfunctions/methods/etc in Xarray than talking about neuroscience.\n\nIn this post, we‚Äôll perform a few common processing and extraction steps.\nThe goal is to do a few munging operations that require manipulating data\nand visualizing simple statistics.\n\n# Imports we'll use later\nimport mne\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom download import download\nimport os\nfrom sklearn.preprocessing import scale\nimport xarray as xr\nxr.set_options(display_style=\"html\")\n\nimport warnings\nwarnings.simplefilter('ignore')\n%matplotlib inline\n\nWe‚Äôll load the data from my GitHub repository (probably not the most efficient\nway to store or retrieve the data, but hey, this was 3 years ago :-) ).\n\nurl_epochs = \"https://github.com/choldgraf/paper-encoding_decoding_electrophysiology/blob/master/raw_data/ecog-epo.fif?raw=true\"\n\npath_data = download(url_epochs, './ecog-epo.fif', replace=True)\necog = mne.read_epochs(path_data, preload=True)\nos.remove(path_data)\n\nHere‚Äôs what the raw data looks like - each horizontal line is electrical activity\nin a channel over time. The faint vertical green lines show the onset of\neach trial (they are concatenated together, but in reality there‚Äôs a bit of time\nbetween trials). This will be one of the last times we use MNE hopefully.\n\n_ = ecog.plot(scalings='auto', n_epochs=5, n_channels=10)\n\n","type":"content","url":"/blog/2019/2019-10-22-xarray-neuro#a-quick-background-on-our-data","position":3},{"hierarchy":{"lvl1":"Analyzing intracranial electrophysiology data with xarray","lvl2":"Converting to xarray"},"type":"lvl2","url":"/blog/2019/2019-10-22-xarray-neuro#converting-to-xarray","position":4},{"hierarchy":{"lvl1":"Analyzing intracranial electrophysiology data with xarray","lvl2":"Converting to xarray"},"content":"First off, we‚Äôll define a helper function that\nconverts the MNE Epochs object into an xarray DataArray object.\nDataArrays provide an N-Dimensional representation of data, but with\nthe option to include a lot of extra metadata.\n\nDataArrays are useful because you can include information\nabout each dimension of the data. For example, we can tell our\nDataArray the name, values, and units of each dimension. In this case,\nin our case one dimension is ‚Äútime‚Äù so we can label it as such.\n\ndef epochs_to_dataarray(epochs):\n    \"\"\"A simple function to convert an Epochs object to DataArray\"\"\"\n    da = xr.DataArray(\n    epochs._data,\n    dims=['epoch', 'channel', 'time'],\n    coords={\n        'time': ecog.times,\n        'channel': ecog.ch_names,\n        'epoch': range(ecog._data.shape[0])\n    },\n    name='Sample dataset',\n    attrs=dict(ecog.info)\n    )\n    return da\n\nJust look at all the metadata that we were able to pack into the DataArray.\nAlmost all of MNE‚Äôs metadata fit nicely into .attrs.\n\n# There's quite a lot of output, so keep scrolling down!\nda = epochs_to_dataarray(ecog)\nda\n\nThe data consists of many trials, channels, and timepoints.\nLet‚Äôs start by selecting a time region within each trial that\nwe can visualize more cleanly.","type":"content","url":"/blog/2019/2019-10-22-xarray-neuro#converting-to-xarray","position":5},{"hierarchy":{"lvl1":"Analyzing intracranial electrophysiology data with xarray","lvl2":"Subsetting out data with da.sel"},"type":"lvl2","url":"/blog/2019/2019-10-22-xarray-neuro#subsetting-out-data-with-da-sel","position":6},{"hierarchy":{"lvl1":"Analyzing intracranial electrophysiology data with xarray","lvl2":"Subsetting out data with da.sel"},"content":"In xarray, we select items with the sel and isel method. This\nbehaves kind of like the pandas loc and iloc methods, however\nbecause we have named dimensions, we can directly specify them in\nour call.\n\n# We'll drop a subset of timepoints for visualization\nda = da.sel(time=slice(-1, 3))\n\nNow let‚Äôs calculate the average across all epochs for each electrode/time point.\nThis is a reduction of our data array, in that it reduces the number of dimensions.\nXarray has many of the same statistical methods that NumPy does. An interesting\ntwist is that you can specify named dimensions instead of simply an axis=<integer>\nargument. In addition, we‚Äôll choose the colors that we‚Äôll use for cycling through\nour channels - because we can quickly reference the channels axis by name, we don‚Äôt\nneed to remember which axis corresponds to channels.\n\nfig, ax = plt.subplots(figsize=(15, 5))\nn_channels = da['channel'].shape[0]\nax.set_prop_cycle(color=plt.cm.viridis(np.linspace(0, 1, n_channels)))\nda.mean(dim='epoch').plot.line(x='time', hue='channel')\nax.get_legend().remove()\n\nIt doesn‚Äôt look like much is going on...let‚Äôs see if we can clean it up a bit.\n\n","type":"content","url":"/blog/2019/2019-10-22-xarray-neuro#subsetting-out-data-with-da-sel","position":7},{"hierarchy":{"lvl1":"Analyzing intracranial electrophysiology data with xarray","lvl2":"De-meaning the data with da.where"},"type":"lvl2","url":"/blog/2019/2019-10-22-xarray-neuro#de-meaning-the-data-with-da-where","position":8},{"hierarchy":{"lvl1":"Analyzing intracranial electrophysiology data with xarray","lvl2":"De-meaning the data with da.where"},"content":"First off - we‚Äôll subtract the ‚Äúpre-baseline mean‚Äù from each trial.\nThis makes it easier to visualize how each channel‚Äôs activity changed\nat time == 0.\n\nTo accomplish this we‚Äôll use da.where. This takes some kind of\nboolean-style mask, does a bunch of clever projections according to the\nnames of coordinates, and returns the dataarray masked values removed\n(as NaNs) and other values unchanged. We can use this to calculate the\nmean of each channel / epoch only for the pre-baseline timepoints.\n\n# This returns a version of the data array with NaNs where the query is False\n# The dimensions will intelligently broadcast \nprebaseline_mean = da.where(da.time < 0).mean(dim='time')\nda_demeaned = da - prebaseline_mean\n\nNow we can visualize the de-baseline-meaned data\n\nfig, ax = plt.subplots(figsize=(15, 5))\nax.set_prop_cycle(color=plt.cm.viridis(np.linspace(0, 1, da['channel'].shape[0])))\nda_demeaned.mean(dim='epoch').plot.line(x='time', hue='channel')\nax.get_legend().remove()\n\nHmmm, there still doesn‚Äôt seem to be much going on (that channel down\nat the bottom looks noisy to me, rather than having a meaningful signal)\nso let‚Äôs transform this signal into something with a bit more SNR to it.\n\n","type":"content","url":"/blog/2019/2019-10-22-xarray-neuro#de-meaning-the-data-with-da-where","position":9},{"hierarchy":{"lvl1":"Analyzing intracranial electrophysiology data with xarray","lvl2":"Extracting a more useful feature with xr.apply_ufunc"},"type":"lvl2","url":"/blog/2019/2019-10-22-xarray-neuro#extracting-a-more-useful-feature-with-xr-apply-ufunc","position":10},{"hierarchy":{"lvl1":"Analyzing intracranial electrophysiology data with xarray","lvl2":"Extracting a more useful feature with xr.apply_ufunc"},"content":"Without going into too much details on the neuroscience, iEEG data is\nparticularly useful because there is information about neuronal activity in\nthe higher frequency parts of the signal (AKA, parts of the electrical signal that\nchange very quickly, but have very low amplitude). To pull that out, we‚Äôll do the following:\n\nHigh-pass filter the signal, which will remove all the slow-moving components\n\nCalculate the envelope of the signal, which will tell us the power of\nhigh-frequency activity over time.","type":"content","url":"/blog/2019/2019-10-22-xarray-neuro#extracting-a-more-useful-feature-with-xr-apply-ufunc","position":11},{"hierarchy":{"lvl1":"Analyzing intracranial electrophysiology data with xarray","lvl3":"High-pass filtering the signal","lvl2":"Extracting a more useful feature with xr.apply_ufunc"},"type":"lvl3","url":"/blog/2019/2019-10-22-xarray-neuro#high-pass-filtering-the-signal","position":12},{"hierarchy":{"lvl1":"Analyzing intracranial electrophysiology data with xarray","lvl3":"High-pass filtering the signal","lvl2":"Extracting a more useful feature with xr.apply_ufunc"},"content":"MNE has a lot of nice functions for filtering a timeseries. Most of these\noperate on numpy arrays instead of MNE objects. We‚Äôll use\nxarray‚Äôs apply_ufunc function to simply map that function onto our dataarray.\nxarray should keep track of the metadata (e.g. coordinates etc) and output a\nnew DataArray with updated values.\n\nflow = 80\nfhigh = 140\nda_lowpass = xr.apply_ufunc(\n    mne.filter.filter_data, da,\n   kwargs=dict(\n       sfreq=da.sfreq,\n       l_freq=flow,\n       h_freq=fhigh,\n   )\n)\n\nVisualizing our data, we can see all the slower fluctuations (e.g. long arcs over time)\nare gone.\n\nfig, ax = plt.subplots(figsize=(15, 5))\nda_lowpass.mean(dim='epoch').plot.line(x='time')\nax.get_legend().remove()\n\n","type":"content","url":"/blog/2019/2019-10-22-xarray-neuro#high-pass-filtering-the-signal","position":13},{"hierarchy":{"lvl1":"Analyzing intracranial electrophysiology data with xarray","lvl3":"Calculate the envelope of this signal with da.groupby","lvl2":"Extracting a more useful feature with xr.apply_ufunc"},"type":"lvl3","url":"/blog/2019/2019-10-22-xarray-neuro#calculate-the-envelope-of-this-signal-with-da-groupby","position":14},{"hierarchy":{"lvl1":"Analyzing intracranial electrophysiology data with xarray","lvl3":"Calculate the envelope of this signal with da.groupby","lvl2":"Extracting a more useful feature with xr.apply_ufunc"},"content":"Next, we‚Äôll calculate the envelope of the high-pass-filtered data. This is roughly\nthe power that is present in these high frequencies over time. We do so by using\nsomething called a hilbert transform.\n\nMNE also has a function for applying Hilbert transforms to data, but it has a weird quirk\nthat expects the data to be of a particular shape. We can work around this by using our\nDataArray‚Äôs groupby method. This works similar to DataFrame.groupby - we‚Äôll iterate\nthrough each channel, which will return a DataArray with shape epochs x timepoints.\nWe can then calculate the Hilbert transform in each and re-combine into the original shape.\n\nNote: This can be an expensive operation depending on the number of channels/epochs and\nthe length of each trial. This might be a good place to insert paralellization via Dask.\n\ndef hilbert_2d(array):\n    \"\"\"Perform a Hilbert transforms on an (n_channels, n_times) array.\"\"\"\n    for ii, channel in enumerate(array):\n        array[ii] = mne.filter._my_hilbert(channel, envelope=True)\n    return array\n\nda_hf_power = da_lowpass.groupby(da.coords['epoch']).apply(hilbert_2d)\n\nThe output dataarray should be the exact same shape, because we haven‚Äôt done any dimensional reductions.\nIf we take a look at the resulting data, we can see what seems to be more structure in there:\n\nfig, ax = plt.subplots(figsize=(15, 5))\nda_hf_power.mean(dim='epoch').plot.line(x='time', hue='channel')\nax.get_legend().remove()\n\n","type":"content","url":"/blog/2019/2019-10-22-xarray-neuro#calculate-the-envelope-of-this-signal-with-da-groupby","position":15},{"hierarchy":{"lvl1":"Analyzing intracranial electrophysiology data with xarray","lvl2":"Cleaning up our HFA data"},"type":"lvl2","url":"/blog/2019/2019-10-22-xarray-neuro#cleaning-up-our-hfa-data","position":16},{"hierarchy":{"lvl1":"Analyzing intracranial electrophysiology data with xarray","lvl2":"Cleaning up our HFA data"},"content":"Next let‚Äôs clean up this high-frequency activity (HFA) data.\n\n","type":"content","url":"/blog/2019/2019-10-22-xarray-neuro#cleaning-up-our-hfa-data","position":17},{"hierarchy":{"lvl1":"Analyzing intracranial electrophysiology data with xarray","lvl3":"Z-scoring our array","lvl2":"Cleaning up our HFA data"},"type":"lvl3","url":"/blog/2019/2019-10-22-xarray-neuro#z-scoring-our-array","position":18},{"hierarchy":{"lvl1":"Analyzing intracranial electrophysiology data with xarray","lvl3":"Z-scoring our array","lvl2":"Cleaning up our HFA data"},"content":"Instead of simple de-meaning\nthe data like before, we‚Äôll re-scale our data using the same baseline timepoints.\nWhat we‚Äôd like to do is the following:\n\nCalculate the mean and standard deviation across trials of all pre-baseline data values, per channel\n\nZ-score each channel using this mean and standard deviation\n\nOnce again we‚Äôll use the groupby / apply combination to apply our function to subsets\nof the data.\n\n# For each channel, apply a z-score that uses the mean/std of pre-baseline activity for all trials\ndef z_score(activity):\n    \"\"\"Take a DataArray and apply a z-score using the baseline\"\"\"\n    baseline = activity.where(activity.time < -.1 )\n    return (activity - np.nanmean(baseline)) / np.nanstd(baseline)\n\nda_hf_zscored = da_hf_power.groupby('channel').apply(z_score)\n\nTaking a look at the result, we can see a much cleaner separation of activity for\nsome of the channels after time==0.\n\nfig, ax = plt.subplots(figsize=(15, 5))\nda_hf_zscored.mean(dim='epoch').plot.line(x='time', hue='channel')\nax.get_legend().remove()\n\n","type":"content","url":"/blog/2019/2019-10-22-xarray-neuro#z-scoring-our-array","position":19},{"hierarchy":{"lvl1":"Analyzing intracranial electrophysiology data with xarray","lvl3":"Smoothing our HFA data","lvl2":"Cleaning up our HFA data"},"type":"lvl3","url":"/blog/2019/2019-10-22-xarray-neuro#smoothing-our-hfa-data","position":20},{"hierarchy":{"lvl1":"Analyzing intracranial electrophysiology data with xarray","lvl3":"Smoothing our HFA data","lvl2":"Cleaning up our HFA data"},"content":"Finally, let‚Äôs smooth this HFA so it has less jitter to it, and pick a smaller window that\nremoves some of the filtering artifacts at the edges.\n\nWe‚Äôll use the same filter_data function as before, but this time\napplied with the .groupby and .apply combination to show two ways\nof accomplishing the same thing. We‚Äôll also use .sel to pick a subset\nof time for visualization\n\nda_hf_zscored_lowpass = da_hf_zscored.groupby('epoch').apply(\n    mne.filter.filter_data,\n    sfreq=da.sfreq,\n    l_freq=None,\n    h_freq=10,\n    verbose=False\n)\n\nNote that quickly selecting a subset of timepoints if we used numpy is much more verbose. Here‚Äôs\na quick comparison:# Numpy alone\nmask_time = (times > -.8) * (times < 2.8)\nepoch_dim = 0\nda_hf_zscored_lowpass[..., mask_time].mean(epoch_dim)\n\n# xarray\nda_hf_zscored_lowpass.sel(time=slice(-.8, 2.8)).mean(dim='epoch')\n\nfig, ax = plt.subplots(figsize=(15, 5))\nda_hf_zscored_lowpass.mean(dim='epoch').sel(time=slice(-.8, 2.8)).plot.line(x='time', hue='channel')\nax.get_legend().remove()\n\nNow we can see there are clearly some channels that become active just after t==0.\nWe can reduce our dataarray to a single dimension of ‚Äúmean post-baseline activity in each channel‚Äù\nand convert it to a DataFrame for further processing:\n\n# Find the channel with the most activity by first converting to a dataframe\ntotal_activity = da_hf_zscored_lowpass.sel(time=slice(0, 2)).mean(dim=['epoch', 'time'])\ntotal_activity = total_activity.to_dataframe()\ntotal_activity.head()\n\nLet‚Äôs grab the channel with maximal activation to look into a bit further.\n\nmax_chan = total_activity.squeeze().sort_values(ascending=False).index[0]\n\n","type":"content","url":"/blog/2019/2019-10-22-xarray-neuro#smoothing-our-hfa-data","position":21},{"hierarchy":{"lvl1":"Analyzing intracranial electrophysiology data with xarray","lvl2":"Time frequency analysis"},"type":"lvl2","url":"/blog/2019/2019-10-22-xarray-neuro#time-frequency-analysis","position":22},{"hierarchy":{"lvl1":"Analyzing intracranial electrophysiology data with xarray","lvl2":"Time frequency analysis"},"content":"As a final step, let‚Äôs expand our DataArray and add another dimension.\nIn the above steps we specifically focused on high-frequency activity. A more\ncommon approach is to first create a spectrogram of your data to see activity\nacross many frequencies.\n\nTo do this, we‚Äôll use another MNE function for creating a Time-Frequency Representation\nor TFR. We‚Äôll define a range of frequencies, and apply MNE‚Äôs function directly on our DataArray.\nThis will return a NumPy array with the filtered values.\n\nfrequencies = [2**ii for ii in np.arange(2, 9, .5)]\ntfr = mne.time_frequency.tfr_array_morlet(\n    da,\n    sfreq=da.sfreq,\n    freqs=frequencies,\n    n_cycles=4,\n)\n\n# Take the absolute value to throw out the non-real parts of the numbers\ntfr = np.abs(tfr)\ntfr[:2, :2, :2, :2]\n\n","type":"content","url":"/blog/2019/2019-10-22-xarray-neuro#time-frequency-analysis","position":23},{"hierarchy":{"lvl1":"Analyzing intracranial electrophysiology data with xarray","lvl3":"Convert this data into a DataArray with .expand_dims","lvl2":"Time frequency analysis"},"type":"lvl3","url":"/blog/2019/2019-10-22-xarray-neuro#convert-this-data-into-a-dataarray-with-expand-dims","position":24},{"hierarchy":{"lvl1":"Analyzing intracranial electrophysiology data with xarray","lvl3":"Convert this data into a DataArray with .expand_dims","lvl2":"Time frequency analysis"},"content":"\n\nNext, we‚Äôll convert this into a DataArray by using the metadata from our original\nDataArray. We can use the expand_dims method to create a new dimension for our DataArray.\nWe‚Äôll use this to store frequency information.\n\nWe‚Äôll then reshape our new DataArray so that it matches the output of the MNE function,\nand use the copy method to create a new DataArray. By supplying the data= argument\nto copy, we directly insert the new data inside the generated DataArray.\n\nda_tfr = (da\n    .expand_dims(frequency=frequencies)\n    .transpose('epoch', 'channel', 'frequency', 'time')\n    .copy(data=np.log(tfr))\n)\n\nWe can now visualize this time-frequency representation over time\n\nfig, ax = plt.subplots(figsize=(15, 5))\n(da_tfr\n    .sel({'frequency': slice(None, 180), 'channel': max_chan})\n    .mean('epoch')\n    .plot.imshow(x='time', y='frequency')\n)\n\nSimilar to our one-dimensional visualizations above, it can be hard to visualize\nrelative changes in activity over a baseline (particularly because the amplitude scales\ninversely with the frequency).\n\nLet‚Äôs apply a re-scaling function to our data so that\nwe can see things more clearly. This time we‚Äôll use MNE‚Äôs rescale function, which\nacts similarly to our zscore function above.\n\nda_tfr_baselined = xr.apply_ufunc(\n    mne.baseline.rescale,\n    da_tfr,\n    kwargs={'times': da_tfr.coords['time'], 'baseline': (None, -.1), \"mode\": 'zscore'}\n)\n\nagain, the result should be a DataArray, so we can directly visualize it:\n\n(da_tfr_baselined\n    .sel({'frequency': slice(None, 180), 'channel': max_chan, 'time': slice(-.8, 2.5)})\n    .mean('epoch')\n    .plot.imshow(x='time', y='frequency')\n)\n\nNow we can see a clear increase in activity in the higher frequencies at t==0.\n\n","type":"content","url":"/blog/2019/2019-10-22-xarray-neuro#convert-this-data-into-a-dataarray-with-expand-dims","position":25},{"hierarchy":{"lvl1":"Analyzing intracranial electrophysiology data with xarray","lvl2":"Combining the two with xr.merge"},"type":"lvl2","url":"/blog/2019/2019-10-22-xarray-neuro#combining-the-two-with-xr-merge","position":26},{"hierarchy":{"lvl1":"Analyzing intracranial electrophysiology data with xarray","lvl2":"Combining the two with xr.merge"},"content":"Finally, let‚Äôs combine these two DataArrays into one. We know that they\nshare much of the same metadata - the first is ‚ÄúAmplitude of High-Frequency Activity‚Äù\nand the second is ‚ÄúTime-frequency power‚Äù. We should be able to merge these\ninto a single xarray DataSet, which will allow us to perform operations across\nboth by using their shared dimensions. DataSets are kind of like collections of\nDataArrays, with assumptions that the DataArrays share some metadata or coordinates.\n\nFirst, we‚Äôll rename each DataArray so that we can merge them nicely. Then, we‚Äôll simply\nuse the xr.merge function, which tries to automatically figure out which dimensions are\nshared based on their names and coordinate values.\n\nda_tfr_baselined.name = \"Time Frequency Representation\"\nda_hf_zscored_lowpass.name = \"Low-pass filtered HFA\"\nds = xr.merge([da_tfr_baselined, da_hf_zscored_lowpass])\nds\n\nSince we‚Äôve got a single dataset, we can grab subsets along each axis across both\nDataArrays at the same time. We‚Äôll select a subset of channels, time, and frequency bands\nto visualize.\n\nds_plt = ds.sel({'channel': max_chan, 'frequency': slice(10, 150), 'time': slice(-.5, 2)})\n\nNow, we‚Äôll plot both the spectrogram and the HFA in the same Matplotlib figure. As you\ncan see, these plots contain somewhat redundant information. The top plot tells us that there is\na general increase in power for high-frequencies. The bottom plot gives us the average increase in\npower across the higher frequencies.\n\nfig, (ax_tfr, ax_hfa) = plt.subplots(2, 1, figsize=(15, 10))\nim = ds_plt['Time Frequency Representation'].mean('epoch').plot.imshow(x='time', y='frequency',\n                                                                       ax=ax_tfr)\n\nds_plt['Low-pass filtered HFA'].mean('epoch').plot.line(x='time', ax=ax_hfa)\n\n","type":"content","url":"/blog/2019/2019-10-22-xarray-neuro#combining-the-two-with-xr-merge","position":27},{"hierarchy":{"lvl1":"Analyzing intracranial electrophysiology data with xarray","lvl2":"Wrapping up"},"type":"lvl2","url":"/blog/2019/2019-10-22-xarray-neuro#wrapping-up","position":28},{"hierarchy":{"lvl1":"Analyzing intracranial electrophysiology data with xarray","lvl2":"Wrapping up"},"content":"In all, I was pretty happy with what you can do using xarray‚Äôs DataArray structure.\nIt‚Äôs pretty nice to be able to refer to axes by their names, and to make more intelligent\nselection / slicing operations using their coordinate values. Moreover, this post is just\nscratching the surface for how to use this information in a way that speeds up the exploration\nand analysis post.\n\nFor example, we might have sped-up some feature extraction steps by using\na distributed processing framework like Dask in the operations above. Dask integrates nicely\nwith xarray, and offers a lot of interesting opportunities to parallelize interactive computation.\nI‚Äôll explore that in another blog post.\n\nFinally - the goal of this post has largely been to learn a bit more about xarray. This means I might\nbe totally mis-using functionality, or missing something that would have made the above process much\neasier. If anybody has tips or thoughts on the code above, please do reach out!","type":"content","url":"/blog/2019/2019-10-22-xarray-neuro#wrapping-up","position":29},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?"},"type":"lvl1","url":"/blog/2019/2019-10-27-jupyter-governance-python","position":0},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?"},"content":"This is the second in a series of blog posts that explores what it‚Äôd look like to\ndirectly port the governance model of other communities into the Jupyter project.\nYou can find the \n\nfirst post about Rust here.\n\nNote: These posts are meant as a thought experiment rather than a proposal. Moreover,\nall the usual caveats come with it, such as the\nfact that I don‚Äôt know the Python governance\nstructure that well, and I might totally\nbotch my characterization of it.","type":"content","url":"/blog/2019/2019-10-27-jupyter-governance-python","position":1},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?","lvl2":"Background on Python‚Äôs Governance"},"type":"lvl2","url":"/blog/2019/2019-10-27-jupyter-governance-python#background-on-pythons-governance","position":2},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?","lvl2":"Background on Python‚Äôs Governance"},"content":"Recently, the Python community underwent a refactoring of their governance\nmodel. This was in large part due to \n\nGuido‚Äôs decision to step down as BDFL.\n\nWhat transpired was a months-long process in which many proposals were made for\na new governance model in the Python community, following the \n\nPEP 8000 series\nof enhancement proposals.\n\nThere were a lot of interesting ideas in the proposals that came forward, but this\npost is going to focus on the one that ended up being chosen, \n\nPEP 8016. You can find the final language of Python‚Äôs governance model \n\nin this PEP.\n\nSo, what would Python‚Äôs governance model look like in Jupyter?","type":"content","url":"/blog/2019/2019-10-27-jupyter-governance-python#background-on-pythons-governance","position":3},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?","lvl2":"A quick overview"},"type":"lvl2","url":"/blog/2019/2019-10-27-jupyter-governance-python#a-quick-overview","position":4},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?","lvl2":"A quick overview"},"content":"At a high level, Python‚Äôs governance model is quite similar to that of Project Jupyter.\nIt has two main bodies:\n\nA steering council\n\nA ‚Äúcore contributors‚Äù group\n\nThe steering council has the ‚Äúfinal say‚Äù in everything within the Python language, though\nthey also have a mandate to delegate as much as possible to teams / groups within the\nPython community. As such, most decisions will likely not be made by the steering council.\nMembership on the council is determined on a rotating basis, and a new council is elected before\neach new major release of Python. This ensures continuity in the council, but also ensures\nthat there are opportunities for membership to evolve over time. Discussion and design\nfor decisions are made via the Python Enhancement Proposal (PEP) process, though the\n‚Äúdecision‚Äù on these PEPs can take different forms (and the council has the ultimate vote\nif need be).\n\nIn many ways, this closely resembles what the Jupyter project already has.\nJupyter uses a steering council to make decisions, though the council is much larger\n(at 21 members). It also has a large, informally defined group of ‚Äúcore contributors‚Äù,\nwhich maps cleanly onto the Python ‚Äúcore team‚Äù model. Python already has a\nwell-established collective decision-making process (PEPs), while Jupyter\nhas a defined-but-not-established process (JEPs).","type":"content","url":"/blog/2019/2019-10-27-jupyter-governance-python#a-quick-overview","position":5},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?","lvl2":"A Jupyter PEP 013"},"type":"lvl2","url":"/blog/2019/2019-10-27-jupyter-governance-python#a-jupyter-pep-013","position":6},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?","lvl2":"A Jupyter PEP 013"},"content":"The rest of this post is going to be a close mirror to the accepted \n\nPEP 013 proposal. I‚Äôll change the language where it makes sense, or in order to\nmap onto whatever structures currently exist in the Jupyter world.","type":"content","url":"/blog/2019/2019-10-27-jupyter-governance-python#a-jupyter-pep-013","position":7},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?","lvl3":"The current Jupyter steering council","lvl2":"A Jupyter PEP 013"},"type":"lvl3","url":"/blog/2019/2019-10-27-jupyter-governance-python#the-current-jupyter-steering-council","position":8},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?","lvl3":"The current Jupyter steering council","lvl2":"A Jupyter PEP 013"},"content":"The current steering council is the only official group of people attached to\nthe Jupyter Project. It consists of 21 members listed here:\n\nhttps://‚Äãjupyter‚Äã.org‚Äã/about\n\nThis team has grown iteratively since the beginning of the Jupyter Project.\n\nFor the sake of clarity, references to ‚Äúthe new steering council‚Äù refers to\nthe organizing body that replaces the current steering council.\n\nIt also assumes that we use a Jupyter Enhancement Proposal (JEP) process\nfor proposing and making decisions. This post will not detail the JEP process,\nbecause \n\nSafia already did a great job of this here.\nIt also assumes that this document is written in the form of a JEP, similar to\nwhat the Python community used.","type":"content","url":"/blog/2019/2019-10-27-jupyter-governance-python#the-current-jupyter-steering-council","position":9},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?","lvl3":"The new Jupyter steering council","lvl2":"A Jupyter PEP 013"},"type":"lvl3","url":"/blog/2019/2019-10-27-jupyter-governance-python#the-new-jupyter-steering-council","position":10},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?","lvl3":"The new Jupyter steering council","lvl2":"A Jupyter PEP 013"},"content":"","type":"content","url":"/blog/2019/2019-10-27-jupyter-governance-python#the-new-jupyter-steering-council","position":11},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?","lvl4":"Composition","lvl3":"The new Jupyter steering council","lvl2":"A Jupyter PEP 013"},"type":"lvl4","url":"/blog/2019/2019-10-27-jupyter-governance-python#composition","position":12},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?","lvl4":"Composition","lvl3":"The new Jupyter steering council","lvl2":"A Jupyter PEP 013"},"content":"The steering council is a 5-person committee.","type":"content","url":"/blog/2019/2019-10-27-jupyter-governance-python#composition","position":13},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?","lvl4":"Mandate","lvl3":"The new Jupyter steering council","lvl2":"A Jupyter PEP 013"},"type":"lvl4","url":"/blog/2019/2019-10-27-jupyter-governance-python#mandate","position":14},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?","lvl4":"Mandate","lvl3":"The new Jupyter steering council","lvl2":"A Jupyter PEP 013"},"content":"The steering council shall work to:\n\nMaintain the quality and stability of the Jupyter core projects,\n\nMake contributing as accessible, inclusive, and sustainable as possible,\n\nFormalize and maintain the relationship between the core team and NumFocus,\n\nEstablish appropriate decision-making processes for teams in the Jupyter community,\n\nSeek consensus among contributors and the core team before acting in a formal capacity,\n\nAct as a ‚Äúcourt of final appeal‚Äù for decisions where all other methods have failed.","type":"content","url":"/blog/2019/2019-10-27-jupyter-governance-python#mandate","position":15},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?","lvl4":"Powers","lvl3":"The new Jupyter steering council","lvl2":"A Jupyter PEP 013"},"type":"lvl4","url":"/blog/2019/2019-10-27-jupyter-governance-python#powers","position":16},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?","lvl4":"Powers","lvl3":"The new Jupyter steering council","lvl2":"A Jupyter PEP 013"},"content":"The council has broad authority to make decisions about the project. For example, they can:\n\nAccept or reject JEPs and PRs\n\nEnforce or update the project‚Äôs code of conduct\n\nWork with NumFocus to manage any project assets\n\nDelegate parts of their authority to other subcommittees or processes\n\nHowever, they cannot modify this JEP, or affect the membership of the core team, except via the mechanisms specified in this document.\n\nThe council should look for ways to use these powers as little as possible. Instead of voting, it‚Äôs better to seek consensus. Instead of ruling on individual JEPs, it‚Äôs better to define a standard process for JEP decision making (for example, by formalizing \n\nthe recently-proposed meta-JEP process).\nIt‚Äôs better to establish a Code of Conduct committee than to rule on individual cases. And so on.\n\nTo use its powers, the council votes. Every council member must either vote or explicitly abstain. Members with conflicts of interest on a particular vote must abstain. Passing requires a strict majority of non-abstaining council members.\n\nWhenever possible, the council‚Äôs deliberations and votes shall be held in public.","type":"content","url":"/blog/2019/2019-10-27-jupyter-governance-python#powers","position":17},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?","lvl4":"Electing the council","lvl3":"The new Jupyter steering council","lvl2":"A Jupyter PEP 013"},"type":"lvl4","url":"/blog/2019/2019-10-27-jupyter-governance-python#electing-the-council","position":18},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?","lvl4":"Electing the council","lvl3":"The new Jupyter steering council","lvl2":"A Jupyter PEP 013"},"content":"A council election consists of two phases:\n\nPhase 1: Candidates advertise their interest in serving. Candidates must be nominated by a core team member. Self-nominations are allowed.\n\nPhase 2: Each core team member can vote for zero or more of the candidates. Voting is performed anonymously. Candidates are ranked by the total number of votes they receive. If a tie occurs, it may be resolved by mutual agreement among the candidates, or else the winner will be chosen at random.\n\nEach phase lasts one to two weeks, at the outgoing council‚Äôs discretion. For the initial election, both phases will last two weeks.\n\nThe election process is managed by \n\na returns officer nominated by the outgoing steering council. For the initial election, the returns officer will be nominated by Brian Granger and Fernando Perez.\n\nThe council should ideally reflect the diversity of Jupyter contributors and users, and core team members are encouraged to vote accordingly.","type":"content","url":"/blog/2019/2019-10-27-jupyter-governance-python#electing-the-council","position":19},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?","lvl4":"Term","lvl3":"The new Jupyter steering council","lvl2":"A Jupyter PEP 013"},"type":"lvl4","url":"/blog/2019/2019-10-27-jupyter-governance-python#term","position":20},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?","lvl4":"Term","lvl3":"The new Jupyter steering council","lvl2":"A Jupyter PEP 013"},"content":"Jupyter will define a cycle of operations, roughly mapping onto a ‚Äúmajor version release cycle‚Äù\n(as if it were a single technical project). The details of what this means will be decided by the\nsteering council and the JEP process.\n\nA new council is elected after cycle. Each council‚Äôs term runs from when their election results are finalized until the next council‚Äôs term starts. There are no term limits.\n\nSome extra details about the steering council. Click to expand if you're interested.","type":"content","url":"/blog/2019/2019-10-27-jupyter-governance-python#term","position":21},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?","lvl4":"Vacancies","lvl3":"The new Jupyter steering council","lvl2":"A Jupyter PEP 013"},"type":"lvl4","url":"/blog/2019/2019-10-27-jupyter-governance-python#vacancies","position":22},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?","lvl4":"Vacancies","lvl3":"The new Jupyter steering council","lvl2":"A Jupyter PEP 013"},"content":"Council members may resign their position at any time.\n\nWhenever there is a vacancy during the regular council term, the council may vote to appoint a replacement to serve out the rest of the term.\n\nIf a council member drops out of touch and cannot be contacted for a month or longer, then the rest of the council may vote to replace them.","type":"content","url":"/blog/2019/2019-10-27-jupyter-governance-python#vacancies","position":23},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?","lvl4":"Conflicts of interest","lvl3":"The new Jupyter steering council","lvl2":"A Jupyter PEP 013"},"type":"lvl4","url":"/blog/2019/2019-10-27-jupyter-governance-python#conflicts-of-interest","position":24},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?","lvl4":"Conflicts of interest","lvl3":"The new Jupyter steering council","lvl2":"A Jupyter PEP 013"},"content":"While we trust council members to act in the best interests of Jupyter rather than themselves or their employers, the mere appearance of any one company dominating Jupyter development could itself be harmful and erode trust. In order to avoid any appearance of conflict of interest, at most 2 members of the council can work for any single employer.\n\nIn a council election, if 3 of the top 5 vote-getters work for the same employer, then whichever of them ranked lowest is disqualified and the 6th-ranking candidate moves up into 5th place; this is repeated until a valid council is formed.\n\nDuring a council term, if changing circumstances cause this rule to be broken (for instance, due to a council member changing employment), then one or more council members must resign to remedy the issue, and the resulting vacancies can then be filled as normal.","type":"content","url":"/blog/2019/2019-10-27-jupyter-governance-python#conflicts-of-interest","position":25},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?","lvl4":"Ejecting core team members","lvl3":"The new Jupyter steering council","lvl2":"A Jupyter PEP 013"},"type":"lvl4","url":"/blog/2019/2019-10-27-jupyter-governance-python#ejecting-core-team-members","position":26},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?","lvl4":"Ejecting core team members","lvl3":"The new Jupyter steering council","lvl2":"A Jupyter PEP 013"},"content":"In exceptional circumstances, it may be necessary to remove someone from the core team against their will. (For example: egregious and ongoing code of conduct violations.) This can be accomplished by a steering council vote, but unlike other steering council votes, this requires at least a two-thirds majority. With 5 members voting, this means that a 3:2 vote is insufficient; 4:1 in favor is the minimum required for such a vote to succeed. In addition, this is the one power of the steering council which cannot be delegated, and this power cannot be used while a vote of no confidence is in process.\n\nIf the ejected core team member is also on the steering council, then they are removed from the steering council as well.","type":"content","url":"/blog/2019/2019-10-27-jupyter-governance-python#ejecting-core-team-members","position":27},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?","lvl4":"Vote of no confidence","lvl3":"The new Jupyter steering council","lvl2":"A Jupyter PEP 013"},"type":"lvl4","url":"/blog/2019/2019-10-27-jupyter-governance-python#vote-of-no-confidence","position":28},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?","lvl4":"Vote of no confidence","lvl3":"The new Jupyter steering council","lvl2":"A Jupyter PEP 013"},"content":"In exceptional circumstances, the core team may remove a sitting council member, or the entire council, via a vote of no confidence.\n\nA no-confidence vote is triggered when a core team member calls for one publically on an appropriate project communication channel, and another core team member seconds the proposal.\n\nThe vote lasts for two weeks. Core team members vote for or against. If at least two thirds of voters express a lack of confidence, then the vote succeeds.\n\nThere are two forms of no-confidence votes: those targeting a single member, and those targeting the council as a whole. The initial call for a no-confidence vote must specify which type is intended. If a single-member vote succeeds, then that member is removed from the council and the resulting vacancy can be handled in the usual way. If a whole-council vote succeeds, the council is dissolved and a new council election is triggered immediately.","type":"content","url":"/blog/2019/2019-10-27-jupyter-governance-python#vote-of-no-confidence","position":29},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?","lvl3":"The core team","lvl2":"A Jupyter PEP 013"},"type":"lvl3","url":"/blog/2019/2019-10-27-jupyter-governance-python#the-core-team","position":30},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?","lvl3":"The core team","lvl2":"A Jupyter PEP 013"},"content":"","type":"content","url":"/blog/2019/2019-10-27-jupyter-governance-python#the-core-team","position":31},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?","lvl4":"Role","lvl3":"The core team","lvl2":"A Jupyter PEP 013"},"type":"lvl4","url":"/blog/2019/2019-10-27-jupyter-governance-python#role","position":32},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?","lvl4":"Role","lvl3":"The core team","lvl2":"A Jupyter PEP 013"},"content":"The core team is the group of trusted volunteers who manage Jupyter. They assume many roles required to achieve the project‚Äôs goals, especially those that require a high level of trust. They make the decisions that shape the future of the project.\n\nCore team members are expected to act as role models for the community and custodians of the project, on behalf of the community and all those who rely on Jupyter.\n\nThey will intervene, where necessary, in online discussions or at official Jupyter events on the rare occasions that a situation arises that requires intervention.\n\nThey have authority over the Jupyter Project infrastructure, including the \n\njupyter.org website itself, the ‚Äúcore Jupyter GitHub organizations and repositories‚Äù, the community forum, the mailing lists, Gitter channels,\nand any cloud infrastructure such as nbconvert, \n\nmybinder.org, etc.","type":"content","url":"/blog/2019/2019-10-27-jupyter-governance-python#role","position":33},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?","lvl4":"Prerogatives","lvl3":"The core team","lvl2":"A Jupyter PEP 013"},"type":"lvl4","url":"/blog/2019/2019-10-27-jupyter-governance-python#prerogatives","position":34},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?","lvl4":"Prerogatives","lvl3":"The core team","lvl2":"A Jupyter PEP 013"},"content":"Core team members may participate in formal votes, typically to nominate new team members and to elect the steering council.","type":"content","url":"/blog/2019/2019-10-27-jupyter-governance-python#prerogatives","position":35},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?","lvl4":"Membership","lvl3":"The core team","lvl2":"A Jupyter PEP 013"},"type":"lvl4","url":"/blog/2019/2019-10-27-jupyter-governance-python#membership","position":36},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?","lvl4":"Membership","lvl3":"The core team","lvl2":"A Jupyter PEP 013"},"content":"Jupyter core team members demonstrate:\n\na good grasp of the philosophy of the Jupyter Project\na solid track record of being constructive and helpful\nsignificant contributions to the project‚Äôs goals, in any form\nwillingness to dedicate some time to improving Jupyter\nAs the project matures, contributions go beyond code. Here‚Äôs an incomplete list of areas where contributions may be considered for joining the core team, in no particular order:\n\nWorking on community management and outreach\n\nProviding support on the mailing lists, gitter rooms, and the community forum\n\nTriaging tickets\n\nWriting patches (code, docs, or tests)\n\nReviewing patches (code, docs, or tests)\n\nParticipating in design decisions\n\nProviding expertise in a particular domain (security, i18n, etc.)\n\nManaging the continuous integration infrastructure\n\nManaging the servers (website, tracker, documentation, etc.)\n\nMaintaining related projects (alternative interpreters, core infrastructure like packaging, etc.)\n\nCreating visual designs\n\nCore team membership acknowledges sustained and valuable efforts that align well with the philosophy and the goals of the Jupyter project.\n\nIt is granted by receiving at least two-thirds positive votes in a core team vote that is open for one week and with no veto by the steering council.\n\nCore team members are always looking for promising contributors, teaching them how the project is managed, and submitting their names to the core team‚Äôs vote when they‚Äôre ready.\n\nThere‚Äôs no time limit on core team membership. However, in order to provide the general public with a reasonable idea of how many people maintain Jupyter, core team members who have stopped contributing are encouraged to declare themselves as ‚Äúinactive‚Äù. Those who haven‚Äôt made any non-trivial contribution in two years may be asked to move themselves to this category, and moved there if they don‚Äôt respond. To record and honor their contributions, inactive team members will continue to be listed alongside active core team members; and, if they later resume contributing, they can switch back to active status at will. While someone is in inactive status, though, they lose their active privileges like voting or nominating for the steering council, and commit access.\n\nThe initial active core team members will consist of everyone with commit access to one of the Jupyter core repositories on Github.","type":"content","url":"/blog/2019/2019-10-27-jupyter-governance-python#membership","position":37},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?","lvl3":"Changing this document","lvl2":"A Jupyter PEP 013"},"type":"lvl3","url":"/blog/2019/2019-10-27-jupyter-governance-python#changing-this-document","position":38},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?","lvl3":"Changing this document","lvl2":"A Jupyter PEP 013"},"content":"Changes to this document require at least a two-thirds majority of votes cast in a core team vote which should be open for two weeks.","type":"content","url":"/blog/2019/2019-10-27-jupyter-governance-python#changing-this-document","position":39},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?","lvl2":"What‚Äôs different between Python and Jupyter?"},"type":"lvl2","url":"/blog/2019/2019-10-27-jupyter-governance-python#whats-different-between-python-and-jupyter","position":40},{"hierarchy":{"lvl1":"What would Python-style governance look like in Jupyter?","lvl2":"What‚Äôs different between Python and Jupyter?"},"content":"I was struck by how similar the new Python governance model is to\nthe Jupyter project‚Äôs current model. They both have a steering council approach,\nand a core team that surrounds it. That means that Jupyter could get pretty close\nby effectively doing these three things:\n\nReduce the steering council from 21 to 5 people\n\nFormalize the ‚Äúcore team‚Äù to be current Jupyter committers\n\nCodify and enforce the JEP process for discussion and decision-making\n\nThat said, there are a few ways in which I think Python and Jupyter are different.\nJupyter feels like it has more moving pieces across different parts of the stack. It has ‚Äúcore projects‚Äù that span everything from kernel specifications\nto user interfaces to cloud infrastructure. It also feels like Jupyter is more OK with\n‚Äúbleeding edge‚Äù software and quick release cycles (though this depends on the project,\nthe notebook has been pretty stable for a while now).\n\nPerhaps the first thing that a new Jupyter steering council should do is to create several\nsub-steering councils (maybe sub-‚Äúteams‚Äù) that serve in a steering-council-like role\nbut for a specific project (like jupyterhub, jupyterlab, the jupyter notebook specification, etc).\nThese could serve similarly to the sub-teams in the Rust community as well.\n\nAnother benefit of this model is that it doesn‚Äôt require a drastic shift from our\ncurrent governance. It‚Äôs more of a refinement and commitment to processes that we\nalready know about but rarely follow strictly.\n\nIn all, the Python model seems like a nice balance between the organized chaos of\na totally collective decision-making process, and the high-stress-and-bottlenecking\napproach of a BDFL-style model.","type":"content","url":"/blog/2019/2019-10-27-jupyter-governance-python#whats-different-between-python-and-jupyter","position":41},{"hierarchy":{"lvl1":"Testing Pandoc and Jupyter Notebooks"},"type":"lvl1","url":"/blog/2019/2019-11-11-ipynb-pandoc","position":0},{"hierarchy":{"lvl1":"Testing Pandoc and Jupyter Notebooks"},"content":"","type":"content","url":"/blog/2019/2019-11-11-ipynb-pandoc","position":1},{"hierarchy":{"lvl1":"Testing Pandoc and Jupyter Notebooks","lvl2":"Jupyter Notebooks to markdown and html with Pandoc"},"type":"lvl2","url":"/blog/2019/2019-11-11-ipynb-pandoc#jupyter-notebooks-to-markdown-and-html-with-pandoc","position":2},{"hierarchy":{"lvl1":"Testing Pandoc and Jupyter Notebooks","lvl2":"Jupyter Notebooks to markdown and html with Pandoc"},"content":"For several months now, the universal \n\ndocument converter pandoc has\nhad \n\nsupport for Jupyter Notebooks. This means that with a single call,\nyou can convert .ipynb files to any of the output formats that Pandoc\nsupports (and vice-versa!). This post is a quick exploration of what this\nlooks like.\n\nNote that for this post, we‚Äôre using Pandoc version 2.7.3. Also, some of what‚Äôs below is hard\nto interpret without actually opening the files that are created by Pandoc. For the sake\nof this blog post, I‚Äôm going to stick with the raw text output here, though you can expand the\noutputs if you wish, I recommend copy/pasting some of these commands on your own if you‚Äôd like to try.\n\nfrom subprocess import run as sbrun\nfrom subprocess import PIPE, CalledProcessError\nfrom pathlib import Path\nfrom IPython.display import HTML, Markdown\n\n# A helper function to capture errors and outputs\ndef run(cmd, *args, **kwargs):\n    try:\n        out = sbrun(cmd.split(), stderr=PIPE, stdout=PIPE, check=True, *args, **kwargs)\n        out = out.stdout.decode()\n        if len(out) > 1:\n            print(out)\n    except CalledProcessError as e:\n        print(e.stderr.decode())\n\n","type":"content","url":"/blog/2019/2019-11-11-ipynb-pandoc#jupyter-notebooks-to-markdown-and-html-with-pandoc","position":3},{"hierarchy":{"lvl1":"Testing Pandoc and Jupyter Notebooks","lvl2":"Our base notebook"},"type":"lvl2","url":"/blog/2019/2019-11-11-ipynb-pandoc#our-base-notebook","position":4},{"hierarchy":{"lvl1":"Testing Pandoc and Jupyter Notebooks","lvl2":"Our base notebook"},"content":"First off, let‚Äôs take a look at our base notebook. We‚Äôll convert this document\nto both Markdown and HTML using Pandoc.\n\nThe notebook will be fairly minimal\nin order to make it easier to inspect its contents. It has a collection\nof markdown with mixed content, as well as code cells with various outputs.\n\nSee this link for the notebook we‚Äôll use.\n\n","type":"content","url":"/blog/2019/2019-11-11-ipynb-pandoc#our-base-notebook","position":5},{"hierarchy":{"lvl1":"Testing Pandoc and Jupyter Notebooks","lvl2":".ipynb to markdown"},"type":"lvl2","url":"/blog/2019/2019-11-11-ipynb-pandoc#id-ipynb-to-markdown","position":6},{"hierarchy":{"lvl1":"Testing Pandoc and Jupyter Notebooks","lvl2":".ipynb to markdown"},"content":"Let‚Äôs try converting this notebook to markdown. This should preserve as much\ninformation as possible about the input Jupyter notebook. This should include\nall markdown cells, cell metadata, and outputs with code cells.","type":"content","url":"/blog/2019/2019-11-11-ipynb-pandoc#id-ipynb-to-markdown","position":7},{"hierarchy":{"lvl1":"Testing Pandoc and Jupyter Notebooks","lvl3":"A few pandoc options","lvl2":".ipynb to markdown"},"type":"lvl3","url":"/blog/2019/2019-11-11-ipynb-pandoc#a-few-pandoc-options","position":8},{"hierarchy":{"lvl1":"Testing Pandoc and Jupyter Notebooks","lvl3":"A few pandoc options","lvl2":".ipynb to markdown"},"content":"Here are a few pandoc options that are relevant to our use-case:\n\n--resource-path defines the path where Pandoc will look for resources that are linked in the notebook.\nThis allows us to discover images etc that are in a different folder from where we are invocing pandoc.\n\n--extract-media is a path where images and other media will be extracted at conversion time. Any links\nto images etc should point to files at this path in the output format.\n\n-s (or --standalone) tells Pandoc that the output should be a ‚Äústandalone‚Äù format. This does different\nthings depending on the output, such as adding a header if converting to HTML.\n\n-o the output file, and implicitly the output file type (e.g., markdown)\n\n-t the type of output file if we want to override the default (e.g., GitHub-flavored markdown vs. Pandoc markdown).\n\n","type":"content","url":"/blog/2019/2019-11-11-ipynb-pandoc#a-few-pandoc-options","position":9},{"hierarchy":{"lvl1":"Testing Pandoc and Jupyter Notebooks","lvl3":"Converting to GitHub-flavored markdown","lvl2":".ipynb to markdown"},"type":"lvl3","url":"/blog/2019/2019-11-11-ipynb-pandoc#converting-to-github-flavored-markdown","position":10},{"hierarchy":{"lvl1":"Testing Pandoc and Jupyter Notebooks","lvl3":"Converting to GitHub-flavored markdown","lvl2":".ipynb to markdown"},"content":"Let‚Äôs start by converting to GitHub-flavored markdown. By not specifying an output file\nwith -o, we‚Äôll cause Pandoc to print the result to the screen, which we‚Äôll display here.\n\n# ipynb -> gfmd\nrun(f'pandoc pandoc_ipynb/inputs/notebooks.ipynb --resource-path=inputs -s --extract-media=outputs/images -t gfm')\n\nNote that cells are divided by hard-coded <div>s, and cell-level metadata (such as tags)\nare encoded within the HTML (e.g. data-tags). Also note that we haven‚Äôt gotten the bibliography\nto render, probably because we didn‚Äôt enable the citeproc processor on pandoc (we‚Äôll try that later).\nFinally, note that there‚Äôs no notebook-level metadata in this output because GFM doesn‚Äôt support\na YAML header.\n\n","type":"content","url":"/blog/2019/2019-11-11-ipynb-pandoc#converting-to-github-flavored-markdown","position":11},{"hierarchy":{"lvl1":"Testing Pandoc and Jupyter Notebooks","lvl3":"To pandoc-flavored markdown","lvl2":".ipynb to markdown"},"type":"lvl3","url":"/blog/2019/2019-11-11-ipynb-pandoc#to-pandoc-flavored-markdown","position":12},{"hierarchy":{"lvl1":"Testing Pandoc and Jupyter Notebooks","lvl3":"To pandoc-flavored markdown","lvl2":".ipynb to markdown"},"content":"\n\n# ipynb -> pandoc md\nrun(f'pandoc pandoc_ipynb/inputs/notebooks.ipynb --resource-path=inputs -s --extract-media=outputs/images')\n\nNow we‚Äôve got something a little bit cleaner without all the hard-coded HTML. The ::: fences\nare how Pandoc-flavored markdown denote different divs, and cell-level metadata is encoded\nsimilar to how GFM worked.","type":"content","url":"/blog/2019/2019-11-11-ipynb-pandoc#to-pandoc-flavored-markdown","position":13},{"hierarchy":{"lvl1":"Testing Pandoc and Jupyter Notebooks","lvl2":".ipynb to HTML"},"type":"lvl2","url":"/blog/2019/2019-11-11-ipynb-pandoc#id-ipynb-to-html","position":14},{"hierarchy":{"lvl1":"Testing Pandoc and Jupyter Notebooks","lvl2":".ipynb to HTML"},"content":"Next let‚Äôs try converting .ipynb to HTML. This should let us view the notebook as a web-page\nas well as include all of the extra metadata inside the HTML elements. We‚Äôll start with\na vanilla HTML conversion. Note that the only thing we had to do was change the output\nfile extension to .html and Pandoc inferred the output type for us:\n\n# ipynb -> HTML\nrun(f'pandoc pandoc_ipynb/inputs/notebooks.ipynb --resource-path=inputs -s --extract-media=outputs/images')\n\nThis time our math rendered properly, along with everything else except for the\nbibliography. Let‚Äôs get that working now.\n\nWe‚Äôve included a bibliography with our input file. With this (and using the\n\n\nciteproc citation style, we can use pandoc-citeproc to automatically render a\nbibliography within each page. To do so, we‚Äôve used the following extra options:\n\n--bibliography specifies the path to a BibTex file\n\n-f ipynb+citations tells Pandoc that our input format has citations in it. Before, the ipynb was\ninferred from the input extension. Now we‚Äôve made it explicit as well.\n\n# ipynb -> HTML with citations\nrun(f'pandoc pandoc_ipynb/inputs/notebooks.ipynb -f ipynb+citations --bibliography pandoc_ipynb/inputsreferences.bib --resource-path=inputs -s --extract-media=outputs/images')\n\nNow we‚Äôve got citations at the bottom of the page, and in-line references interspersed\nin the text. Pretty cool!\n\n","type":"content","url":"/blog/2019/2019-11-11-ipynb-pandoc#id-ipynb-to-html","position":15},{"hierarchy":{"lvl1":"Testing Pandoc and Jupyter Notebooks","lvl2":"Wrapping up"},"type":"lvl2","url":"/blog/2019/2019-11-11-ipynb-pandoc#wrapping-up","position":16},{"hierarchy":{"lvl1":"Testing Pandoc and Jupyter Notebooks","lvl2":"Wrapping up"},"content":"It seems like we can get pretty far with converting .ipynb files into\nvarious flavors of markdown or HTML. My guess is that things will get a bit\ntrickier if we tried to do this with more complex cell outputs or metdata,\nbut it‚Äôs a good start. Using Pandoc also means that it would be relatively\nstraightforward to convert notebooks into latex, pdf, or even Microsoft Word\nformat. I‚Äôll try to dig into this more in the future.","type":"content","url":"/blog/2019/2019-11-11-ipynb-pandoc#wrapping-up","position":17},{"hierarchy":{"lvl1":"What do people think about rST?"},"type":"lvl1","url":"/blog/2020/2020-01-22-rst-thoughts","position":0},{"hierarchy":{"lvl1":"What do people think about rST?"},"content":"Publishing computational narratives has always been a dream of the Jupyter Project,\nand there is still a lot of work to be done in improving these use-cases. We‚Äôve made\na lot of progress in providing open infrastructure for reproducible science with\n\n\nJupyterHub and\n\n\nthe Binder Project, but what about the documents themselves?\nWe‚Äôve recently been working on tools like \n\nJupyter Book,\nwhich aim to improve the writing and publishing process with the Jupyter ecosystem.\nThis is hopefully the first post of a few that ask how we can best-improve the state\nof publishing with Jupyter.\n\nUpdate!\n\nMany of the ideas in this post have now made their way into a new flavor of markdown called \n\nMarkedly Structured Text, or MyST. It brings all of the features of rST into Markdown. Check it out!\n\nPython has a fairly sophisticated publishing tool in its stack. \n\nSphinx\nhas been a staple for publishing documentation for packages for several years now.\nInterestingly, publishing a book is more similar to publishing a package‚Äôs documentation\nthan it is to, say, publishing a blog. Maybe we could use Sphinx more heavily for\nwriting computational narratives.\n\nOne of the major challenges with Sphinx is that its default markup language is reStructuredText,\na fairly old but battle-tested markup language. The benefit of reStructuredText is that it is\na semantic language, meaning that it has ways to store more information about the nature\nof the text (e.g. something is an ‚Äúauthor‚Äù, something is a ‚Äúreference‚Äù, etc). It is also a standard\nthat has remained very stable over time (whether that‚Äôs a good or bad thing I‚Äôll leave to you to decide).\n\nHowever, there are a few major problems with reStructuredText that have impeded its adoption by\ncommunities outside of the Python documentation world. I recently \n\nasked around on Twitter\nwhat these problems were.\nI got some interesting responses! Here is a quick summary of people‚Äôs thoughts.","type":"content","url":"/blog/2020/2020-01-22-rst-thoughts","position":1},{"hierarchy":{"lvl1":"What do people think about rST?","lvl2":"The syntax of rST is too confusing"},"type":"lvl2","url":"/blog/2020/2020-01-22-rst-thoughts#the-syntax-of-rst-is-too-confusing","position":2},{"hierarchy":{"lvl1":"What do people think about rST?","lvl2":"The syntax of rST is too confusing"},"content":"By far the most common response was that rST syntax is simply too confusing. Here were the main\npain points.","type":"content","url":"/blog/2020/2020-01-22-rst-thoughts#the-syntax-of-rst-is-too-confusing","position":3},{"hierarchy":{"lvl1":"What do people think about rST?","lvl3":"Link syntax","lvl2":"The syntax of rST is too confusing"},"type":"lvl3","url":"/blog/2020/2020-01-22-rst-thoughts#link-syntax","position":4},{"hierarchy":{"lvl1":"What do people think about rST?","lvl3":"Link syntax","lvl2":"The syntax of rST is too confusing"},"content":"Many folks particularly mentioned that they needed to look up how to construct links every time they wrote rST.\n\nFor reference, a link in rST looks like this:This `Is a link to <https://google.com>`_.\n\nCompared with markdown:This [is a link to](https://google.com)","type":"content","url":"/blog/2020/2020-01-22-rst-thoughts#link-syntax","position":5},{"hierarchy":{"lvl1":"What do people think about rST?","lvl3":"Header complexity","lvl2":"The syntax of rST is too confusing"},"type":"lvl3","url":"/blog/2020/2020-01-22-rst-thoughts#header-complexity","position":6},{"hierarchy":{"lvl1":"What do people think about rST?","lvl3":"Header complexity","lvl2":"The syntax of rST is too confusing"},"content":"rST uses ‚Äúsetext‚Äù headers, which means that you put a bunch of underline-like\ncharacters under (or under+over) the header name itself, like so:This is my first header\n=======================\n\n======================\nThis is another header\n======================\n\nCompare this to ‚ÄúATX headers‚Äù, which markdown uses and look like this:# This is a header\n\nSetting aside the annoyance of having to hold down the ‚Äú=‚Äù a bunch of times,\nthe big problem with rST headers is that header characters in rST have no single\nmapping onto header hierarchy. For example:This is my first header\n~~~~~~~~~~~~~~~~~~~~~~~\n\nAnd this is my second header\n============================\n\nis the same asThis is my first header\n=======================\nAnd this is my second header\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThis is a case where too much flexibility makes life more difficult than it needs to be. Many\nresponses wished that rST simply used ‚ÄúATX‚Äù headers (using # in front of titles) or chose\na single hierarchy of header characters.","type":"content","url":"/blog/2020/2020-01-22-rst-thoughts#header-complexity","position":7},{"hierarchy":{"lvl1":"What do people think about rST?","lvl3":"In-line code","lvl2":"The syntax of rST is too confusing"},"type":"lvl3","url":"/blog/2020/2020-01-22-rst-thoughts#in-line-code","position":8},{"hierarchy":{"lvl1":"What do people think about rST?","lvl3":"In-line code","lvl2":"The syntax of rST is too confusing"},"content":"Many folks also dislike the fact that in rST, you must denote in-line code blocks with two\nbackticks instead of one. For example, this is rST:Here is some inline code: ``a = 2``\n\nWhile here is some markdownHere is some inline code: `a = 2`\n\nIt may seem silly, but markdown‚Äôs ubiquity has given most people the assumption that backticks==code.\nThe fact that rST deviates from this adds unnecessary cognitive burden to most users.","type":"content","url":"/blog/2020/2020-01-22-rst-thoughts#in-line-code","position":9},{"hierarchy":{"lvl1":"What do people think about rST?","lvl3":"Nesting in-line markup","lvl2":"The syntax of rST is too confusing"},"type":"lvl3","url":"/blog/2020/2020-01-22-rst-thoughts#nesting-in-line-markup","position":10},{"hierarchy":{"lvl1":"What do people think about rST?","lvl3":"Nesting in-line markup","lvl2":"The syntax of rST is too confusing"},"content":"Finally, there were several mentions of rST‚Äôs strange inability to nest in-line formatting.\nE.g. being able to bold a link by nesting ** inside of your link syntax.","type":"content","url":"/blog/2020/2020-01-22-rst-thoughts#nesting-in-line-markup","position":11},{"hierarchy":{"lvl1":"What do people think about rST?","lvl3":"A quick summary","lvl2":"The syntax of rST is too confusing"},"type":"lvl3","url":"/blog/2020/2020-01-22-rst-thoughts#a-quick-summary","position":12},{"hierarchy":{"lvl1":"What do people think about rST?","lvl3":"A quick summary","lvl2":"The syntax of rST is too confusing"},"content":"Here is a quick list of the tweets that touched on the topic of syntax:\n\nLinks syntax is confusing (9 total)\n\nLinks are confusing: \n\nhttps://‚Äãtwitter‚Äã.com‚Äã/asmeurer‚Äã/status‚Äã/1212468755768336384\n\nBad syntax, esp links: \n\nhttps://‚Äãtwitter‚Äã.com‚Äã/njgoldbaum‚Äã/status‚Äã/1212059796142055424\n\nLinks structure: \n\nhttps://‚Äãtwitter‚Äã.com‚Äã/SylvainCorlay‚Äã/status‚Äã/1212061834116816898\n\nExternal links: \n\nhttps://‚Äãtwitter‚Äã.com‚Äã/WillingCarol‚Äã/status‚Äã/1212152304800894976\n\nSyntax / too complex to write: \n\nhttps://‚Äãtwitter‚Äã.com‚Äã/minrk‚Äã/status‚Äã/1212119686009233410\n\nSyntax, esp links and tables: \n\nhttps://‚Äãtwitter‚Äã.com‚Äã/‚Äã_JacobTomlinson‚Äã/status‚Äã/1212104705809289219\n\nImprove links: \n\nhttps://‚Äãtwitter‚Äã.com‚Äã/andreazonca‚Äã/status‚Äã/1212166686389858306\n\nLinks and heading permalinks: \n\nhttps://‚Äãtwitter‚Äã.com‚Äã/goerz‚Äã/status‚Äã/1212610069252263936\n\nInline hyperlinks: \n\nhttps://‚Äãtwitter‚Äã.com‚Äã/moorepants‚Äã/status‚Äã/1212072806994739200\n\nHeader / title syntax (3 total)\n\nUsing underlines for headers: \n\nhttps://‚Äãtwitter‚Äã.com‚Äã/phaustin‚Äã/status‚Äã/1212067821976375296\n\nTitles: \n\nhttps://‚Äãtwitter‚Äã.com‚Äã/mfcabrera‚Äã/status‚Äã/1212126606170431489\n\nTitle lines are confusing: \n\nhttps://‚Äãtwitter‚Äã.com‚Äã/alienghic‚Äã/status‚Äã/1212119125398437888\n\nTwo backticks for code\n\nTwo Backticks for code literals: \n\nhttps://‚Äãtwitter‚Äã.com‚Äã/brettsky‚Äã/status‚Äã/1212422437683351553\n\nNested inline markup (e.g. ‚Äòem‚Äô inside of ‚Äòstrong‚Äô) (2 total)\n\nNested inline markup: \n\nhttps://‚Äãtwitter‚Äã.com‚Äã/asmeurer‚Äã/status‚Äã/1212468755768336384\n\nnested ‚Äúem‚Äù in ‚Äústrong‚Äù: \n\nhttps://‚Äãtwitter‚Äã.com‚Äã/uranusjr‚Äã/status‚Äã/1212179877341720577","type":"content","url":"/blog/2020/2020-01-22-rst-thoughts#a-quick-summary","position":13},{"hierarchy":{"lvl1":"What do people think about rST?","lvl2":"Error reporting and the complexity of Sphinx"},"type":"lvl2","url":"/blog/2020/2020-01-22-rst-thoughts#error-reporting-and-the-complexity-of-sphinx","position":14},{"hierarchy":{"lvl1":"What do people think about rST?","lvl2":"Error reporting and the complexity of Sphinx"},"content":"The other major complaint people had was in the toolchain itself. Sphinx is an incredibly powerful tool,\nbut this comes with a degree of complexity that many find difficult to work through. This isn‚Äôt helped\nby the fact that the \n\nSphinx documentation\nis itself incomplete in many sections (the irony of this is not lost on me).\n\nIn particular, several people commented about the difficulty in surfacing and debugging errors that happen\nin the Sphinx build chain. They also mentioned that Sphinx can be slow to build sometimes, which\nbogs down the development and writing process.\n\nHere are the tweets about the Sphinx toolchain itself:\n\nError reporting / complexity of Sphinx itself (3 total)\n\nError reporting: \n\nhttps://‚Äãtwitter‚Äã.com‚Äã/asmeurer‚Äã/status‚Äã/1212468909078544384\n\nError reporting in Sphinx etc: \n\nhttps://‚Äãtwitter‚Äã.com‚Äã/GaelVaroquaux‚Äã/status‚Äã/1212058374981988352\n\nDocumentation is bad: \n\nhttps://‚Äãtwitter‚Äã.com‚Äã/AkhmerovAnton‚Äã/status‚Äã/1212059839033171968\n\nThere were also a few miscellaneous responses that didn‚Äôt quite fit into the above categories:\n\nMisc (6 total)\n\nIsn‚Äôt a standard: \n\nhttps://‚Äãtwitter‚Äã.com‚Äã/aterrel‚Äã/status‚Äã/1212132546307350529\n\nNewlines in nested lists, not easy to deploy: \n\nhttps://‚Äãtwitter‚Äã.com‚Äã/westurner‚Äã/status‚Äã/1212178375571365891\n\nNo notebook support: \n\nhttps://‚Äãtwitter‚Äã.com‚Äã/moorepants‚Äã/status‚Äã/1212062811599147008\n\nHas state: \n\nhttps://‚Äãtwitter‚Äã.com‚Äã/Mbussonn‚Äã/status‚Äã/1212069224245551104\n\nGeneral complexity: \n\nhttps://‚Äãtwitter‚Äã.com‚Äã/mrocklin‚Äã/status‚Äã/1212123762595811328","type":"content","url":"/blog/2020/2020-01-22-rst-thoughts#error-reporting-and-the-complexity-of-sphinx","position":15},{"hierarchy":{"lvl1":"What do people think about rST?","lvl2":"What can be done?"},"type":"lvl2","url":"/blog/2020/2020-01-22-rst-thoughts#what-can-be-done","position":16},{"hierarchy":{"lvl1":"What do people think about rST?","lvl2":"What can be done?"},"content":"Overall, it seems like reStructuredText could be much-improved with a few minor\nmodifications to its syntax. These don‚Äôt seem like they are structurally\nincompatible with rST, and would alleviate some of the cognitive\nburden that users report when they use it. Here‚Äôs a quick list of simple things:\n\nrST could use markdown syntax for external links.\n\nrST could decide on a fixed interpretation of header characters to levels in the header hierarchy\n\nrST could default to interpreting single backticks as raw code spans\n\nand a list of slightly more complex things:\n\nrST could support nested styling inside of links and other elements\n\nSphinx could improve its error reporting and debugging machinery\n\nSphinx could improve its documentation so that it was easier to find an answer to a question","type":"content","url":"/blog/2020/2020-01-22-rst-thoughts#what-can-be-done","position":17},{"hierarchy":{"lvl1":"What do people think about rST?","lvl2":"Or could we bring reStructuredText into Markdown?"},"type":"lvl2","url":"/blog/2020/2020-01-22-rst-thoughts#or-could-we-bring-restructuredtext-into-markdown","position":18},{"hierarchy":{"lvl1":"What do people think about rST?","lvl2":"Or could we bring reStructuredText into Markdown?"},"content":"There is another option, of course, which is to go in the opposite direction. Start with markdown,\nand then ask ‚Äúhow could we build the flexibility of rST into markdown‚Äù rather than bringing the\nsimplicity of markdown into reStructuredText. I often wonder if the easiest thing to do would be\nto simply decide on a markdown syntax that maps on to ‚Äúdirectives‚Äù and ‚Äúroles‚Äù (perhaps the Pandoc\ncode fence ::: for directives, and link attributes []{attribute} for roles). I think that both\nare worth exploring.\n\nIn summary, I was surprised at the consistency of people‚Äôs complains about the rST language. It\nseems that many people are hung up about the same relatively minor syntax choices, and that making\nmodifications to these choices would improve the experience for many. It‚Äôs also clear that Sphinx\ncould use some developer time to make it more robust, debuggable, and well-documented. I hope that\nwe can make some progress on these issues in the coming years.","type":"content","url":"/blog/2020/2020-01-22-rst-thoughts#or-could-we-bring-restructuredtext-into-markdown","position":19},{"hierarchy":{"lvl1":"Contributing to open source: A short guide for organizations"},"type":"lvl1","url":"/blog/2020/organizations-help-oss-guide","position":0},{"hierarchy":{"lvl1":"Contributing to open source: A short guide for organizations"},"content":"Over the years I‚Äôve had a recurring question from people who are in organizations both big and small: how can we participate in open source communities?\n\nWhether it is because of altruism or strategic importance, many companies, research groups, non-profits, etc want to be involved in open source projects (particularly large and impactful ones like Jupyter), but getting involved can be an opaque and confusing process if you‚Äôre not already familiar with open source. Each community has its own nuances and social dynamics, and approaching from the outside can be a challenge.\n\nSo this post is a short guide to provide some ideas for how others can begin to engage with open source. My hope is that it gives people who are embedded in organizations some ideas about how they could engage with open source, or advocate for engagement internally.\n\nNote\n\nThis is all just personal opinion, and based on my own experiences so far. I‚Äôll try to keep it updated over time as my own thoughts evolve. If you‚Äôve got ideas of your own I‚Äôd love to hear them in the comments.","type":"content","url":"/blog/2020/organizations-help-oss-guide","position":1},{"hierarchy":{"lvl1":"Contributing to open source: A short guide for organizations","lvl2":"Meet open source communities where they are"},"type":"lvl2","url":"/blog/2020/organizations-help-oss-guide#meet-open-source-communities-where-they-are","position":2},{"hierarchy":{"lvl1":"Contributing to open source: A short guide for organizations","lvl2":"Meet open source communities where they are"},"content":"If there is anything I‚Äôd emphasize the most, it is that organizations need to meet open communities where they are - that means abiding by community values and social norms, and accepting that community priorities may be different from those of an organization.\nA few quick ideas for doing this:\n\nAt first, treat yourself as a visitor. It will take time to build understanding, status, and trust within a community. Until that happens, it‚Äôs important to meet open source communities as peers and consider yourself a visitor. Listen, learn, engage, and adapt your behavior to that of the community. Don‚Äôt assume that you should have any special status because you‚Äôre a large or well-known organization.\n\nParticipate in community spaces. The easiest way to learn how a community operates is to participate in the online spaces where it operates. If there are community meetings, attend them and introduce yourself. If the community has a forum, show up and say hello. If the community uses GitHub issues, watch for what kinds of things people discuss. Offer help, advise, and guidance wherever you can. This will help you understand the needs and direction of the project.\n\nAbide by community culture. For many communities, a lot of care and thought is put into building community dynamics that are open, inclusive, deliberate, and that balance power across the many stakeholders in the ecosystem. It is crucial that you abide by these cultural dynamics, especially with respect to things like Code of Conduct and interpersonal interactions. Taking this time to understand the culture will make the experience much more pleasant for everybody involved.\n\nRespect the community priorities. Many organizations reach out only at the moment where they need something. This is understandable, but a very inefficient way of getting things done in open source communities. There are a variety of needs, priorities, and interests in the community. You should begin by understanding what these are and finding ways to engage with them before focusing on your own priorities or asking for (or offering) changes to suit your needs. The trust that comes from helping someone else with their problem will make it much easier for you to get your own issues fixed in the future.\n\nAccept that things will move more slowly. I know that you have a product or a feature to ship, and that you want to make rapid progress. But, open source communities generally are (and generally should be) slower and more deliberate in their decision-making than many organizations are used-to. Slowing things down ensures that a diverse collection of voices have an opportunity to weigh in on questions, and keeps the community more inclusive and participatory. It also often leads to better decisions that take into consideration more perspectives.\n\nAfter you spend some time engaging with open source communities, there are a few concrete steps you can take towards making contributions. Here are a few ideas...","type":"content","url":"/blog/2020/organizations-help-oss-guide#meet-open-source-communities-where-they-are","position":3},{"hierarchy":{"lvl1":"Contributing to open source: A short guide for organizations","lvl2":"Help with development and repositories"},"type":"lvl2","url":"/blog/2020/organizations-help-oss-guide#help-with-development-and-repositories","position":4},{"hierarchy":{"lvl1":"Contributing to open source: A short guide for organizations","lvl2":"Help with development and repositories"},"content":"After you‚Äôve spent some time interacting with others in the community, perhaps you‚Äôd like to make contributions of your own to the codebase or tools. How can you do this in a way that benefits the community?\n\nA note on non-coding support\n\nNote that, while code is often the output of an open source community, there are many non-coding ways to contribute to that community. Many of the ‚Äúcode-related‚Äù activities are actually ‚Äúpeople-related‚Äù activities, such as assisting others with their problems, or engaging in discussion in issues.","type":"content","url":"/blog/2020/organizations-help-oss-guide#help-with-development-and-repositories","position":5},{"hierarchy":{"lvl1":"Contributing to open source: A short guide for organizations","lvl3":"Upstream improvements when using core technology","lvl2":"Help with development and repositories"},"type":"lvl3","url":"/blog/2020/organizations-help-oss-guide#upstream-improvements-when-using-core-technology","position":6},{"hierarchy":{"lvl1":"Contributing to open source: A short guide for organizations","lvl3":"Upstream improvements when using core technology","lvl2":"Help with development and repositories"},"content":"By far the most-common way that organizations contribute back is by ‚Äúupstreaming‚Äù improvements to the open source project as they notice problems or opportunities to improve the code.\n\nIf you‚Äôre using a tool regularly, or building it into another tool like one of your products, look for small ways that it could be better. Rather than fixing these issues in your fork of the project or your internal codebase, open an issue in the core repository and offer to fix it if others agree it‚Äôs a problem. If a team member has an idea to improve the tool, don‚Äôt just build it in your products, improve it in core.\n\nExample: The Jupyter Server\n\nThe \n\nJupyter Server project is an attempt at pulling out much of the server architecture from the original Notebook application. It‚Äôs a very low-level library that is re-usable across a number of other potential tools. Its presence benefits the whole ecosystem, but because it is so generically useful, it has few dedicated resources to maintian and develop.\n\nExample: Accessibility\n\nThis is one of those things that is super hard to get volunteer support for because it is not sexy and it is hard work. Many organizations care about accessibility for a variety of reasons, and taking the time to give back guidance and development to make core tools more accessible is a huge benefit. On this point in particular, I think it is important that organizations not make accessibility a competetive advantage over open source tools, but instead contribute to the common good so that these tools are more inclusive and available to all.","type":"content","url":"/blog/2020/organizations-help-oss-guide#upstream-improvements-when-using-core-technology","position":7},{"hierarchy":{"lvl1":"Contributing to open source: A short guide for organizations","lvl3":"Contribute to the documentation","lvl2":"Help with development and repositories"},"type":"lvl3","url":"/blog/2020/organizations-help-oss-guide#contribute-to-the-documentation","position":8},{"hierarchy":{"lvl1":"Contributing to open source: A short guide for organizations","lvl3":"Contribute to the documentation","lvl2":"Help with development and repositories"},"content":"Documentation is crucial to the success of open source, as it is the interface between people and the code. Documentation should be clear, searchable and findable, and should cover all of the functionality in a tool. However, it‚Äôs very difficult to properly-document open source software!\n\nNew users are in an extremely good position to point out inconsistencies, missing information, and unclear explanations in the documentation. If someone in your organization is trying to use an open-source tool for the first time, ask them to spend a few moments providing feedback about how the documentation could be improved, or even better, making a PR to the docs.","type":"content","url":"/blog/2020/organizations-help-oss-guide#contribute-to-the-documentation","position":9},{"hierarchy":{"lvl1":"Contributing to open source: A short guide for organizations","lvl3":"Work on major features the community has prioritized","lvl2":"Help with development and repositories"},"type":"lvl3","url":"/blog/2020/organizations-help-oss-guide#work-on-major-features-the-community-has-prioritized","position":10},{"hierarchy":{"lvl1":"Contributing to open source: A short guide for organizations","lvl3":"Work on major features the community has prioritized","lvl2":"Help with development and repositories"},"content":"If you‚Äôve got more substantial development time to devote to a project, see what kinds of features people want, and offer to help tackle something that aligns with your own interests. This is much easier if you do so after interacting with the community for a bit, as you will understand its priorities, as well as the underlying codebase, enough to be more efficient and productive.\n\nTake a look at the issues or roadmap of a project and find ones that you‚Äôd also benefit from, chime in and mention that you‚Äôd like to try and make these changes to see if others are excited about it. Then, follow-through and engage with the community as you make changes. Don‚Äôt be resistant to feedback - remember that you are a part of a broader community with its own norms and expectations around code style, structure, etc.\n\nExample: Look for üëç in issues\n\nThe Executable Books Project has \n\na feature voting leaderboard that it uses to let users vote with a üëç. You can even see this list for any GitHub repository by sorting issues by üëç reactions. For example, \n\nhere are the issues with the most üëç in JupyterLab.","type":"content","url":"/blog/2020/organizations-help-oss-guide#work-on-major-features-the-community-has-prioritized","position":11},{"hierarchy":{"lvl1":"Contributing to open source: A short guide for organizations","lvl3":"Provide core maintenance and support","lvl2":"Help with development and repositories"},"type":"lvl3","url":"/blog/2020/organizations-help-oss-guide#provide-core-maintenance-and-support","position":12},{"hierarchy":{"lvl1":"Contributing to open source: A short guide for organizations","lvl3":"Provide core maintenance and support","lvl2":"Help with development and repositories"},"content":"If you have spent some time interacting with a community, working through bugs and features, etc, you may consider providing more ongoing core support for a project. This is the holy grail of engagement, as core support and maintenance are one of the scarcest resources. Offer to read through issues and triage on a regular basis, routinely engage in pull requests and provide feedback and advise, spot-check bugs over time that others have brought up, do community work to organize meetings or work plans.\n\nWhen I try to figure out who is being helpful in the Jupyter ecosystem, I ask questions like ‚Äúwho is doing the things that nobody else is doing?‚Äù. E.g., who is the first person to respond to an issue? who is making documentation improvements, or helping an inexperienced contributor improve their PR? These little ‚Äúcarrying water and chopping wood‚Äù tasks are not flashy and exciting, but they are crucial to keeping the community dynamic, welcoming, and productive.\n\nExample: Contributing guidelines\n\nTake a look at the \n\nJupyterHub Team contributing guidelines - most of them aren‚Äôt strictly technical, they‚Äôre about helping others, helping the team in conversations, and generally just being friendly and productive community members.","type":"content","url":"/blog/2020/organizations-help-oss-guide#provide-core-maintenance-and-support","position":13},{"hierarchy":{"lvl1":"Contributing to open source: A short guide for organizations","lvl2":"Collaborate with open source projects"},"type":"lvl2","url":"/blog/2020/organizations-help-oss-guide#collaborate-with-open-source-projects","position":14},{"hierarchy":{"lvl1":"Contributing to open source: A short guide for organizations","lvl2":"Collaborate with open source projects"},"content":"Beyond directly working inside of an open source community, there are also plenty of ways that you engage with them as collaborators and partners. Here are a few concrete ideas.","type":"content","url":"/blog/2020/organizations-help-oss-guide#collaborate-with-open-source-projects","position":15},{"hierarchy":{"lvl1":"Contributing to open source: A short guide for organizations","lvl3":"Collaborate on open standards and APIs","lvl2":"Collaborate with open source projects"},"type":"lvl3","url":"/blog/2020/organizations-help-oss-guide#collaborate-on-open-standards-and-apis","position":16},{"hierarchy":{"lvl1":"Contributing to open source: A short guide for organizations","lvl3":"Collaborate on open standards and APIs","lvl2":"Collaborate with open source projects"},"content":"Beyond the open source core, there are often places where you wish to build out your own functionality. In these cases, it‚Äôs important that new APIs and patterns of interaction happen in conversation with the open source community, in order to ensure as much standardization as possible across ecosystems.\n\nAny time that you‚Äôd like to extend functionality beyond what‚Äôs already there, and have to make a semi-arbitrary decision of what pattern to expose to users, standardize on something the community already uses so that there isn‚Äôt unnecessary duplication and fracturing across ecosystems. If no standard exists, don‚Äôt just create one - reach out to the open source community and lead a process that gets others to brainstorm and buy-in to a new standard.\n\nExample: The ipynb format\n\nThe ipynb format. One of Jupyter‚Äôs core goals is to standardize tools and patterns across the data science community. The ipynb format is probably the most common example. As more tools build their own notebook functionality and wish to extend ipynb to do new things, they should follow metadata standards (or lead processes to create new ones). For example, we don‚Äôt want ipynb files that have platform-specific metadata. Even little stuff like ‚Äúfor cell-level metadata to hide inputs, do we call it hide_input or hideInput?‚Äù This can seem trivial and arbitrary but it is important and in many ways the most important thing that open communities do! When in doubt, ask around and get buy-in from others.","type":"content","url":"/blog/2020/organizations-help-oss-guide#collaborate-on-open-standards-and-apis","position":17},{"hierarchy":{"lvl1":"Contributing to open source: A short guide for organizations","lvl3":"Advocate for open tools from the community","lvl2":"Collaborate with open source projects"},"type":"lvl3","url":"/blog/2020/organizations-help-oss-guide#advocate-for-open-tools-from-the-community","position":18},{"hierarchy":{"lvl1":"Contributing to open source: A short guide for organizations","lvl3":"Advocate for open tools from the community","lvl2":"Collaborate with open source projects"},"content":"While developing and improving technology is a core part of open source, there is also a great deal of advocacy needed to expand their use and to get feedback. Organizational partners can be a huge help here by finding ways to support and highlight open source tooling in their stack, and by explicitly highlighting this tooling in tandem with organizational products.\n\nThis can be a bit tricky (competive landscape etc), but the rubric that I shoot for is: use community-led products wherever possible, and if that‚Äôs not possible, make sure to highlight and advocate for community-led products in-tandem with your own products.\n\nExample: Workshops and training material\n\nMany organizations run workshops, demos, and training sessions for their products. They also have sizeable marketing departments for the work they do. In all of these efforts, make it clear when you are relying on an open source tool for support, and highlight the ways in which you are engaging with its community and giving back.\n\nFor example, \n\nspacy.io is an NLP framework built by \n\nexplosion.ai. As part of their online documentation, they include interactive code sessions powered by \n\nBinder kernels. They make sure to include the name ‚ÄúBinder‚Äù whenever these kernels are spun-up in order to give credit and thanks to the free \n\nmybinder.org service.","type":"content","url":"/blog/2020/organizations-help-oss-guide#advocate-for-open-tools-from-the-community","position":19},{"hierarchy":{"lvl1":"Contributing to open source: A short guide for organizations","lvl3":"Financially support open source projects","lvl2":"Collaborate with open source projects"},"type":"lvl3","url":"/blog/2020/organizations-help-oss-guide#financially-support-open-source-projects","position":20},{"hierarchy":{"lvl1":"Contributing to open source: A short guide for organizations","lvl3":"Financially support open source projects","lvl2":"Collaborate with open source projects"},"content":"Some open source projects have ways to support them directly through funding. For those that do, an easy way to contribute is to provide some of your own financial resources for these things. Check whether the project, or any major contributors in the project, have a \n\nGitHub Sponsors or a \n\nPatreon page. Investigate whether the \n\nproject is fiscally sponsored and can accept donations. Other projects perform focused fundraising, for example the Jupyter Project has a \n\nJupyter Contributor in Residence position that it must raise funds for.\n\nAlternatively, look into whether you can sponsor open-source work via other companies or contractors. Many companies do contract-style work in open source, and welcome funding to ‚Äúbuy out‚Äù open source time.  Fundraising can be a time-consuming effort, and offering your resources can help make a quick impact if the project is in a good position to accept them.","type":"content","url":"/blog/2020/organizations-help-oss-guide#financially-support-open-source-projects","position":21},{"hierarchy":{"lvl1":"Contributing to open source: A short guide for organizations","lvl2":"Bottom-line"},"type":"lvl2","url":"/blog/2020/organizations-help-oss-guide#bottom-line","position":22},{"hierarchy":{"lvl1":"Contributing to open source: A short guide for organizations","lvl2":"Bottom-line"},"content":"There are many different ways that you can participate in open source communities. Fundamentally, this comes down to being attentive, flexible, and generous. Each community is different, but some combination of the above steps will help you make progress and make an impact in working with many open source communities.\n\nIf I‚Äôve missed something important, please do chime in and suggest other ways that organizations can get involved. I would love to continue updating this list as new ideas come to light.","type":"content","url":"/blog/2020/organizations-help-oss-guide#bottom-line","position":23},{"hierarchy":{"lvl1":"A new blog with Sphinx"},"type":"lvl1","url":"/blog/2020/sphinx-blogging","position":0},{"hierarchy":{"lvl1":"A new blog with Sphinx"},"content":"I recently re-wrote all of the infrastructure for my blog so that it now builds on top of the Sphinx ecosystem! This is a short post to describe the reasons for doing so, and a bit about the implementation.","type":"content","url":"/blog/2020/sphinx-blogging","position":1},{"hierarchy":{"lvl1":"A new blog with Sphinx","lvl2":"Why did you re-write your blog?"},"type":"lvl2","url":"/blog/2020/sphinx-blogging#why-did-you-re-write-your-blog","position":2},{"hierarchy":{"lvl1":"A new blog with Sphinx","lvl2":"Why did you re-write your blog?"},"content":"This is a great question. The answer to ‚Äúshould you re-work your blog to use a new SSG‚Äù is almost always ‚Äúno, it‚Äôs a waste of your time‚Äù, but I think I had a few good reasons ;-)\n\nüê∂ Dog Fooding\n\nFirst, I‚Äôve been doing a lot of work with the \n\nExecutable Books Project lately. As a result, \n\nJupyter Book now depends on Sphinx. The more I use Sphinx in my own workflows, the more I‚Äôll be equipped to improve the Sphinx ecosystem and Jupyter Book in ways that will benefit its users.\n\nüéÅ Upstreaming\n\nA big reason for moving Jupyter Book to Sphinx was to give ourselves more excuse to upstream improvements to the broader community. Switching over my own blog is for the same reason.\n\nüå∑ Simplicity\n\nI understand the Sphinx ecosystem relatively well, and use it across many of my projects. It‚Äôs got a number of great themes and a ton of extensions to do more things with my content. Moreover, it‚Äôs üíØ Python and I don‚Äôt have to worry about installing extra languages etc to build my pages.\n\nüöÄ Features\n\nIt turns out that building your blog (or any content for that matter) on top of a documentation engine gives you a lot of extra things to try that you don‚Äôt usually get from a SSG. Being able to use \n\nSphinx roles/directives from within a page is pretty cool. I can do stuff like this:\n\nHere‚Äôs a dropdown!\n\nAnd here‚Äôs some stuff inside!\n\nWow, a tip!\n\nWhat a great tip!\n\nü™ê Executable content and notebooks\n\nOn top of the base Sphinx features, \n\nMyST-NB now lets me write my entire post in notebooks, and will execute and cache the content for me if I wish. I can also \n\nwrite the whole notebook as markdown which keeps my posts easily diff-able.","type":"content","url":"/blog/2020/sphinx-blogging#why-did-you-re-write-your-blog","position":3},{"hierarchy":{"lvl1":"A new blog with Sphinx","lvl2":"The components that make up this blog"},"type":"lvl2","url":"/blog/2020/sphinx-blogging#the-components-that-make-up-this-blog","position":4},{"hierarchy":{"lvl1":"A new blog with Sphinx","lvl2":"The components that make up this blog"},"content":"OK so how is this blog actually built now? Here is a quick rundown.\n\nWhere is the blog source code? At \n\nthis github repository\n\nWhat engine builds the website? The \n\nSphinx documentation engine is the core documentation engine.\n\nWhat engine builds the blog? I use the \n\nablog Sphinx blog extension to host the actual blog.\n\nWhat theme do you use for the blog? I use the \n\nPyData Sphinx Theme\n\nHow do you write Sphinx docs in markdown? I use the \n\nmyst-parser package\n\nHow do you write content in notebooks? I use the \n\nMyST-NB package.\n\nHow did you make those fancy buttons and dropdowns? That was the \n\nsphinx-panels Sphinx extension.\n\nSpecifically, \n\nmy website is hosted here and all of the blog posts are \n\nin this folder.\nI‚Äôm configuring ABlog to \n\nfind any markdown files in this folder and treat them as blog posts.","type":"content","url":"/blog/2020/sphinx-blogging#the-components-that-make-up-this-blog","position":5},{"hierarchy":{"lvl1":"A new blog with Sphinx","lvl2":"In conclusion"},"type":"lvl2","url":"/blog/2020/sphinx-blogging#in-conclusion","position":6},{"hierarchy":{"lvl1":"A new blog with Sphinx","lvl2":"In conclusion"},"content":"So far, I quite like this new blog setup. Sphinx is familiar to me, and I think that using this for my own blog will help me understand how the ecosystem could be further-improved to support blogs in other contexts. If you want to check out this blog‚Äôs setup, see \n\nthis blog‚Äôs repository to get started! Next up, I‚Äôm going to see how easy it is to do this in Jupyter Book!","type":"content","url":"/blog/2020/sphinx-blogging#in-conclusion","position":7},{"hierarchy":{"lvl1":"Build a simple timeline with sphinx-design"},"type":"lvl1","url":"/blog/2020/sphinx-design-timeline","position":0},{"hierarchy":{"lvl1":"Build a simple timeline with sphinx-design"},"content":"This probably doesn‚Äôt work anymore\n\nI‚Äôm building my blog with the \n\nMyST Markdown engine now, which means that all of this sphinx-specific stuff probably doesn‚Äôt work anymore :-)\n\nIf you want to see a version of this page that worked, \n\ncheck out this file in GitHub.","type":"content","url":"/blog/2020/sphinx-design-timeline","position":1},{"hierarchy":{"lvl1":"Serving in two roles at once via pre-recorded tutorials"},"type":"lvl1","url":"/blog/2021/2021-12-18-hybrid-tutorial-prerecord","position":0},{"hierarchy":{"lvl1":"Serving in two roles at once via pre-recorded tutorials"},"content":"At AGU 2021 this year I was asked to give \n\na short tutorial introduction to Jupyter Book.\nThe tutorial was 30 minutes long, and the session was fully remote.\n\nThis posed a few challenges:\n\nTutorials almost always go over time - particularly if you‚Äôre taking questions from attendees.\n\nIt is tricky to go back and forth between lecture-style talking and going through steps yourself to make sure that you‚Äôre not out-pacing the attendees.\n\nMy time working with \n\nthe Carpentries taught me that having helpers in a tutorial is extremely useful to keep things on track.\n\nSo, I decided to try an expriment this time: I‚Äôd pre-record my tutorial via Zoom, and then attend the session as a helper. The rest of this post is about my experience!\n\nIf you‚Äôd like to check out the tutorial\n\nHere‚Äôs a video of the tutorial in case you‚Äôre interested!\n\nYouTube video player","type":"content","url":"/blog/2021/2021-12-18-hybrid-tutorial-prerecord","position":1},{"hierarchy":{"lvl1":"Serving in two roles at once via pre-recorded tutorials","lvl2":"An experiment: record myself ahead of time"},"type":"lvl2","url":"/blog/2021/2021-12-18-hybrid-tutorial-prerecord#an-experiment-record-myself-ahead-of-time","position":2},{"hierarchy":{"lvl1":"Serving in two roles at once via pre-recorded tutorials","lvl2":"An experiment: record myself ahead of time"},"content":"Here‚Äôs what I did:","type":"content","url":"/blog/2021/2021-12-18-hybrid-tutorial-prerecord#an-experiment-record-myself-ahead-of-time","position":3},{"hierarchy":{"lvl1":"Serving in two roles at once via pre-recorded tutorials","lvl3":"Before the session","lvl2":"An experiment: record myself ahead of time"},"type":"lvl3","url":"/blog/2021/2021-12-18-hybrid-tutorial-prerecord#before-the-session","position":4},{"hierarchy":{"lvl1":"Serving in two roles at once via pre-recorded tutorials","lvl3":"Before the session","lvl2":"An experiment: record myself ahead of time"},"content":"Opened two windows on my screen on top of each other: one window was a Google Slides window with my presentation, the other was a JupyterLab window that I used to demonstrate all of the steps.\n\nHere‚Äôs how it looked:\n\n\n\nMy two-window setup in recording the tutorial.\n\nIn my slides, I included several prompts with explicit text for people to type into their own terminals and text files. I followed the instructions in these slides and typed them into the JupyterLab window myself.\n\n\n\nInstructions on the top, real-time results on the bottom.\n\nI used Zoom to record myself as I clicked through the slides on the top, and followed along on the bottom as if I were a participant myself.\n\nI split the recording into roughly 5 minute sections.\nI‚Äôd stop the recording briefly for each section, take a quick breather, and then move on to recording the next. This ensured that I had a moment to collect myself and if I messed up a section, I‚Äôd only have to re-record 5 minutes instead of 30.\n\nAfter the recording, I stitched together each of the sections and did a simple fade-out / fade-in using the built-in \n\nWindows Video Editor.","type":"content","url":"/blog/2021/2021-12-18-hybrid-tutorial-prerecord#before-the-session","position":5},{"hierarchy":{"lvl1":"Serving in two roles at once via pre-recorded tutorials","lvl3":"During the session","lvl2":"An experiment: record myself ahead of time"},"type":"lvl3","url":"/blog/2021/2021-12-18-hybrid-tutorial-prerecord#during-the-session","position":6},{"hierarchy":{"lvl1":"Serving in two roles at once via pre-recorded tutorials","lvl3":"During the session","lvl2":"An experiment: record myself ahead of time"},"content":"During the tutorial, I played the pre-recorded video as the ‚Äúpresenter‚Äù, but spent my time answering questions and addressing issues via the chat box.\nAttendees would watch the pre-recorded video, and speak with me in the chat if anything came up.\nThis meant that I could address questions in real-time but without breaking up the flow of the tutorial.\nIf there was a really important question that was worth discussing as a group, I could have stopped the video to bring it up with everyone.","type":"content","url":"/blog/2021/2021-12-18-hybrid-tutorial-prerecord#during-the-session","position":7},{"hierarchy":{"lvl1":"Serving in two roles at once via pre-recorded tutorials","lvl2":"Thoughts"},"type":"lvl2","url":"/blog/2021/2021-12-18-hybrid-tutorial-prerecord#thoughts","position":8},{"hierarchy":{"lvl1":"Serving in two roles at once via pre-recorded tutorials","lvl2":"Thoughts"},"content":"In all, this worked really nicely!\nThe biggest downside is that it took a bit of extra time to pre-record the video before the talk itself.\nHowever, I believe I might have actually saved time, because pre-recording the talk meant that I didn‚Äôt feel the need to tinker with my slides in the hours leading up to the talk.\n\nHere are some of the benefits that I noticed:\n\nI could serve as presenter and assistant in one talk - two roles with one person!\n\nIt gave me more control over the talk length and pacing, because I knew how much total time was there.\n\nBy recording it ahead of time, I was ‚Äúdone‚Äù with the talk much earlier than I normally am (usually I‚Äôm stressing about slides until a few moments before the talk)\n\nIt also meant I had less anxiety before the event itself.\n\nI‚Äôll give this another shot the next time I‚Äôm giving a remote tutorial (or maybe an in-person tutorial by myself).\nIn the meantime, I wanted to write up these thoughts while they were fresh in case this might work for somebody else.","type":"content","url":"/blog/2021/2021-12-18-hybrid-tutorial-prerecord#thoughts","position":9},{"hierarchy":{"lvl1":"Ask Twitter: Why don‚Äôt academic researchers use cloud services?"},"type":"lvl1","url":"/blog/2022/cloud-services-academia","position":0},{"hierarchy":{"lvl1":"Ask Twitter: Why don‚Äôt academic researchers use cloud services?"},"content":"this is an experiment at making my \n\nTwitter conversations a bit more useful and archivable over time. It‚Äôs going to be a bit messy and unpolished, but hopefully that makes it more likely I‚Äôll actually do it :-)\n\nOver the past decade, cloud infrastructure has become increasingly popular in industry.\nAn ecosystem of modular tools and cloud services (often called \n\nthe Modern Data Stack) has filled many data needs for companies.\n\nHowever, academic research and education still largely does not utilize this stack.\nInstead, they optimize for local workflows or shared infrastructure that exists on-premise.\nIf you believe (as I do) that cloud infrastructure has the potential to help people do work more effectively and collaboratively, then it‚Äôs important to understand why people don‚Äôt use these kinds of tools.\n\nSo, I decided to ask Twitter why academics tend to not utilize cloud infrastructure:\n\nhttps://‚Äãtwitter‚Äã.com‚Äã/choldgraf‚Äã/status‚Äã/1564614538309390345\n\nBelow is a brief summary of the major points that several people made.","type":"content","url":"/blog/2022/cloud-services-academia","position":1},{"hierarchy":{"lvl1":"Ask Twitter: Why don‚Äôt academic researchers use cloud services?","lvl2":"Fear of high costs"},"type":"lvl2","url":"/blog/2022/cloud-services-academia#fear-of-high-costs","position":2},{"hierarchy":{"lvl1":"Ask Twitter: Why don‚Äôt academic researchers use cloud services?","lvl2":"Fear of high costs"},"content":"The most common challenge is the fear of over-running cloud costs.\nWith on-prem infrastructure, you pay for allotments of time up-front (or don‚Äôt pay at all).\nCloud infrastructure allows you to ‚Äúpay as you go‚Äù, but this sometimes means over-paying if you used resources you didn‚Äôt expect:\n\nhttps://‚Äãtwitter‚Äã.com‚Äã/anshulkundaje‚Äã/status‚Äã/1551585264262295552\n\nCost monitoring needs to be a more obvious part of cloud services.\nFortunately, most cloud providers have their own budgeting and cost monitoring services to prevent this from happening.\nHowever, these don‚Äôt seem to be well-understood or utilized.\nThat brings us to the next concern:","type":"content","url":"/blog/2022/cloud-services-academia#fear-of-high-costs","position":3},{"hierarchy":{"lvl1":"Ask Twitter: Why don‚Äôt academic researchers use cloud services?","lvl2":"A confusing landscape of tools and services"},"type":"lvl2","url":"/blog/2022/cloud-services-academia#a-confusing-landscape-of-tools-and-services","position":4},{"hierarchy":{"lvl1":"Ask Twitter: Why don‚Äôt academic researchers use cloud services?","lvl2":"A confusing landscape of tools and services"},"content":"There are hundreds of cloud services for data workflows in existence.\nAll of them make one thing a little bit easier, but you still need knowledge to:\n\nChoose the right cloud services\n\nIntegrate them together\n\nhttps://‚Äãtwitter‚Äã.com‚Äã/cboettig‚Äã/status‚Äã/1564671199547838464\n\nAs a result, many cloud services that might be useful are effectively not used because they get lost in all the noise out there.\n\nIn short, researchers need support in navigating this space.\nThey need organizations to understand their workflows and recommend a few specific things to use instead of exposing them to the hundreds of options out there.","type":"content","url":"/blog/2022/cloud-services-academia#a-confusing-landscape-of-tools-and-services","position":5},{"hierarchy":{"lvl1":"Ask Twitter: Why don‚Äôt academic researchers use cloud services?","lvl2":"High start-up costs and no DevOps experience"},"type":"lvl2","url":"/blog/2022/cloud-services-academia#high-start-up-costs-and-no-devops-experience","position":6},{"hierarchy":{"lvl1":"Ask Twitter: Why don‚Äôt academic researchers use cloud services?","lvl2":"High start-up costs and no DevOps experience"},"content":"Even if you do find the set of services that you want, you need some way to expose these services to your collaborators or research group.\nYou also need to manage this integration over time as things change, break, etc.\nUniversity groups often do not have dedicated cloud expertise, and so this lands at the feet of post-docs and graduate students with energy and interest to try things out.\n\nhttps://‚Äãtwitter‚Äã.com‚Äã/ixek‚Äã/status‚Äã/1565645082275057664\n\nHowever, asking a student to learn and run your cloud infrastructure is also a risky move.\nStudents move on, and DevOps skills are in high demand.\nWhat happens if you lose that person, or if your grant runs out?\n\nResearchers need others to manage cloud infrastructure for them.\nIntegrating and using most services still requires expertise similar to a systems administrator (but now with cloud infrastructure as well).\nWe need more services that manage this complexity for them.","type":"content","url":"/blog/2022/cloud-services-academia#high-start-up-costs-and-no-devops-experience","position":7},{"hierarchy":{"lvl1":"Ask Twitter: Why don‚Äôt academic researchers use cloud services?","lvl2":"Pain to first compute is higher for cloud"},"type":"lvl2","url":"/blog/2022/cloud-services-academia#pain-to-first-compute-is-higher-for-cloud","position":8},{"hierarchy":{"lvl1":"Ask Twitter: Why don‚Äôt academic researchers use cloud services?","lvl2":"Pain to first compute is higher for cloud"},"content":"Universities recognize that managed services are useful to research - they just prioritize on-premises infrastructure over everything else.\nFor example, many people noted that HPC is more heavily used simply because the university puts most of their resources into easing the use of this infrastructure.\n\nhttps://‚Äãtwitter‚Äã.com‚Äã/SpectralFilter‚Äã/status‚Äã/1564697622660763648\n\nUniversities need to invest in lowering the energy barrier to cloud services as well as on-prem infrastructure. We don‚Äôt want universities to divest from all of their on-prem hardware and services, but we should make at least marginal investments in similarly reducing the barriers to using cloud infrastructure. What would it look like if universities made it as easy to set up and pay for cloud infrastructure as they do for HPC? I bet a lot more people would experiment and learn with the cloud.","type":"content","url":"/blog/2022/cloud-services-academia#pain-to-first-compute-is-higher-for-cloud","position":9},{"hierarchy":{"lvl1":"Ask Twitter: Why don‚Äôt academic researchers use cloud services?","lvl2":"Continuity of service / lock-in concerns"},"type":"lvl2","url":"/blog/2022/cloud-services-academia#continuity-of-service-lock-in-concerns","position":10},{"hierarchy":{"lvl1":"Ask Twitter: Why don‚Äôt academic researchers use cloud services?","lvl2":"Continuity of service / lock-in concerns"},"content":"When you‚Äôre relying on a service exposed by some other organization, you must hope that the service continues to be useful.\nMany academics have been burned when services discontinue.\n\nThis might happen for a number of reasons:\n\nAn internal service discontinues because the organization no-longer has capacity to manage it themselves. This is particularly challenging in the context of research funding, which tends to be ‚Äúboom or bust‚Äù.\nYou get a grant and are well-funded for 3 years.\nAfter it runs out, maybe you‚Äôve got a few months of a gap without dedicated resources for infrastructure.\nWhat happens if your DevOps person‚Äôs salary depends on that grant?\n\nAn external service discontinues because the organization running it pivoted to a new model or cost structure. This is challenging because many cloud services are designed for enterprise, the research community. They are also not accountible to the research community, and do not give it direct representation or governing power over the direction of the services.\n\nMoreover, when our workflows are not easily portable, it creates extra switching cost and strain on a researcher‚Äôs workflow.\nMany services (both external and internal) do not properly leverage pre-existing and modular tools to make it easy to switch (and why would they, if they‚Äôre optimizing for their own growth over the growth of the ecosystem).\n\nResearchers need cost-sharing mechanisms for cloud infrastructure that is designed for them.\nThis would allow them to pool their resources and ensure continuity and quality of service without relying on boom and bust cycles of single grants.\nIt would also allow them to support reliable and sustainable services that are designed for the unique problems that the research community has (like being a huge, globally-distributed, heterogeneous community with almost no top-down control and thousands of different workflows).","type":"content","url":"/blog/2022/cloud-services-academia#continuity-of-service-lock-in-concerns","position":11},{"hierarchy":{"lvl1":"Ask Twitter: Why don‚Äôt academic researchers use cloud services?","lvl2":"Wrap-up: we need more cloud-native services for research"},"type":"lvl2","url":"/blog/2022/cloud-services-academia#wrap-up-we-need-more-cloud-native-services-for-research","position":12},{"hierarchy":{"lvl1":"Ask Twitter: Why don‚Äôt academic researchers use cloud services?","lvl2":"Wrap-up: we need more cloud-native services for research"},"content":"Others shared a few examples of successful cloud services.\nFor example, the \n\nCyVerse project was largely seen as helpful to academics, though it depended initially on NSF funding to support it and so did not develop a self-sustaining cost recovery model.\nOther projects like \n\nPangeo have shown the value that hosted infrastructure can bring to distributed communities that wish to standardize on similar tools, workflows, and data.\n\nUltimately, it sounds like what we need are a combination of two things:\n\nMore organizations that are dedicated to serving cloud infrastructure to the research community. These should experiment with various models of service delivery as well as cost recovery to provide stable and reliable services for many stakeholders.\n\nMore bureaucratic innovation at easing cloud workflows in large research organizations. These should reduce the artificial barriers to using and paying for cloud infrastructure, to make it easier for researchers to experiment with the cloud and learn how it can be most useful for their work.\n\nI hope that \n\n2i2c can help be a part of the solution here - I think there‚Äôs a lot of potential for impact!\n\nA few quick ideas: central funding from universities, multi-year grant allotments from funding agencies, asking users to pay the cost of running the service for them, asking organizations to pay for infrastructure onbehalf of their users, philanthropic gifts to cover costs on behalf of a group of users. Really it‚Äôll need to be a subset of all of these things.\n\nApparently there‚Äôs also \n\na great landscape analysis from Ithaka that I have yet to read but it looks promising.","type":"content","url":"/blog/2022/cloud-services-academia#wrap-up-we-need-more-cloud-native-services-for-research","position":13},{"hierarchy":{"lvl1":"GitHub year in review"},"type":"lvl1","url":"/blog/2022/github-2022","position":0},{"hierarchy":{"lvl1":"GitHub year in review"},"content":"TODO: Add a count of number of words written by month and total\n\nhttps://‚Äãdocs‚Äã.github‚Äã.com‚Äã/en‚Äã/graphql‚Äã/overview‚Äã/explorerfrom github_activity.graphql import GitHubGraphQlQuery\nimport os\nfrom rich import print\nfrom IPython.display import Markdown\nimport pandas as pd\nimport altair as alt\nfrom itertools import productdates = [\n    \"2022-01-01..2022-06-01\",\n    \"2022-06-01..2022-12-31\",\n]\nkinds = [\"author\", \"commenter\"]\nusername = \"choldgraf\"\ndata = []\nfor date, kind in product(dates, kinds):\n    query = f\"{kind}:{username} updated:{date} is:issue\"\n    resp = GitHubGraphQlQuery(query=query, auth=os.environ[\"GITHUB_TOKEN\"])\n    resp.request()\n    data.append(resp.data)issues = pd.concat(data)\norig = issues.shape[0]\nissues = issues.drop_duplicates(subset=\"id\")\nnew = issues.shape[0]\nMarkdown(f\"Dropped **{orig-new}** duplicate issues...\")# Extract comments into a standalone df\ncomments = []\nfor ix, row in issues.iterrows():\n    # Extract comments\n    for edge in row[\"comments\"][\"edges\"]:\n        comment = edge[\"node\"]\n        comment[\"issue_id\"] = row[\"id\"]\n        comment[\"repo\"] = row[\"repo\"]\n        comment[\"org\"] = row[\"org\"]\n        comments.append(comment)\ncomments = pd.DataFrame(comments)\n\n# Convert date fields into datetime objects\ndate_fields = [\"updatedAt\", \"createdAt\", \"closedAt\"]\nfor field in date_fields:\n    if field in comments.columns:\n        comments[field] = pd.to_datetime(comments[field])\n    if field in issues.columns:\n        issues[field] = pd.to_datetime(issues[field])\n    \n# Only include comments from this year\ncomments = comments[comments[\"updatedAt\"] > \"2022-01-01\"]","type":"content","url":"/blog/2022/github-2022","position":1},{"hierarchy":{"lvl1":"GitHub year in review","lvl2":"Issues created"},"type":"lvl2","url":"/blog/2022/github-2022#issues-created","position":2},{"hierarchy":{"lvl1":"GitHub year in review","lvl2":"Issues created"},"content":"opened = issues[issues[\"createdAt\"] > \"2022-01-01\"]\nclosed = issues[issues[\"closedAt\"] > \"2022-01-01\"]","type":"content","url":"/blog/2022/github-2022#issues-created","position":3},{"hierarchy":{"lvl1":"GitHub year in review","lvl3":"Over time","lvl2":"Issues created"},"type":"lvl3","url":"/blog/2022/github-2022#over-time","position":4},{"hierarchy":{"lvl1":"GitHub year in review","lvl3":"Over time","lvl2":"Issues created"},"content":"weekly = opened.groupby([\"org\"]).resample(\"M\", on=\"createdAt\").count()[\"state\"].reset_index()\ntop_orgs = weekly.groupby(\"org\").sum().sort_values(\"state\", ascending=False).head(10).index.values\nweekly = weekly.query(\"org in @top_orgs\")\nalt.Chart(weekly, width=500).mark_bar(width=25).encode(\n    x=\"createdAt\",\n    y=\"state\",\n    color=\"org\",\n    order=alt.Order('sum(state):Q', sort='descending'),\n    tooltip=[\"state\", \"org\"],\n).interactive()","type":"content","url":"/blog/2022/github-2022#over-time","position":5},{"hierarchy":{"lvl1":"GitHub year in review","lvl3":"By repository","lvl2":"Issues created"},"type":"lvl3","url":"/blog/2022/github-2022#by-repository","position":6},{"hierarchy":{"lvl1":"GitHub year in review","lvl3":"By repository","lvl2":"Issues created"},"content":"charts = []\nfor kind, name in [(opened, \"opened\"), (closed, \"closed\")]:\n    counts = kind.groupby([\"org\", \"repo\"]).agg({\"state\": \"count\"}).rename(columns={\"state\": \"count\"})\n    plt_counts = counts.reset_index().sort_values(\"count\", ascending=False).head(20)\n    plt_counts[\"orgrepo\"] = plt_counts.apply(lambda a: f\"{a['org']}/{a['repo']}\", axis=1)\n    ch = alt.Chart(plt_counts, title=f\"{name} issues in 2022\").mark_bar().encode(\n        x=alt.X(\"orgrepo\", sort=alt.SortField(\"count\", order=\"descending\")),\n        y=\"count\",\n        tooltip=[\"org\", \"repo\", \"count\"],\n        color=\"org\",\n    ).interactive()\n    charts.append(ch)\ncharts[0] & charts[1]","type":"content","url":"/blog/2022/github-2022#by-repository","position":7},{"hierarchy":{"lvl1":"GitHub year in review","lvl3":"By activity","lvl2":"Issues created"},"type":"lvl3","url":"/blog/2022/github-2022#by-activity","position":8},{"hierarchy":{"lvl1":"GitHub year in review","lvl3":"By activity","lvl2":"Issues created"},"content":"Most commentsmy_issues = opened.query(\"author=='choldgraf'\").copy()\nmy_issues[\"ncomments\"] = my_issues[\"comments\"].map(lambda a: len(a[\"edges\"]))\nmy_issues.sort_values(\"ncomments\", ascending=False).query(\"ncomments > 5\")\n\nMost thumbs upopened.query(\"author=='choldgraf'\").sort_values([\"thumbsup\"], ascending=False)","type":"content","url":"/blog/2022/github-2022#by-activity","position":9},{"hierarchy":{"lvl1":"GitHub year in review","lvl2":"GitHub Comments"},"type":"lvl2","url":"/blog/2022/github-2022#github-comments","position":10},{"hierarchy":{"lvl1":"GitHub year in review","lvl2":"GitHub Comments"},"content":"Markdown(f\"Total comments: **{comments.shape[0]}**\")counts = comments.groupby([\"org\", \"repo\"]).agg({\"url\": \"count\"}).rename(columns={\"url\": \"count\"}).reset_index()countstop_orgs = counts.groupby(\"org\").sum().sort_values(\"count\", ascending=False).head(10).index.values\nplt_counts = counts.query(\"org in @top_orgs\")\nalt.Chart(plt_counts, width=500).mark_bar(width=25).encode(\n    x=alt.X(\"org\", sort=alt.SortField(\"count\")),\n    y=\"count\",\n    color=\"org\",\n    tooltip=[\"count\", \"org\"],\n).interactive()","type":"content","url":"/blog/2022/github-2022#github-comments","position":11},{"hierarchy":{"lvl1":"Install dependencies from GitHub with pyproject.toml or requirements.txt"},"type":"lvl1","url":"/blog/2022/install-github-from-pyproject","position":0},{"hierarchy":{"lvl1":"Install dependencies from GitHub with pyproject.toml or requirements.txt"},"content":"This is a short post to demonstrate how to install packages directly from GitHub with pyprojects.toml or requirements.txt, including custom branches and commits.\nIt will focus on pyprojects.toml because this is newer and there‚Äôs less information about it, but the general pattern holds for requirements.txt as well.\n\nIn pyproject.toml, you can specify dependencies for a project via the dependencies field.\nFor example, to specify \n\nSphinx as a dependency:\n\ndependencies = [\n  \"sphinx\",\n]\n\npyproject.toml\n\nHowever, this will install the version that is published to [PyPI](Here‚Äôs how to install a specific branch in pyproject.toml:\n).\nWhat if you want to install from @main, or from a specific commit or branch?\n\nTo do so, use a pattern like this:\n\ndependencies = [\n  \"<packagename>@git+<url-to-repo>#egg=<branch or hash>\",\n]\n\npyproject.toml\n\nHere are a few recipes for doing this:","type":"content","url":"/blog/2022/install-github-from-pyproject","position":1},{"hierarchy":{"lvl1":"Install dependencies from GitHub with pyproject.toml or requirements.txt","lvl2":"Install directly from GitHub"},"type":"lvl2","url":"/blog/2022/install-github-from-pyproject#install-directly-from-github","position":2},{"hierarchy":{"lvl1":"Install dependencies from GitHub with pyproject.toml or requirements.txt","lvl2":"Install directly from GitHub"},"content":"dependencies = [\n  \"sphinx@git+https://github.com/sphinx-doc/sphinx\",\n]\n\npyproject.toml","type":"content","url":"/blog/2022/install-github-from-pyproject#install-directly-from-github","position":3},{"hierarchy":{"lvl1":"Install dependencies from GitHub with pyproject.toml or requirements.txt","lvl2":"Install from a specific branch"},"type":"lvl2","url":"/blog/2022/install-github-from-pyproject#install-from-a-specific-branch","position":4},{"hierarchy":{"lvl1":"Install dependencies from GitHub with pyproject.toml or requirements.txt","lvl2":"Install from a specific branch"},"content":"dependencies = [\n  \"sphinx@git+https://github.com/sphinx-doc/sphinx#egg=branchname\",\n]\n\npyproject.toml","type":"content","url":"/blog/2022/install-github-from-pyproject#install-from-a-specific-branch","position":5},{"hierarchy":{"lvl1":"Install dependencies from GitHub with pyproject.toml or requirements.txt","lvl2":"With requirements.txt"},"type":"lvl2","url":"/blog/2022/install-github-from-pyproject#with-requirements-txt","position":6},{"hierarchy":{"lvl1":"Install dependencies from GitHub with pyproject.toml or requirements.txt","lvl2":"With requirements.txt"},"content":"Using these patterns with requirements.txt is nearly the same, but it‚Äôs a bit simpler.\nYou don‚Äôt need to specify the packagename@ pattern used above.\nYou can simply add a line that points to git like this:\n\nrequirement1\nrequirement2\ngit+https://github.com/pydata/pydata-sphinx-theme\n\nrequirements.txt\n\nThis will install from the default branch.\nYou can also specify specific branches or eggs in the same way as above.\n\nThanks to \n\nMatthew Feickhart for reminding me of this.","type":"content","url":"/blog/2022/install-github-from-pyproject#with-requirements-txt","position":7},{"hierarchy":{"lvl1":"Report from the JupyterLite workshop: WebAssembly is pretty cool"},"type":"lvl1","url":"/blog/2022/jupyterlite-workshop","position":0},{"hierarchy":{"lvl1":"Report from the JupyterLite workshop: WebAssembly is pretty cool"},"content":"I recently attended \n\nthe JupyterLite community workshop in Paris, here are some quick thoughts from the three-day event.\n\nFor those without any background, JupyterLite is a distribution of Jupyter‚Äôs user interfaces and a Python kernel that runs entirely in the browser.\nIts goal is to provide a low-overhead and accessible way to use a Jupyter interface via the browser.\nSee \n\nthe jupyterlite documentation for more information.","type":"content","url":"/blog/2022/jupyterlite-workshop","position":1},{"hierarchy":{"lvl1":"Report from the JupyterLite workshop: WebAssembly is pretty cool","lvl2":"Capytale shows that WebAssembly and JupyterLite can boost accessibility to interactive computation"},"type":"lvl2","url":"/blog/2022/jupyterlite-workshop#capytale-shows-that-webassembly-and-jupyterlite-can-boost-accessibility-to-interactive-computation","position":2},{"hierarchy":{"lvl1":"Report from the JupyterLite workshop: WebAssembly is pretty cool","lvl2":"Capytale shows that WebAssembly and JupyterLite can boost accessibility to interactive computation"},"content":"We had a demonstration of \n\nthe capytale platform, a project run by the French educational system to provide remote access to fully in-browser learning environments.\n\nCapytale provides a number of learning modules.\nMany of them are JavaScript-based, but they also needed to provide Python as well.\nInitially, they tried to run a server that launched Python sessions for every person that used their service, but found this to be unscalable.\n\nInstead, the began to use \n\nthe notebook platform basthon.fr.\nThis re-uses the Classic Jupyter Notebook interface and connects to \n\na pyodide kernel.\nThis means that all of the learner‚Äôs work is done in their own browser.\n\nAs a result, they are able to run nearly 60,000 notebook sessions a week, while running only a very lightweight server.\nBecause the computation is done purely in the browser, they only need to run a server for authentication and for sending files to the user sessions (files for learning modules are stored in a shared database that they manage).\n\nThis was a really inspiring example to me, because it‚Äôs clear that for basic and introductory learning, WebAssembly can significantly reduce the barrier to learning and the overhead of managing shared infrastructure for learning.\nI hope to see more experiments like this in the future, and would love to find ways that we can build WebAssembly workflows into JupyterHub and Binder.","type":"content","url":"/blog/2022/jupyterlite-workshop#capytale-shows-that-webassembly-and-jupyterlite-can-boost-accessibility-to-interactive-computation","position":3},{"hierarchy":{"lvl1":"Report from the JupyterLite workshop: WebAssembly is pretty cool","lvl2":"JupyterLite should fit on a USB stick for low-internet communities"},"type":"lvl2","url":"/blog/2022/jupyterlite-workshop#jupyterlite-should-fit-on-a-usb-stick-for-low-internet-communities","position":4},{"hierarchy":{"lvl1":"Report from the JupyterLite workshop: WebAssembly is pretty cool","lvl2":"JupyterLite should fit on a USB stick for low-internet communities"},"content":"One problem with the WebAssembly / JupyterLite approach is that it requires you to download a pretty big bundle the first time you access some content (something like 16mb at the smallest).\nThis isn‚Äôt too bad, but it‚Äôs still pretty large if you live in a part of the world with really bad internet.\n\nI had a quick chat with Jeremy about this, and we had an idea to bundle JupyterLite in a USB stick.\nIn many parts of the world, it might be less work to put JupyterLite on a USB and mail it to them than to ask them to download it themselves.\nI \n\nopened up an idea post in the community forum to share and discuss.","type":"content","url":"/blog/2022/jupyterlite-workshop#jupyterlite-should-fit-on-a-usb-stick-for-low-internet-communities","position":5},{"hierarchy":{"lvl1":"Report from the JupyterLite workshop: WebAssembly is pretty cool","lvl2":"WebAssembly and JupyterLite is coming to Jupyter Book"},"type":"lvl2","url":"/blog/2022/jupyterlite-workshop#webassembly-and-jupyterlite-is-coming-to-jupyter-book","position":6},{"hierarchy":{"lvl1":"Report from the JupyterLite workshop: WebAssembly is pretty cool","lvl2":"WebAssembly and JupyterLite is coming to Jupyter Book"},"content":"We made progress on a bunch of places where Jupyter Book can leverage the WebAssembly / JupyterLite ecosystem.\nIn particular, Steve has been doing \n\ngreat work on thebe-lite, which will allow Thebe to launch interactive kernels via JupyterLite.\nThis means you‚Äôll be able to make code cells on a static webpage interactive purely in the browser.\n\nI also re-discovered \n\nthe jupyterlite-sphinx extension, which (relatively) easily packages JupyterLite for sharing with a Sphinx site.\n\nThere are many things to improve here, but I think we‚Äôre making good progress.","type":"content","url":"/blog/2022/jupyterlite-workshop#webassembly-and-jupyterlite-is-coming-to-jupyter-book","position":7},{"hierarchy":{"lvl1":"Report from the JupyterLite workshop: WebAssembly is pretty cool","lvl2":"WebAssembly still can‚Äôt work for many workflows"},"type":"lvl2","url":"/blog/2022/jupyterlite-workshop#webassembly-still-cant-work-for-many-workflows","position":8},{"hierarchy":{"lvl1":"Report from the JupyterLite workshop: WebAssembly is pretty cool","lvl2":"WebAssembly still can‚Äôt work for many workflows"},"content":"One final thing that I noted is that WebAssembly still trips over itself in many somewhat common situations in computation.\nI had a number of conversations with teachers that mentioned there are basic things you want to teach, but cannot with pyodide.\nFor example, things like using the input or the sleep modules in Python, or downloading files from the web.\n\nWhile I suspect that most teachers will be able to work around this, and that the pyodide ecosystem will continue to patch over these paper cuts, it seems like for the forseeable future, there will be a subset of use-cases where you simply need a live Python server.\nI think our challenge as technologists will be to figure out the right User Experience around moving between an in-browser pyodide workflow, a cloud-based python server workflow, and a local installation workflow.\nLearnings will need to use all of these approaches (particularly as they move beyond introductory courses), and we need to make sure that we have a similar set of technology and servies that can meet each need without requiring a whole new workflow.\n\nMany thanks to the \n\nQuantStack team for organizing this event, and to \n\nOVHCloud for providing a physical space for everyone.\n\nAs an aside, I am continually impressed with France‚Äôs innovation around technology for teaching and learning. There seems to be a lot of experimentation and trying things out with open technology and services, and lots of clever ideas come out of this. It‚Äôs exciting to see people experimenting in this way instead of using vendored SaaS products (though I‚Äôm sure there‚Äôs plenty of that too).","type":"content","url":"/blog/2022/jupyterlite-workshop#webassembly-still-cant-work-for-many-workflows","position":9},{"hierarchy":{"lvl1":"Load and plot a remote font with Matplotlib"},"type":"lvl1","url":"/blog/2022/matplotlib-remote-font","position":0},{"hierarchy":{"lvl1":"Load and plot a remote font with Matplotlib"},"content":"As part of \n\nmy sphinx-social-previews prototype, I wanted to be able to use the \n\nRoboto Font from Google in image previews.\nHowever, Roboto is often not loaded on your local filesystem, so it took some digging to figure out how to make it possible to load via \n\nMatplotlib‚Äôs text plotting functionality.\n\nHere‚Äôs the solution that finally worked for me, inspired \n\nfrom this Tweet with a similar implementation from \n\nthe dmol book.\n\nBelow I‚Äôll use \n\nthe Fira Code font from Mozilla but you could do this with any open font that you have the .ttf file for.","type":"content","url":"/blog/2022/matplotlib-remote-font","position":1},{"hierarchy":{"lvl1":"Load and plot a remote font with Matplotlib","lvl2":"Create a temporary folder and download the font file"},"type":"lvl2","url":"/blog/2022/matplotlib-remote-font#create-a-temporary-folder-and-download-the-font-file","position":2},{"hierarchy":{"lvl1":"Load and plot a remote font with Matplotlib","lvl2":"Create a temporary folder and download the font file"},"content":"I‚Äôll show how to do this with a local folder, so it‚Äôll re-download each time you run the code.\nAlternatively you could download the font locally and just register the path instead of downloading it:\n\nFirst, download the .ttf of the font locally via urlretrieve.\nFor example, the Fira Code font from Mozilla is located \n\nat this URL.\nTo download it locally into a temporary folder using \n\nthe tempfile.mkdtemp function, run this code:\n\nimport tempfile\nfrom pathlib import Path\nimport urllib\nfrom rich import print\n\n# Create a temporary directory for the font file\npath = Path(tempfile.mkdtemp())\n\n# URL and downloaded path of the font\nurl_font = \"https://github.com/google/fonts/raw/main/ofl/firacode/FiraCode%5Bwght%5D.ttf\"\npath_font = path / \"Fira-Code.ttf\"\n\n\n# Download the font to our temporary directory\nurllib.request.urlretrieve(url_font, path_font)\n\n","type":"content","url":"/blog/2022/matplotlib-remote-font#create-a-temporary-folder-and-download-the-font-file","position":3},{"hierarchy":{"lvl1":"Load and plot a remote font with Matplotlib","lvl2":"Link the font with the font_manager API"},"type":"lvl2","url":"/blog/2022/matplotlib-remote-font#link-the-font-with-the-font-manager-api","position":4},{"hierarchy":{"lvl1":"Load and plot a remote font with Matplotlib","lvl2":"Link the font with the font_manager API"},"content":"\n\nNow that the file is loaded, we can link it using \n\nMatplotlib‚Äôs font_manager API.\nThis allows you to register fonts that Matplotlib knows how to use.\n\nfrom matplotlib import font_manager\n\n# Create a Matplotlib Font object from our `.ttf` file\nfont = font_manager.FontEntry(fname=str(path_font), name=\"Fira-Downloaded\")\n\n# Register this object with Matplotlib's ttf list\nfont_manager.fontManager.ttflist.append(font)\n# Print the last few items to see what they look like\nprint(font_manager.fontManager.ttflist[-3:])\n\nTip\n\nTo list all of the fonts that Matplotlib knows how to plot, inspect the list matplotlib.font_manager.fontManager.ttflist.\n\n","type":"content","url":"/blog/2022/matplotlib-remote-font#link-the-font-with-the-font-manager-api","position":5},{"hierarchy":{"lvl1":"Load and plot a remote font with Matplotlib","lvl2":"Make a plot with this font"},"type":"lvl2","url":"/blog/2022/matplotlib-remote-font#make-a-plot-with-this-font","position":6},{"hierarchy":{"lvl1":"Load and plot a remote font with Matplotlib","lvl2":"Make a plot with this font"},"content":"Now that we‚Äôve registered the font, we can plot it with ax.text.\nTo do so, use the plt.rc_context context manager to temporarily update our default font:\n\nimport matplotlib.pyplot as plt\n\nfig, axs = plt.subplots(1, 3, figsize=(9, 5))\n\n# This will now render as a `Roboto Flex` font\naxs[0].text(.1, .5, \"The default font!\", fontdict={\"fontsize\": 15})\naxs[1].text(.1, .5, \"A Matplotlib font\", fontdict={\"fontsize\": 15, \"family\": \"cmmi10\"})\naxs[2].text(.1, .5, \"The Fira Code font!\", fontdict={\"fontsize\": 15, \"family\": font.name})\n\nfor ax in axs:\n  ax.set_axis_off()\n\nAnd that‚Äôs it!","type":"content","url":"/blog/2022/matplotlib-remote-font#make-a-plot-with-this-font","position":7},{"hierarchy":{"lvl1":"Automatically updating my publications page with ORCID and doi.org"},"type":"lvl1","url":"/blog/2022/orcid-auto-update","position":0},{"hierarchy":{"lvl1":"Automatically updating my publications page with ORCID and doi.org"},"content":"For a while I‚Äôve had a hand-crafted .bibtex file stored locally for \n\nmy publications/ page.\nHowever, manually updating local text file is a pain to remember, especially since there are many services out there that automatically track new publications.\n\nUpdate!\n\nA \n\nhelpful suggestion on Twitter allowed me to include the full citation information, including lists of authors, using the \n\ndoi.org API!\n\nHere‚Äôs the workflow I‚Äôd prefer:\n\nTreat one online service provider as a Single Source of Truth for my publications list.\n\nUse an API to programmatically ask this provider for the latest data about my publications.\n\nReshape that data into a form that I can insert into my website.\n\nHere‚Äôs how I accomplished this:","type":"content","url":"/blog/2022/orcid-auto-update","position":1},{"hierarchy":{"lvl1":"Automatically updating my publications page with ORCID and doi.org","lvl2":"Use ORCID to grab a list of DOIs for my publications"},"type":"lvl2","url":"/blog/2022/orcid-auto-update#use-orcid-to-grab-a-list-of-dois-for-my-publications","position":2},{"hierarchy":{"lvl1":"Automatically updating my publications page with ORCID and doi.org","lvl2":"Use ORCID to grab a list of DOIs for my publications"},"content":"ORCID is a service for identifying scholars and their contributions.\nIt links various kinds of publications and activities to a unique account for each person.\nIt doesn‚Äôt cover all kinds of outputs (like talks, posters, etc), but it seems to cover the most important ones.\n\nüëâ Here is my ORCID ID and page: \n\nhttps://orcid.org/0000-0002-2391-0678.\n\nORCID has \n\na public-facing API that allows you to automatically query information about an ORCID ID.\nFollowing \n\nan example notebook shared from the TAPIR project.","type":"content","url":"/blog/2022/orcid-auto-update#use-orcid-to-grab-a-list-of-dois-for-my-publications","position":3},{"hierarchy":{"lvl1":"Automatically updating my publications page with ORCID and doi.org","lvl2":"Use the doi.org API to grab citation information"},"type":"lvl2","url":"/blog/2022/orcid-auto-update#use-the-doi-org-api-to-grab-citation-information","position":4},{"hierarchy":{"lvl1":"Automatically updating my publications page with ORCID and doi.org","lvl2":"Use the doi.org API to grab citation information"},"content":"While the ORCID API has a lot of useful information in it, there were some important pieces missing, like co-author information.\nFortunately, I learned \n\nfrom a suggestion on Twitter that \n\ndoi.org is accessible via an API call!\n\nYou can ask doi.org for the reference, bibtex file, or a JSON structure of reference data by adding a header to a \n\ndoi.org URL, like so:curl -L -H \"Accept:text/x-bibliography; style=apa\" -H \"User-Agent: mailto:youremail@email.com\" https://dx.doi.org/10.1371/journal.pcbi.1009651\n\nThis returns a fully-resolved reference like so:DuPre, E., Holdgraf, C., Karakuzu, A., Tetrel, L., Bellec, P., Stikov, N., & Poline, J.-B. (2022). Beyond advertising: New infrastructures for publishing integrated research objects. PLOS Computational Biology, 18(1), e1009651. https://doi.org/10.1371/journal.pcbi.1009651\n\nNote\n\nThe -H \"User-Agent: mailto:youremail@email.com\" is a way to identify yourself to the doi.org API, which reduces the likelihood that you will have your access revoked.\n\nIn Python, the same call looks like this:from requests import get\ndoi = \"10.1371/journal.pcbi.1009651\"\nurl = f\"https://dx.doi.org/{doi}\"\nheader = {\"accept\": \"text/x-bibliography; style=apa\", \"User-Agent\": \"mailto:youremail@email.com\"}\nr = requests.get(url, headers=header)\nprint(r.content)\n\nYou can also use other kinds of header configuration, like:# A citeproc-styled JSON structure\nheader = {'accept': \"citeproc+json\"}\n# A bibtex entry\nheader = {'accept': \"bibtex\"}\n\nThe citeproc JSON structure has a ton of information in it, including information about all of the co-authors (and an extra bonus - a link to their ORCID pages!).\nThis is all the information I needed for my website.","type":"content","url":"/blog/2022/orcid-auto-update#use-the-doi-org-api-to-grab-citation-information","position":5},{"hierarchy":{"lvl1":"Automatically updating my publications page with ORCID and doi.org","lvl2":"A script to do this all at once"},"type":"lvl2","url":"/blog/2022/orcid-auto-update#a-script-to-do-this-all-at-once","position":6},{"hierarchy":{"lvl1":"Automatically updating my publications page with ORCID and doi.org","lvl2":"A script to do this all at once"},"content":"I wrote a little script that runs each time my Sphinx site is built.\nIt generates a markdown snippet that is then inserted into my publications.md page.\n\nPython snippet to download ORCID data\n\nNote that the below is \n\na jupytext document which is why there‚Äôs extra metadata at the top.\n\n# ---\n# jupyter:\n#   jupytext:\n#     formats: py:light\n#     text_representation:\n#       extension: .py\n#       format_name: light\n#       format_version: '1.5'\n#       jupytext_version: 1.14.1\n#   kernelspec:\n#     display_name: Python 3 (ipykernel)\n#     language: python\n#     name: python3\n# ---\n\n# +\nimport pandas as pd\nimport requests\nfrom IPython.display import Markdown, JSON\nfrom pathlib import Path\nfrom rich import progress\n\n# My ORCID\norcid_id = \"0000-0002-2391-0678\"\nORCID_RECORD_API = \"https://pub.orcid.org/v3.0/\"\n\n# Download all of my ORCID records\nprint(\"Retrieving ORCID entries from API...\")\nresponse = requests.get(\n    url=requests.utils.requote_uri(ORCID_RECORD_API + orcid_id),\n    headers={\"Accept\": \"application/json\"},\n)\nresponse.raise_for_status()\norcid_record = response.json()\n\n# +\n# Just to visualize in a notebook if need be\n# JSON(orcid_record)\n\n# +\n\n###\n# Resolve my DOIs from ORCID as references\n# Shamelessly copied from:\n# https://gist.github.com/brews/8d3b3ede15d120a86a6bd6fc43859c5e\nimport requests\nimport json\n\n\ndef fetchmeta(doi, fmt=\"reference\", **kwargs):\n    \"\"\"Fetch metadata for a given DOI.\n\n    Parameters\n    ----------\n    doi : str\n    fmt : str, optional\n        Desired metadata format. Can be 'dict' or 'bibtex'.\n        Default is 'dict'.\n    **kwargs :\n        Additional keyword arguments are passed to `json.loads()` if `fmt`\n        is 'dict' and you're a big JSON nerd.\n\n    Returns\n    -------\n    out : str or dict or None\n        `None` is returned if the server gives unhappy response. Usually\n        this means the DOI is bad.\n\n    Examples\n    --------\n    >>> fetchmeta('10.1016/j.dendro.2018.02.005')\n    >>> fetchmeta('10.1016/j.dendro.2018.02.005', 'bibtex')\n\n    References\n    ----------\n    https://www.doi.org/hb.html\n    \"\"\"\n    # Parse args and setup the server response we want.\n    accept_type = \"application/\"\n    if fmt == \"dict\":\n        accept_type += \"citeproc+json\"\n    elif fmt == \"bibtex\":\n        accept_type += \"x-bibtex\"\n    elif fmt == \"reference\":\n        accept_type = \"text/x-bibliography; style=apa\"\n    else:\n        raise ValueError(f\"Unrecognized `fmt`: {fmt}\")\n\n    # Request data from server.\n    url = \"https://dx.doi.org/\" + str(doi)\n    header = {\"accept\": accept_type}\n    r = requests.get(url, headers=header)\n\n    # Format metadata if server response is good.\n    out = None\n    if r.status_code == 200:\n        if fmt == \"dict\":\n            out = json.loads(r.text, **kwargs)\n        else:\n            out = r.text\n    return out\n\n\n# -\n\n# Extract metadata for each entry\ndf = []\nfor iwork in progress.track(\n    orcid_record[\"activities-summary\"][\"works\"][\"group\"], \"Fetching reference data...\"\n):\n    isummary = iwork[\"work-summary\"][0]\n\n    # Extract the DOI\n    for ii in isummary[\"external-ids\"][\"external-id\"]:\n        if ii[\"external-id-type\"] == \"doi\":\n            year = isummary[\"publication-date\"][\"year\"][\"value\"]\n            doi = ii[\"external-id-value\"]\n            break\n    df.append({\"year\": year, \"doi\": doi})\ndf = pd.DataFrame(df)\n\n# Convert into a markdown string\nmd = [\"|Year|Publications|\", \"|===|===|\"]\nfor year, items in df.groupby(\"year\", sort=False):\n    this_pubs = []\n    for _, item in items.iterrows():\n        this_pubs.append(f'{{cite}}`{item[\"doi\"]}`')\n    md.append(f\"|{year}|{', '.join(this_pubs)}|\")\nmds = \"\\n\".join(md)\n\n# +\n# Uncomment to preview in a notebook\n# Markdown(mds)\n# -\n\n# This will only work if this is run as a script\npath_out = Path(__file__).parent.parent / \"_static/publications.txt\"\npath_out.write_text(mds)\nprint(f\"Finished updating ORCID entries at: {path_out}\")\n","type":"content","url":"/blog/2022/orcid-auto-update#a-script-to-do-this-all-at-once","position":7},{"hierarchy":{"lvl1":"Fix phantom GitHub workflows in your ci-cd with protected branch rules"},"type":"lvl1","url":"/blog/2022/phantom-workflows-pull-requests","position":0},{"hierarchy":{"lvl1":"Fix phantom GitHub workflows in your ci-cd with protected branch rules"},"content":"Have you ever had a GitHub pull request show ‚Äúphantom‚Äù workflows that never pass?\nThis looks like one or more workflows that are in a constant waiting state, with a yellow status indicator, and that never complete.\n\nIt looks something like this:\n\nIf you run into this, it may be because of \n\nbranch protection rules in their repository.\nThese allow you to ensure that certain conditions are met before a pull request can be merged.\n\nIn particular, a common step is to ensure that a certain GitHub Workflow passes before you can merge.\nThis is often the source of the phantom workflows that never complete.\nThis is configured via settings -> branches -> branch protection rules, like so:\n\nIf you specify a test here, and then delete it in your YAML configuration file, this will cause phantom workflows in your Pull Requests.\nGitHub will wait for the test to finish, because it is specified in your repository settings, but it will never finish because the test isn‚Äôt configured in your workflow configuration.\n\nSo, to fix it, you simply delete the entry in your protected branch rules associated with that test.\nAs a result, GitHub will no longer wait for that test to complete, and you can merge away.\n\nThis one has bitten me several times, and so I decided to just document it here for future reference.\n\n\nJacob Tomlinson was the one that first told me about this fix!","type":"content","url":"/blog/2022/phantom-workflows-pull-requests","position":1},{"hierarchy":{"lvl1":"Automatically update pre-commit hook versions"},"type":"lvl1","url":"/blog/2022/precommit-autoupdate","position":0},{"hierarchy":{"lvl1":"Automatically update pre-commit hook versions"},"content":"I figured out a way to automatically update all of the git pre-commit hook versions at once!\n\npre-commit is a useful command line tool for running simple commands before every git commit.\nI use it to enforce things like \n\nflake8 and \n\nblack in many of my projects.\n\nHowever, I find it really annoying to keep manually updating my pre-commit hooks with new versions, particularly because pre-commit doesn‚Äôt let you specify wild-cards.\n\nFortunately, I recently came across \n\nthe pre-commit autoupdate documentation.\nThis lets you automatically update to the latest released versions of all-precommit hooks.\nSimply run:pre-commit autoupdate\n\nAnd it will update your .pre-commit-config.yaml file with the latest versions.\nThis feels like the easiest way to keep these configurations updated, at least until \n\nGitHub adds dependabot support for pre-commit.","type":"content","url":"/blog/2022/precommit-autoupdate","position":1},{"hierarchy":{"lvl1":"Automatically update pre-commit hook versions","lvl2":"Automate the above with pre-commit.ci"},"type":"lvl2","url":"/blog/2022/precommit-autoupdate#automate-the-above-with-pre-commit-ci","position":2},{"hierarchy":{"lvl1":"Automatically update pre-commit hook versions","lvl2":"Automate the above with pre-commit.ci"},"content":"Update: A few folks mentioned that you can actually automate this whole process by using \n\npre-commit.ci, a service for using pre-commit‚Äôs functionality with automated jobs.\n\nThat service will both automatically run pre-commit on your Pull Requests, and will also update your pre-commit dependencies on the fly.","type":"content","url":"/blog/2022/precommit-autoupdate#automate-the-above-with-pre-commit-ci","position":3},{"hierarchy":{"lvl1":"subprocess.run can execute shell commands directly"},"type":"lvl1","url":"/blog/2022/shell-split","position":0},{"hierarchy":{"lvl1":"subprocess.run can execute shell commands directly"},"content":"I often run shell commands in Python via the \n\nsubprocess.run command.\nOne thing that has always bugged me is that this required you to split commands into a list before it‚Äôd work properly.\nFor example, you‚Äôd have to do:import subprocess\nimport shlex\n\nsubprocess.run(*shlex.split(\"ls -l\"))\n\nToday I discovered that you don‚Äôt have to do this!\nThere‚Äôs a shell= keyword that can be used to tell subprocess to simply run the command directly in the shell.\n\nFor example:import subprocess\nsubprocess.run(\"ls -l\", shell=True)\n\nApparently there are some \n\nsecurity considerations but this seems like a big papercut saver to me.","type":"content","url":"/blog/2022/shell-split","position":1},{"hierarchy":{"lvl1":"Custom roles and domains in Sphinx with one line"},"type":"lvl1","url":"/blog/2022/sphinx-custom-crossrefs","position":0},{"hierarchy":{"lvl1":"Custom roles and domains in Sphinx with one line"},"content":"I was working on \n\nthe roles and structure section of the 2i2c Team Compass and found a nifty feature in Sphinx that I hadn‚Äôt known before.\n\nYou can currently add labels to any section with the following MyST Markdown structure:(mylabel)=\n## My header\n\nAnd now I [reference it](mylabel).\n\nHowever, there are no semantics attached to this label.\nInstead I‚Äôd like to be able to specify what kind of a label this is.\nIn my case, it‚Äôs because I wanted to define a group of labels attached to our organization‚Äôs roles.\n\nFortunately this is pretty easy to do!\nJust not well-documented.\n\nHere‚Äôs how to define a custom role and reference it in Sphinx:\n\nRegister your new role group. In your conf.py configuration, use app.add_crossref_type like so:\n\ndef setup(app):\n    app.add_crossref_type(\"labelmygroup\", \"mygrp\")\n\nconf.py\n\nThe first argument labelmygroup is the name of a directive that you‚Äôll now be able to use to attach a section of your documentation to this role group.\n\nThe second argument mygrp is the name of a role that you can now use to refer to an group that was labeled with labelmygroup.\n\nAdd a group label to a section with a directive. You can do this like so:```{labelmygroup} Some name\n```\n## Group description\n\nReference the label with a role. You can now reference the Some name group with a role, like so: Here is {mygrp}`Some name`.\n\nThis is a nice way to have semantic references throughout your docs (like {role}`Executive Director`) rather than generic ones.\n\nSee \n\nthe Sphinx Documentation on this for more details.","type":"content","url":"/blog/2022/sphinx-custom-crossrefs","position":1},{"hierarchy":{"lvl1":"Automatically redirect folders in Sphinx websites"},"type":"lvl1","url":"/blog/2022/sphinx-redirects-folder","position":0},{"hierarchy":{"lvl1":"Automatically redirect folders in Sphinx websites"},"content":"I spent a bit of time today updating my website after some changes in the MyST-NB and Sphinx Design ecosystems.\nAlong the way, I decided to redirect /posts/ to /blog/, since it seems /blog/ is a much more common folder to use for blog posts.\n\nThis posed a problem, because \n\nthe sphinx-rediraffe extension does not allow you to redirect folders with wildcards.\nAKA, you cannot do:rediraffe_redirects = {\n    \"posts/**/*.md\": \"blog/**/*.md\",\n}\n\nI also didn‚Äôt want to have to manually specify every single blog post, since that‚Äôd be a very long list.\n\nFortunately, I figured out a solution because Sphinx‚Äôs configuration is also a Python script.\nThis means you can dynamically populate this configuration with pathlib.\nI‚Äôll share the code snippet below in case it‚Äôs useful for others:\n\n# These are posts I *want* to manually specify\nrediraffe_redirects = {\n    \"rust-governance.md\": \"blog/2018/rust_governance.md\",\n}\n# Update the posts/* section of the rediraffe redirects to find all files\nredirect_folders = {\n    \"posts\": \"blog\",\n}\nfrom pathlib import Path\nfor old, new in redirect_folders.items():\n    for newpath in Path(new).rglob(\"**/*\"):\n        if newpath.suffix in [\".ipynb\", \".md\"]:\n            oldpath = str(newpath).replace(\"blog/\", \"posts/\", 1)\n            rediraffe_redirects[oldpath] = str(newpath)\n\nconf.py","type":"content","url":"/blog/2022/sphinx-redirects-folder","position":1},{"hierarchy":{"lvl1":"How to update Sphinx options during the build"},"type":"lvl1","url":"/blog/2022/sphinx-update-config","position":0},{"hierarchy":{"lvl1":"How to update Sphinx options during the build"},"content":"As part of \n\nthe pydata-sphinx-theme we have a few settings that auto-enable extensions and configure them on behalf of the user.\nIt has always been mysterious to me how to do this properly during the Sphinx build.\nIt‚Äôs easy to configure things with conf.py ahead of time, but what if you want to manually set a value during the build?\n\nI finally figured it out, so documenting the process here.","type":"content","url":"/blog/2022/sphinx-update-config","position":1},{"hierarchy":{"lvl1":"How to update Sphinx options during the build","lvl2":"Use the builder-inited event"},"type":"lvl2","url":"/blog/2022/sphinx-update-config#use-the-builder-inited-event","position":2},{"hierarchy":{"lvl1":"How to update Sphinx options during the build","lvl2":"Use the builder-inited event"},"content":"Define a Sphinx event for builder-inited. This will trigger after the builder has been selected, but before the environent is finalized for the build.\nThis should be a function that takes a single (app) parameter.\n\nSee Also\n\nSee \n\nthe Sphinx Core Events documentation for more information about Sphinx‚Äôs events system.","type":"content","url":"/blog/2022/sphinx-update-config#use-the-builder-inited-event","position":3},{"hierarchy":{"lvl1":"How to update Sphinx options during the build","lvl2":"Use app._raw_config to find the user-provided config"},"type":"lvl2","url":"/blog/2022/sphinx-update-config#use-app-raw-config-to-find-the-user-provided-config","position":4},{"hierarchy":{"lvl1":"How to update Sphinx options during the build","lvl2":"Use app._raw_config to find the user-provided config"},"content":"Use the app._raw_config object to detect user-given config. \n\nThis is first written when Sphinx is initialized and should be a good indication of what the user provided.\n\nThis is useful if you only want to over-ride something if the user didn‚Äôt set it themselves.\nHowever, the app.config object will have all of the config options, including defaults.","type":"content","url":"/blog/2022/sphinx-update-config#use-app-raw-config-to-find-the-user-provided-config","position":5},{"hierarchy":{"lvl1":"How to update Sphinx options during the build","lvl2":"Update configuration options with the app.config object"},"type":"lvl2","url":"/blog/2022/sphinx-update-config#update-configuration-options-with-the-app-config-object","position":6},{"hierarchy":{"lvl1":"How to update Sphinx options during the build","lvl2":"Update configuration options with the app.config object"},"content":"Update app.config.keyname.\nYou can set and update configuration values directly with app.config.keyname = \"foo\".\nThe way \n\nSphinx does this is by directly setting app.config.__dict__[\"keyname\"] = \"foo\", but this doesn‚Äôt seem to be necessary and I‚Äôm not sure why they do it that way.\n\napp.config.values has the defaults!\n\nIt might seem like app.config.values is the right place for this, but that isn't the case. app.config.values` has the defaults for Sphinx, and if you directly replace items, then Sphinx will behave unpredictably.\n\n\nHere‚Äôs where Sphinx configures these defaults","type":"content","url":"/blog/2022/sphinx-update-config#update-configuration-options-with-the-app-config-object","position":7},{"hierarchy":{"lvl1":"How to update Sphinx options during the build","lvl2":"Update HTML theme options with app.builder.theme_options"},"type":"lvl2","url":"/blog/2022/sphinx-update-config#update-html-theme-options-with-app-builder-theme-options","position":8},{"hierarchy":{"lvl1":"How to update Sphinx options during the build","lvl2":"Update HTML theme options with app.builder.theme_options"},"content":"Update app.builder.theme_options.\nMany people (including myself) incorrectly try to update app.config.html_theme_options during a build event.\nBut this doesn‚Äôt do anything because the‚Äôve already been copied over to app.builder.theme_options early in the build process.\nAnnoyingly, the copied dictionary in app.builder.theme_options does not point to the same points in memory as app.config.html_theme_options.\n\nThis \n\ncopy action is done here.","type":"content","url":"/blog/2022/sphinx-update-config#update-html-theme-options-with-app-builder-theme-options","position":9},{"hierarchy":{"lvl1":"How to update Sphinx options during the build","lvl2":"An example"},"type":"lvl2","url":"/blog/2022/sphinx-update-config#an-example","position":10},{"hierarchy":{"lvl1":"How to update Sphinx options during the build","lvl2":"An example"},"content":"Here‚Äôs an example of the whole process in action:# This function will update a single configuration value\ndef update_config(app):\n   # Check if a value was provided by the user\n   if \"foo\" in app.config._raw_values:\n      # Update a config value\n      app.config.__dict__[\"foo\"] = \"bar\"\n\n   # Update an HTML theme value\n   app.builder.theme_options[\"foo2\"] = \"bar2\"\n\n# Register the above function to be called during the builder-inited phase\ndef setup(app):\n  app.connect(\"builder-inited\", update_config)","type":"content","url":"/blog/2022/sphinx-update-config#an-example","position":11},{"hierarchy":{"lvl1":"A few random opportunities in AI for Social Good"},"type":"lvl1","url":"/blog/2023/ai-for-good","position":0},{"hierarchy":{"lvl1":"A few random opportunities in AI for Social Good"},"content":"Recently a few friends have reached out asking if I knew of any opportunities to work on AI-related things that also have some kind of pro-social tie-in.\nI think a lof people see AI as a technology with a lot of potential, but in an environment of companies that don‚Äôt seem to prioritize the benefit of human-kind over the never-ending hype machine and the promise of hyperscale growth.\n\nSo I asked around a few places to see if folks had recommendations for using skills in machine learning or artificial intelligence, but in a context that was explicitly for the benefit of humanity or otherwise pro-social.\nHere are a few that stood out (they are 100% not vetted, I‚Äôm just passing along information in case it‚Äôs useful so please tell me if there‚Äôs something wrong or problematic in here):\n\nDataScience for Social Good (DSSG):\n\nDSSG leverages data science to tackle social challenges and will likely intersect with the AI/ML space as well.\n\nDataseer:\n\nUses AI but in the context of research paper analytics for scholarly knowledge generation.\n\nErsilia:\n\nProvides data science tools for infections and neglected disease research, largely for universities, hospitals, and laboratories in low-resourced countries.\n\nChan Zuckerberg Initiative (CZI):\n\nCZI has an open position for Director of Artificial Intelligence across CZI‚Äôs programs.\n\nDAIR Institute:\n\nDAIR is a space for independent, community-rooted AI research, free from Big Tech‚Äôs pervasive influence.\n\nOpen@RIT:\n\nDoes a lot of work at the intersection of open science, data science, and open source technology, and is involved in some projects with ML/AI.\n\nPolicy and Ethics in AI:\n\nThere are many policy and advocacy efforts around AI.\nIn particular, the \n\nElectronic Frontier Foundation was mentioned along with a community called \n\n‚ÄúAI Snake Oil‚Äù and the US Digital Response.\n\nA recent AI for Good Panel Discussion:\n\nThe panel, hosted by \n\nDeeplearning.AI, explored real-world problem-solving using AI.\n\nStanford Human-Centered AI group (HAI):\n\nHAI at Stanford focuses on human-centric AI research, with research opportunities.\n\nOpen Science Initiatives in AI:\n\nThere are some community initiatives in AI, such as \n\nCohere for AI, and the \n\nResponsible AI License (RAIL) initiative.\n\nData research, advocacy, and technology institutes:\n\nA few more institutes were mentioned: the \n\nAI Now Institute, the \n\nAda Lovelace Institute, and \n\nConnected by Data.\n\nThis is not even close to an exhaustive list, but it‚Äôs what came out of this relatively simple question.\nMaybe somebody will find these useful to follow up on!\nI was encouraged by how many people responded to my questions - it seems like the intersection of AI and social good is on many people‚Äôs minds.","type":"content","url":"/blog/2023/ai-for-good","position":1},{"hierarchy":{"lvl1":"Report from FOSDEM23: beautiful chaos in a conference"},"type":"lvl1","url":"/blog/2023/fosdem","position":0},{"hierarchy":{"lvl1":"Report from FOSDEM23: beautiful chaos in a conference"},"content":"I recently attended \n\nFOSDEM 2023, my first FOSDEM!\nI had heard of the conference before, but hadn‚Äôt really looked into it too much.\nFortunately, after some urging from friends and social media, I took a deeper look and decided I should join to see what all the fuss was about.\n\nHere are a few things that I noticed while I was there.","type":"content","url":"/blog/2023/fosdem","position":1},{"hierarchy":{"lvl1":"Report from FOSDEM23: beautiful chaos in a conference","lvl2":"FOSDEM is beautiful chaos"},"type":"lvl2","url":"/blog/2023/fosdem#fosdem-is-beautiful-chaos","position":2},{"hierarchy":{"lvl1":"Report from FOSDEM23: beautiful chaos in a conference","lvl2":"FOSDEM is beautiful chaos"},"content":"I feel like FOSDEM tries to bring the ‚Äúbeautiful chaos‚Äù of open source communities into a conference setting.\nThere‚Äôs no registration, and little ‚Äúconference infrastructure‚Äù compared to other conferences of its size and scope.\nPeople who attended for the first time seemed to be overwhelmed in a similar way to when they navigate a large open source community for the first time.\n\nThe conference relies heavily on volunteer labor, and only has a few roles and responsibilities where they deploy paid resources and equipment.\nGenerally speaking, this seemed to work surprisingly well!\nThe conference is very big, and the fact that thousands of people can swarm a university in Brussels and largely have a productive time is a testament to FOSDEM‚Äôs ability to self-organize.","type":"content","url":"/blog/2023/fosdem#fosdem-is-beautiful-chaos","position":3},{"hierarchy":{"lvl1":"Report from FOSDEM23: beautiful chaos in a conference","lvl2":"But yes, it is very chaotic üôÉ"},"type":"lvl2","url":"/blog/2023/fosdem#but-yes-it-is-very-chaotic","position":4},{"hierarchy":{"lvl1":"Report from FOSDEM23: beautiful chaos in a conference","lvl2":"But yes, it is very chaotic üôÉ"},"content":"That said, the chaos occasionally gets in the way of having a good conference experience.\nI probably spent 20% of my time being confused about how to find the talk I was trying to attend, and it didn‚Äôt help that the \n\nULB Campus is large and hard to navigate.\n\nThere was also a lack of organization-wide policy and enforcement for a few high-level things.\nFor example, there didn‚Äôt seem to be any kind of official policy towards COVID and masking procedures.\nThe general vibe seemed to be ‚Äúif you‚Äôre feeling sick you should probably wear a mask or stay home‚Äù, but there was no mechanism to enforce this.\nSome sessions had facilitators with stronger opinions on this, some had less-strong opinions, but your experience depended a lot on the specific groups you were hanging around with.\n\nI suspect that this makes FOSDEM less accessible.\nWithout a strong policy for community behavior in certain key areas, people will not know what to expect when they arrive.\nWith enough uncertainty over important topics (e.g. if a person who really needs to avoid COVID), some people will simply filter themselves out.\nI bet this is one place where FOSDEM suffers from its lack of structure or resources - something like setting and enforcing a COVID policy takes a lot of work and resources.\nIt reminds me of the community policing and policy-setting parts of open communities that are often under-apprciated, very stressful, and very complex.","type":"content","url":"/blog/2023/fosdem#but-yes-it-is-very-chaotic","position":5},{"hierarchy":{"lvl1":"Report from FOSDEM23: beautiful chaos in a conference","lvl2":"It is HUGE, as is the open source community"},"type":"lvl2","url":"/blog/2023/fosdem#it-is-huge-as-is-the-open-source-community","position":6},{"hierarchy":{"lvl1":"Report from FOSDEM23: beautiful chaos in a conference","lvl2":"It is HUGE, as is the open source community"},"content":"FOSDEM reminded me just how gigantic the open source community is.\nI tend to spend my time in the scientific python and open research space (there was an \n\nOpen Research Tools and Technology devroom), but this was just a tiny part of the FOSDEM conference.\n\nI often think of \n\nProject Jupyter as a ‚Äúlarge‚Äù open source project, but you really get a feel for how tiny it is when see how many people show up representing \n\nFedora Linux.\nThere are large and complex communities out there, with their own stakeholder dynamics and focus areas.\nA place like FOSDEM is an opportunity to cross community boundaries and learn about the problems others have, and how they‚Äôre trying to solve them.","type":"content","url":"/blog/2023/fosdem#it-is-huge-as-is-the-open-source-community","position":7},{"hierarchy":{"lvl1":"Report from FOSDEM23: beautiful chaos in a conference","lvl2":"All of our communities have the same problems"},"type":"lvl2","url":"/blog/2023/fosdem#all-of-our-communities-have-the-same-problems","position":8},{"hierarchy":{"lvl1":"Report from FOSDEM23: beautiful chaos in a conference","lvl2":"All of our communities have the same problems"},"content":"And on that note, every community seems to have the same problems.\nI spoke with a number of folks from adjacent communities, and many of the stories were the same:\n\nWe want to give sub-communities the freedom to do what they want, but find that it becomes hard to get everybody moving in the same direction.\n\nWe have a few large stakeholders that employ people to work directly on the project, and this gives them a level of power that many are not comfortable with.\n\nWe have a hard time attracting new core contributors unless they end up working at a specific subset of companies.\n\nWe are worried about company XXX building a similar tool that competes with our open source project because they didn‚Äôt want to abide by our community governance.\n\nWe systemically under-resource community guidance, management, and strategy efforts.\n\nWe are worried about our increasing reliance on a software and services that are controlled by a few giant tech companies.\n\nThe list goes on.\nIn some ways, this was reassuring, the problems in all of my communities are certainly not unique.\nIn other ways, it was discouraging that so many of these projects have the same problems and yet none have really managed to solve them.\nMaybe that‚Äôs because solving these problems can‚Äôt happen with a single action, but more like a series of mini-solutoins and continued improvements over time.","type":"content","url":"/blog/2023/fosdem#all-of-our-communities-have-the-same-problems","position":9},{"hierarchy":{"lvl1":"Report from FOSDEM23: beautiful chaos in a conference","lvl2":"It is really a bunch of mini-conferences all strung together"},"type":"lvl2","url":"/blog/2023/fosdem#it-is-really-a-bunch-of-mini-conferences-all-strung-together","position":10},{"hierarchy":{"lvl1":"Report from FOSDEM23: beautiful chaos in a conference","lvl2":"It is really a bunch of mini-conferences all strung together"},"content":"As a result of a general lack of structure, FOSDEM really felt like a bunch of little conferences all strung together.\nThe main ‚Äúunit‚Äù of the conference is a \n\ndevroom, which is a way to organize talks and people around a particular subject.\n\nMany people seemed to pick a single devroom and stick with it the whole time.\nYou could also tell that there was a lot of cultural correlation between people in the same devroom, so you could have more consistent behavioal norms within each one.\nDevrooms were places where colleagues were more likely too meet up with each other, and the way to ensure you‚Äôre meeting other people with similar interests.\nYou heard statements like ‚ÄúI‚Äôm going to spend the day hanging out in the security devroom.‚Äù\nSome of the devrooms even provided their own baked goods and snacks for those who attended!\n\nIf I go to FOSDEM again, I‚Äôll pay more attention to the devrooms and pick one to three where I should hang out and learn.\nThis seems like a better strategy than cherry-picking each talk.","type":"content","url":"/blog/2023/fosdem#it-is-really-a-bunch-of-mini-conferences-all-strung-together","position":11},{"hierarchy":{"lvl1":"Report from FOSDEM23: beautiful chaos in a conference","lvl2":"The community devroom is pretty great"},"type":"lvl2","url":"/blog/2023/fosdem#the-community-devroom-is-pretty-great","position":12},{"hierarchy":{"lvl1":"Report from FOSDEM23: beautiful chaos in a conference","lvl2":"The community devroom is pretty great"},"content":"The devroom where I spent the most time was \n\nthe community devroom.\nPerhaps unsurprisingly, I really enjoyed the talks that I heard, and felt that most people there came with a similar perspective to myself.\nI was happy to see that there was so much emphasis on the human and community aspects of open source, especially in light of recent \n\ncultural tensions in the ‚ÄúFree Software Foundation‚Äù world.\nIt was worried that there would be a ‚Äúit‚Äôs just code, don‚Äôt bother me with human problems like diversity and inclusion‚Äù vibe, but for the spaces that I participated in, this was not a huge issue (I suspect it would have been different in other devrooms).","type":"content","url":"/blog/2023/fosdem#the-community-devroom-is-pretty-great","position":13},{"hierarchy":{"lvl1":"Report from FOSDEM23: beautiful chaos in a conference","lvl2":"‚ÄúWe don‚Äôt do that here‚Äù is a nice tool for policing behavior"},"type":"lvl2","url":"/blog/2023/fosdem#we-dont-do-that-here-is-a-nice-tool-for-policing-behavior","position":14},{"hierarchy":{"lvl1":"Report from FOSDEM23: beautiful chaos in a conference","lvl2":"‚ÄúWe don‚Äôt do that here‚Äù is a nice tool for policing behavior"},"content":"One thing that stood out to me was the following phrase:\n\nWe don‚Äôt do that here.\n\nThis described a way to politely but firmly tell somebody that their behavior wasn‚Äôt acceptible in a community.\nIt is a way of making expectations clear without opening the floor up for debate about the cultural expectations themselves (there‚Äôs a time and a place for this, but the middle of a violation of that culture is not the moment to debate it).\nI‚Äôll spend more time thinking about this and how I can practice it in my own work.","type":"content","url":"/blog/2023/fosdem#we-dont-do-that-here-is-a-nice-tool-for-policing-behavior","position":15},{"hierarchy":{"lvl1":"Report from FOSDEM23: beautiful chaos in a conference","lvl2":"People were more explicit about their open-source and employer hats"},"type":"lvl2","url":"/blog/2023/fosdem#people-were-more-explicit-about-their-open-source-and-employer-hats","position":16},{"hierarchy":{"lvl1":"Report from FOSDEM23: beautiful chaos in a conference","lvl2":"People were more explicit about their open-source and employer hats"},"content":"One thing that I felt in almost all of the devrooms was a tension between ‚Äúcorporate‚Äù and ‚Äúnon-corporate‚Äù tech.\nOn the one hand, you have those driven by ideals and values who believe in ‚ÄúFree as in speech‚Äù open source and see corporations as inherently mis-aligned with those ideals.\nOn the other hand, there‚Äôs representation from a lot of big tech companies at the conference and they clearly see it as a strategically important conference to attend.\nThis is something that I‚Äôve felt in basically every tech conference, but especially felt it at FOSDEM.\n\nOne way that this became clear is the fact that more people spoke about ‚Äúhats‚Äù at this conference than I usually notice.\nI heard several stories of people intentionally not attending their company‚Äôs booth because they explicitly wanted to attend via their affiliation to an open source project.\nIn some cases I heard of people not reimbursing their expenses because they didn‚Äôt want to feel beholden to their employer‚Äôs strategy at the conference.\nI thought that was pretty cool, as I‚Äôm a big fan of recognizing that employers and open source communities have distinct missions and goals.","type":"content","url":"/blog/2023/fosdem#people-were-more-explicit-about-their-open-source-and-employer-hats","position":17},{"hierarchy":{"lvl1":"Report from FOSDEM23: beautiful chaos in a conference","lvl2":"More conferences need to be free"},"type":"lvl2","url":"/blog/2023/fosdem#more-conferences-need-to-be-free","position":18},{"hierarchy":{"lvl1":"Report from FOSDEM23: beautiful chaos in a conference","lvl2":"More conferences need to be free"},"content":"Finally, it really stood out to me that FOSDEM was free for anybody to attend.\nI had many conversations with people who noted that they wouldn‚Äôt have attended if they had to pay a significant cost to do so (travel and hotels is already expensive enough!), and the extra effort of requiring ‚Äúapplications for financial aid‚Äù is both off-putting and a burden (why should people have to do extra work to attend just because they don‚Äôt make as much money?).\n\nI wish that more conferences found a way to make attendence free with zero extra effort from attendees.\nFor example, I \n\nreally like how Open News prototyped a ‚Äúpay what you can‚Äù model for their annual conference.\nIf that means that we need to say in cities and venues with lower cost, and lower our production values a little bit, then I think that is 100% worth it.\n\nI know that it takes (some financial) resources to run something complex like a conference.\nBut come on, many of the organizations represented there have market capitalizations in the billions, we should be able to find a way to subsidize many tickets down to zero.","type":"content","url":"/blog/2023/fosdem#more-conferences-need-to-be-free","position":19},{"hierarchy":{"lvl1":"Report from FOSDEM23: beautiful chaos in a conference","lvl2":"A conference that reflects the open source community"},"type":"lvl2","url":"/blog/2023/fosdem#a-conference-that-reflects-the-open-source-community","position":20},{"hierarchy":{"lvl1":"Report from FOSDEM23: beautiful chaos in a conference","lvl2":"A conference that reflects the open source community"},"content":"It really is extraordinary that a conference like FOSDEM exists in the first place.\nThe fact that thousands of people around the world just...show up in Brussles and arrive at the same place without any official registration is pretty cool.\nIt reminds me of the ways that many people are surprised when they first learn how open source communities work.\n\nAnd just as with open source communities, I suspect that there is a lot of unpaid and under-appreciated labor that goes into organizing FOSDEM.\nSo a million thanks to the organizers and those who participated.\nIt is a conference format and a cultural phenomenon that shows off the power of open community models (as well as some of its challenges).\nMaybe I‚Äôll see you in a devroom next time!\n\nThis was originally \n\nwritten about in this blog post from Aja Hammerly and described at the conference in \n\na talk by Floor‚Äôd for teaching me about it in their talk.\n\nPerhaps that is because the corporate side of tech has largely won the battle in many other tech conferences that have become expensive and inaccessible to many people who refuse to join the corporate secetor.","type":"content","url":"/blog/2023/fosdem#a-conference-that-reflects-the-open-source-community","position":21},{"hierarchy":{"lvl1":"A Sphinx directive for social media embeds"},"type":"lvl1","url":"/blog/2023/social-directive","position":0},{"hierarchy":{"lvl1":"A Sphinx directive for social media embeds"},"content":"This probably doesn‚Äôt work anymore\n\nI‚Äôve since moved my blog to use \n\nthe MyST Document Engine so this example will no longer work on my personal blog. See \n\nthis permalink for the latest working version.\n\nI often want to link to social and other types of web-based media in my Sphinx documentation and blog.\nRather than embedding it all in custom HTML code, I decided to write a little wrapper to turn it into a directive.\n\nIt‚Äôs called {socialpost}, and it works with Twitter, Mastodon, and YouTube links.","type":"content","url":"/blog/2023/social-directive","position":1},{"hierarchy":{"lvl1":"A Sphinx directive for social media embeds","lvl2":"How it works"},"type":"lvl2","url":"/blog/2023/social-directive#how-it-works","position":2},{"hierarchy":{"lvl1":"A Sphinx directive for social media embeds","lvl2":"How it works"},"content":"Here‚Äôs a brief description of how this directive works:\n\nParse the directive content (the thing that comes after {socialpost}, e.g. https://twitter.com/choldgraf/status/1564614538309390345)\n\nDo some basic pattern matching to decide if it is Twitter / Mastodon / YouTube\n\nParse the URL for the proper unique identifier for the post (e.g. above it is 1564614538309390345)\n\nUse an embed template that embeds this identifier into each service. E.g., for Twitter it is:<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">\n  <a href=\"https://twitter.com/choldgraf/status/1564614538309390345\">Tweet from @choldgraf</a>\n</blockquote>\n\nThis is all wrapped up in a Directive object that outputs an HTML raw node so I can pass through raw HTML.\n\nLoad any necessary JS files if this directive is detected on a page.\nThis is done with an html-page-context event.\n\nI then connected them both to Sphinx in my site‚Äôs conf.py setup function.","type":"content","url":"/blog/2023/social-directive#how-it-works","position":3},{"hierarchy":{"lvl1":"Bundle extensions with your Sphinx theme"},"type":"lvl1","url":"/blog/2023/sphinx-add-extensions","position":0},{"hierarchy":{"lvl1":"Bundle extensions with your Sphinx theme"},"content":"Sphinx is great because it has a ton of useful extensions that let you grow its functionality.\nHowever, a downside of this is that users have to actually learn about those extensions and activate them manually.\nIt‚Äôs not hard, but it‚Äôs a non-trivial amount of discovery work.\n\nOne way to solve this is for themes to bundle extensions on their own.\nThis way they can include functionality via an extension rather than writing custom code on their own.\n\nHowever, this doesn‚Äôt often happen, I think because it can be pretty confusing how to do so.\nI believe I‚Äôve figured out the major gotchas to avoid and patterns to follow, so here‚Äôs a quick overview.","type":"content","url":"/blog/2023/sphinx-add-extensions","position":1},{"hierarchy":{"lvl1":"Bundle extensions with your Sphinx theme","lvl2":"Use app.setup_extension to add the extension"},"type":"lvl2","url":"/blog/2023/sphinx-add-extensions#use-app-setup-extension-to-add-the-extension","position":2},{"hierarchy":{"lvl1":"Bundle extensions with your Sphinx theme","lvl2":"Use app.setup_extension to add the extension"},"content":"First off, the \n\nSphinx docs mention app.setup_extension as ‚Äúthe‚Äù way to set up a Sphinx extension.\n\nThis activates the extension as if you‚Äôd put it in the extensions variable.\nHowever, it leaves out a lot of nuance.","type":"content","url":"/blog/2023/sphinx-add-extensions#use-app-setup-extension-to-add-the-extension","position":3},{"hierarchy":{"lvl1":"Bundle extensions with your Sphinx theme","lvl2":"Don‚Äôt forget the theme is activated after config-inited"},"type":"lvl2","url":"/blog/2023/sphinx-add-extensions#dont-forget-the-theme-is-activated-after-config-inited","position":4},{"hierarchy":{"lvl1":"Bundle extensions with your Sphinx theme","lvl2":"Don‚Äôt forget the theme is activated after config-inited"},"content":"Many extensions add functionality in \n\nSphinx events.\nThese are emitted throughout the build process and you can hook into them with functions to modify the document etc.\n\nHowever, your theme is activated after the config-inited event.\nThat means that if you manually activate an extension, but that extension registers a callback that waits for config-inited, then nothing will happen.\n\nSo in general, if something doesn‚Äôt seem like it‚Äôs happening, then double check that your extension isn‚Äôt using a config-inited event hook.\n\nSphinx extension authors: don‚Äôt use config-inited!\n\nBecause of this limitation, I‚Äôd recommend that Sphinx extension authors use builder-inited rather than config-inited.\nIt is also early in the build process, but after the theme is activated.","type":"content","url":"/blog/2023/sphinx-add-extensions#dont-forget-the-theme-is-activated-after-config-inited","position":5},{"hierarchy":{"lvl1":"Bundle extensions with your Sphinx theme","lvl2":"Workaround: You can try emitting your own config-inited"},"type":"lvl2","url":"/blog/2023/sphinx-add-extensions#workaround-you-can-try-emitting-your-own-config-inited","position":6},{"hierarchy":{"lvl1":"Bundle extensions with your Sphinx theme","lvl2":"Workaround: You can try emitting your own config-inited"},"content":"I think I figured out a workaround to the above limitation in case you can‚Äôt control which events your extensions hook into.\nIt‚Äôs based on the idea that Sphinx has a registry of event listeners at app.events.listeners.\nThis is a dictionary of event-name:[list-of-listener-objects] pairs.\nWhen app.emit(event) is triggered, it loops through the respective list and runs each listener object.\n\nSo we should be able to just remove that list, then activate our extensions (which will add to the list), and then manually trigger the config-inited event.\nTry the following steps:\n\nStore a copy of the old event listeners.\nThey‚Äôve already been run (since config-inited has happened already), but we‚Äôll keep them just in case.old_listeners = app.events.listeners[\"config-inited\"]\n\nReplace the old listeners with an empty list.\nThis means there are effectively no listeners for the event.app.events.listeners[\"config-inited\"] = []\n\nActivate your extra extensions.\nYou can now loop through the list of extensions you want to bundle and activate each one.for ext in your_extensions:\n   app.setup_extension(ext)\n\nThis will add event listeners to config-inited for any extensions that require it.\nAny extension that has already been activated will be a no-op.\n\nEmit your own config-inited event.\nNow that you‚Äôve activated the extensions and added their event listeners, re-emit the config-inited event.\nThis will now only call the new listeners you‚Äôve added.app.emit(\"config-inited\", app.config)\n\nCombine the two lists of listeners.\nFinally, combine the two lists of listeners just so that your Sphinx application‚Äôs state matches what has actually happened (I have no idea if this is actually needed).# This prepends the list\n# ref: https://stackoverflow.com/questions/19736058/can-i-extend-list-in-python-with-prepend-elements-instead-of-append\napp.events.listeners[\"config-inited\"][:0] = old_listeners \n\nNow you can let the Sphinx build move forward as normal.\nYou will have manually emitted the config-inited event for your new extensions, and they‚Äôll be picked up for any future events that happen.","type":"content","url":"/blog/2023/sphinx-add-extensions#workaround-you-can-try-emitting-your-own-config-inited","position":7},{"hierarchy":{"lvl1":"Bundle extensions with your Sphinx theme","lvl2":"Addendum: you could also make your theme an extension"},"type":"lvl2","url":"/blog/2023/sphinx-add-extensions#addendum-you-could-also-make-your-theme-an-extension","position":8},{"hierarchy":{"lvl1":"Bundle extensions with your Sphinx theme","lvl2":"Addendum: you could also make your theme an extension"},"content":"If you really don‚Äôt want to do the hacky steps above, another option is to ask users to add your theme as an extension as well as a theme.\nFor example:extensions = [\"my_theme\"]\nhtml_theme = \"my_theme\"\n\nHowever, I‚Äôve found that this usually confuses people and is pretty clunky from a UX perspective.\nI‚Äôd much rather try to handle the complexity under the hood.","type":"content","url":"/blog/2023/sphinx-add-extensions#addendum-you-could-also-make-your-theme-an-extension","position":9},{"hierarchy":{"lvl1":"Better blog lists with the MyST AST"},"type":"lvl1","url":"/blog/2024/blog-list","position":0},{"hierarchy":{"lvl1":"Better blog lists with the MyST AST"},"content":"On my journey to learn more about writing with \n\nthe new MyST engine, I built upon \n\nmy recent update to my blog infrastructure and made some improvements to my blog post list.\nHere‚Äôs what it looks like now:\n\nClick here to see how it looks now\n\nBetter blog lists with the MyST AST\n\n\n\n\nOn my journey to learn more about writing with [the new MyST engine](https:///mystmd.org), I built upon [my recent update to my blog infrastructure](./programmatic-myst-with-jupyter.md) and made some improvements to my blog post list.\nHere's what it looks like now:\n\n````{note} Click here to see how it looks now\n:class: dropdown\n```{postlist}\n:number: 3\n```\n````\n\nHere's a quick rundown\n\nDate: November 09, 2024 | Author: Chris Holdgraf\n\nGenerate MyST with Jupyter and insert it into content programmatically\n\n\n\n\n\nWhile I've been [converting my blog to use the new MyST engine](./mystmd-with-the-blog.md), I discovered a useful MyST feature. It's not yet possible to [natively parse Jupyter Markdown outputs as MyST](https://github.com/jupyter-book/mystmd/issues/1026) but there's a workaround if you don't mind generating a temporary file.\n\nThe trick is to _write to a temporary file_\n\nDate: November 04, 2024 | Author: Chris Holdgraf\n\nRe-building my blog with MySTMD\n\n\n\nWow it has been a long time since I've last-written here. It turns out that having two small children and a very demanding job means you don't have as much time for blogging. But that's a whole different blog post...\n\nI've decided to convert my blog to use the new [MyST\n\nDate: November 01, 2024 | Author: Chris Holdgraf\n\nHere‚Äôs a quick rundown of what I‚Äôve improved.","type":"content","url":"/blog/2024/blog-list","position":1},{"hierarchy":{"lvl1":"Better blog lists with the MyST AST","lvl2":"Use the MyST sandbox to determine what AST to generate"},"type":"lvl2","url":"/blog/2024/blog-list#use-the-myst-sandbox-to-determine-what-ast-to-generate","position":2},{"hierarchy":{"lvl1":"Better blog lists with the MyST AST","lvl2":"Use the MyST sandbox to determine what AST to generate"},"content":"I realized that MyST cards are a first-class citizen in the AST, meaning that I should be able to generate them directly with my Python plugin. These look a lot nicer than a list of bullet points.\n\nHowever, generating MyST AST from scratch is cumbersome, so I headed over to \n\nthe MyST sandbox to quickly see what the AST needed to look like for card outputs.\n\n\n\nUsing the sandbox to preview what the AST looks like.\n\n\nHere‚Äôs an example of a card directive in the MyST sandbox.\n\nWith this in mind, I simply modified \n\nmy blogpost Python script to generate AST like the above rather than the bulleted list I was generating before.\n\nGenerating output manually with MyST AST takes some getting used-to, but the sandbox-based workflow above helps a lot.\nI think it‚Äôll be way nicer once we can do this programmatically with Jupyter cells, \n\nhere‚Äôs the issue tracking parsing cell outputs as MyST and \n\nthis one specifically about generating AST from notebook cells.","type":"content","url":"/blog/2024/blog-list#use-the-myst-sandbox-to-determine-what-ast-to-generate","position":3},{"hierarchy":{"lvl1":"Re-building my blog with MySTMD"},"type":"lvl1","url":"/blog/2024/mystmd-with-the-blog","position":0},{"hierarchy":{"lvl1":"Re-building my blog with MySTMD"},"content":"Wow it has been a long time since I‚Äôve last-written here. It turns out that having two small children and a very demanding job means you don‚Äôt have as much time for blogging. But that‚Äôs a whole different blog post...\n\nI‚Äôve decided to convert my blog to use the new \n\nMyST Document Engine. This is part of a dogfooding experiment to see what‚Äôs possible with MyST, since it‚Äôs where the Jupyter Book project is heading, and I want to see how close to ‚Äúproduction-ready‚Äù we are already.\n\nTo begin, I wanted to share a few things I learned today as I tried to re-tool my old blog for use with MyST MD.","type":"content","url":"/blog/2024/mystmd-with-the-blog","position":1},{"hierarchy":{"lvl1":"Re-building my blog with MySTMD","lvl2":"There‚Äôs no blog functionality in MyST yet"},"type":"lvl2","url":"/blog/2024/mystmd-with-the-blog#theres-no-blog-functionality-in-myst-yet","position":2},{"hierarchy":{"lvl1":"Re-building my blog with MySTMD","lvl2":"There‚Äôs no blog functionality in MyST yet"},"content":"As MyST is still quite young, there‚Äôs no out-of-the-box functionality for MyST (see \n\njupyter‚Äã-book‚Äã/mystmd‚Äã#840 for the issue tracking that). So, I wanted to accomplish at least two things with my initial transfer:\n\nGenerate a list of recent blog posts that I can insert into a few places in my site.\n\nGenerate an RSS feed that keeps the same URLs, content, etc.","type":"content","url":"/blog/2024/mystmd-with-the-blog#theres-no-blog-functionality-in-myst-yet","position":3},{"hierarchy":{"lvl1":"Re-building my blog with MySTMD","lvl2":"What didn‚Äôt work: using the parsed MyST documents"},"type":"lvl2","url":"/blog/2024/mystmd-with-the-blog#what-didnt-work-using-the-parsed-myst-documents","position":4},{"hierarchy":{"lvl1":"Re-building my blog with MySTMD","lvl2":"What didn‚Äôt work: using the parsed MyST documents"},"content":"My first thought was to use a MyST plugin that defines a directive I could use to insert a list of blog posts.\nHowever, I learned that MyST plugins have no way to access all of the parsed documents at build time (see \n\nthis issue about accessing all of the parsed documents to track that one).\n\nFortunately, \n\n@rowanc1 made me realize that I could just manually parse my blog files and use that to build up something like a blog list. So that‚Äôs what the rest of this post is about.","type":"content","url":"/blog/2024/mystmd-with-the-blog#what-didnt-work-using-the-parsed-myst-documents","position":5},{"hierarchy":{"lvl1":"Re-building my blog with MySTMD","lvl2":"You can run scripts in JavaScript as part of your MyST build"},"type":"lvl2","url":"/blog/2024/mystmd-with-the-blog#you-can-run-scripts-in-javascript-as-part-of-your-myst-build","position":6},{"hierarchy":{"lvl1":"Re-building my blog with MySTMD","lvl2":"You can run scripts in JavaScript as part of your MyST build"},"content":"The \n\nMySTMD plugins documentation shows a few examples for how to define your own MyST plugins. These are little JavaScript files that get executed every time you build your MyST site.\n\nThe easiest thing to do here would be to write a JavaScript plugin for MyST that I can attach to my site build. You could use a JS library to parse the markdown files in blog/, grab their YAML metadata, and return MyST AST structure that would be inserted into the document. But I‚Äôm not too comfortable with JavaScript, and I found two ways that are much hackier, but much more accessible for somebody that is comfortable with Python üòé.","type":"content","url":"/blog/2024/mystmd-with-the-blog#you-can-run-scripts-in-javascript-as-part-of-your-myst-build","position":7},{"hierarchy":{"lvl1":"Re-building my blog with MySTMD","lvl2":"Write a MyST extension in Python"},"type":"lvl2","url":"/blog/2024/mystmd-with-the-blog#write-a-myst-extension-in-python","position":8},{"hierarchy":{"lvl1":"Re-building my blog with MySTMD","lvl2":"Write a MyST extension in Python"},"content":"I bet most folks don‚Äôt know that you can write MyST extensions entirely in Python (or any other language that you can execute locally). \n\nHere is the MyST documentation on writing an executable MyST extension.\n\nExecutable extensions are treated like a black box in MyST - the MyST build process simply executes a file that you specify in myst.yml, and treats anything printed to stdout as MyST AST that will be inserted back into the MyST document.\n\nCheck out the examples\n\nThere seem to be some specific ways to define the arguments that your script takes, based on whether a role, directive, or transform triggers it. See the \n\nexamples in the executable plugin docs for inspiration.","type":"content","url":"/blog/2024/mystmd-with-the-blog#write-a-myst-extension-in-python","position":9},{"hierarchy":{"lvl1":"Re-building my blog with MySTMD","lvl2":"How do you know what MyST AST looks like?"},"type":"lvl2","url":"/blog/2024/mystmd-with-the-blog#how-do-you-know-what-myst-ast-looks-like","position":10},{"hierarchy":{"lvl1":"Re-building my blog with MySTMD","lvl2":"How do you know what MyST AST looks like?"},"content":"I mention ‚Äúall you need to do is output MyST AST‚Äù, but what does that even mean?\nThe MyST AST is the abstract structure of a MyST document. It has all of the relevant information about the content in a document, as well as all the semantic tags for different types of content that can exist (e.g., ‚Äúitalics‚Äù, or ‚Äúadmonition boxes‚Äù).\n\nWhen a MyST Markdown document is parsed, the result is MyST AST. You can see a ton of examples of AST for various MyST markdown in \n\nthe MyST guide.\nJust look for the litte ‚Äúinteractive demo‚Äù boxes that show off sample MyST Markdown.\n\nUse the JSON representation of the AST\n\nYou‚Äôll note that clicking the AST button gives you a few different ways to view it.\nBy default, you‚Äôll see YAML, but JSON is closer to the structure that MyST plugins expect you to output. In my case, I grabbed the JSON output, and converted it for use as a Python dictionary (this basically just meant turning true into True).\n\nIn my case, I needed a list of blog posts, so I found the relevant AST for what this looks like at the following locations:\n\nThe \n\nlist items documentation showed me the AST for lists.\n\nThe \n\nDefinition lists documentation had sample AST for an internal link.\n\nThe \n\nInline text formatting documentation had examples for things like bold, italics, etc.","type":"content","url":"/blog/2024/mystmd-with-the-blog#how-do-you-know-what-myst-ast-looks-like","position":11},{"hierarchy":{"lvl1":"Re-building my blog with MySTMD","lvl2":"Turning this into a Python plugin for MyST"},"type":"lvl2","url":"/blog/2024/mystmd-with-the-blog#turning-this-into-a-python-plugin-for-myst","position":12},{"hierarchy":{"lvl1":"Re-building my blog with MySTMD","lvl2":"Turning this into a Python plugin for MyST"},"content":"With this in mind, I wrote a little Python extension that:\n\nAt build time, parses all of my blog post markdown files and extracts metadata from their YAML headers.\n\nDefines a bloglist directive that will insert a list of blog posts where it exists.\n\nManually converts the blog post metadata into MyST AST that it prints to stdout.\n\nAs a result, when I call the directive in my blog, it will replace the directive with whatever AST is spit out by the Python script. You can take a look at \n\nthe entire Python script here.\n\nNow I can insert lists of blog posts wherever I like, for example here‚Äôs a list of the latest three:\n\nBetter blog lists with the MyST AST\n\n\n\n\nOn my journey to learn more about writing with [the new MyST engine](https:///mystmd.org), I built upon [my recent update to my blog infrastructure](./programmatic-myst-with-jupyter.md) and made some improvements to my blog post list.\nHere's what it looks like now:\n\n````{note} Click here to see how it looks now\n:class: dropdown\n```{postlist}\n:number: 3\n```\n````\n\nHere's a quick rundown\n\nDate: November 09, 2024 | Author: Chris Holdgraf\n\nGenerate MyST with Jupyter and insert it into content programmatically\n\n\n\n\n\nWhile I've been [converting my blog to use the new MyST engine](./mystmd-with-the-blog.md), I discovered a useful MyST feature. It's not yet possible to [natively parse Jupyter Markdown outputs as MyST](https://github.com/jupyter-book/mystmd/issues/1026) but there's a workaround if you don't mind generating a temporary file.\n\nThe trick is to _write to a temporary file_\n\nDate: November 04, 2024 | Author: Chris Holdgraf\n\nRe-building my blog with MySTMD\n\n\n\nWow it has been a long time since I've last-written here. It turns out that having two small children and a very demanding job means you don't have as much time for blogging. But that's a whole different blog post...\n\nI've decided to convert my blog to use the new [MyST\n\nDate: November 01, 2024 | Author: Chris Holdgraf","type":"content","url":"/blog/2024/mystmd-with-the-blog#turning-this-into-a-python-plugin-for-myst","position":13},{"hierarchy":{"lvl1":"Re-building my blog with MySTMD","lvl2":"Adding an RSS feed"},"type":"lvl2","url":"/blog/2024/mystmd-with-the-blog#adding-an-rss-feed","position":14},{"hierarchy":{"lvl1":"Re-building my blog with MySTMD","lvl2":"Adding an RSS feed"},"content":"Because I‚Äôm running Python to define my MyST plugin, I can also use Python to build a custom RSS feed! This was relatively easy since I‚Äôd already parsed the metadata from each file.\n\nI found \n\nthe python-feedgen package, which is a little helper package for generating RSS feeds in Python. Since my MyST plugin was already written in Python, I just added a few more lines to do so.\n\nClick to see the whole Python plugin script\n\n#!/usr/bin/env python3\nimport argparse\nimport json\nimport sys\nfrom yaml import safe_load\nfrom pathlib import Path\nimport pandas as pd\nfrom feedgen.feed import FeedGenerator\nimport unist as u\n\nDEFAULTS = {\"number\": 10}\n\nroot = Path(__file__).parent.parent\n\n# Aggregate all posts from the markdown and ipynb files\nposts = []\nfor ifile in root.rglob(\"blog/**/*.md\"):\n    if \"drafts\" in str(ifile):\n        continue\n\n    text = ifile.read_text()\n    try:\n        _, meta, content = text.split(\"---\", 2)\n    except Exception:\n        print(f\"Skipping file with error: {ifile}\", file=sys.stderr)\n        continue\n\n    # Load in YAML metadata\n    meta = safe_load(meta)\n    meta[\"path\"] = ifile.relative_to(root).with_suffix(\"\")\n    if \"title\" not in meta:\n        lines = text.splitlines()\n        for ii in lines:\n            if ii.strip().startswith(\"#\"):\n                meta[\"title\"] = ii.replace(\"#\", \"\").strip()\n                break\n    \n    # Summarize content\n    skip_lines = [\"#\", \"--\", \"%\", \"++\"]\n    content = \"\\n\".join(ii for ii in content.splitlines() if not any(ii.startswith(char) for char in skip_lines))\n    N_WORDS = 50\n    words = \" \".join(content.split(\" \")[:N_WORDS])\n    if not \"author\" in meta or not meta[\"author\"]:\n        meta[\"author\"] = \"Chris Holdgraf\"\n    meta[\"content\"] = meta.get(\"description\", words)\n    posts.append(meta)\nposts = pd.DataFrame(posts)\nposts[\"date\"] = pd.to_datetime(posts[\"date\"]).dt.tz_localize(\"US/Pacific\")\nposts = posts.dropna(subset=[\"date\"])\nposts = posts.sort_values(\"date\", ascending=False)\n\n# Generate an RSS feed\nfg = FeedGenerator()\nfg.id(\"http://chrisholdgraf.com\")\nfg.title(\"Chris Holdgraf's blog\")\nfg.author({\"name\": \"Chris Holdgraf\", \"email\": \"choldgraf@gmail.com\"})\nfg.link(href=\"http://chrisholdgraf.com\", rel=\"alternate\")\nfg.logo(\"http://chrisholdgraf.com/_static/profile.jpg\")\nfg.subtitle(\"Chris' personal blog!\")\nfg.link(href=\"http://chrisholdgraf.com/rss.xml\", rel=\"self\")\nfg.language(\"en\")\n\n# Add all my posts to it\nfor ix, irow in posts.iterrows():\n    fe = fg.add_entry()\n    fe.id(f\"http://chrisholdgraf.com/{irow['path']}\")\n    fe.published(irow[\"date\"])\n    fe.title(irow[\"title\"])\n    fe.link(href=f\"http://chrisholdgraf.com/{irow['path']}\")\n    fe.content(content=irow[\"content\"])\n\n# Write an RSS feed with latest posts\nfg.atom_file(root / \"atom.xml\", pretty=True)\nfg.rss_file(root / \"rss.xml\", pretty=True)\n\nplugin = {\n    \"name\": \"Blog Post list\",\n    \"directives\": [\n        {\n            \"name\": \"postlist\",\n            \"doc\": \"An example directive for showing a nice random image at a custom size.\",\n            \"alias\": [\"bloglist\"],\n            \"arg\": {},\n            \"options\": {\n                \"number\": {\n                    \"type\": \"int\",\n                    \"doc\": \"The number of posts to include\",\n                }\n            },\n        }\n    ],\n}\n\nchildren = []\nfor ix, irow in posts.iterrows():\n    children.append(\n        {\n          \"type\": \"card\",\n          \"url\": f\"/{irow['path'].with_suffix('')}\",\n          \"children\": [\n            {\n              \"type\": \"cardTitle\",\n              \"children\": [u.text(irow[\"title\"])]\n            },\n            {\n              \"type\": \"paragraph\",\n              \"children\": [u.text(irow['content'])]\n            },\n            {\n              \"type\": \"footer\",\n              \"children\": [\n                u.strong([u.text(\"Date: \")]), u.text(f\"{irow['date']:%B %d, %Y} | \"),\n                u.strong([u.text(\"Author: \")]), u.text(f\"{irow['author']}\"),\n              ]\n            },\n          ]\n        }\n    )\n\n\ndef declare_result(content):\n    \"\"\"Declare result as JSON to stdout\n\n    :param content: content to declare as the result\n    \"\"\"\n\n    # Format result and write to stdout\n    json.dump(content, sys.stdout, indent=2)\n    # Successfully exit\n    raise SystemExit(0)\n\n\ndef run_directive(name, data):\n    \"\"\"Execute a directive with the given name and data\n\n    :param name: name of the directive to run\n    :param data: data of the directive to run\n    \"\"\"\n    assert name == \"postlist\"\n    opts = data[\"node\"].get(\"options\", {})\n    number = int(opts.get(\"number\", DEFAULTS[\"number\"]))\n    output = children[:number]\n    return output\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    group = parser.add_mutually_exclusive_group()\n    group.add_argument(\"--role\")\n    group.add_argument(\"--directive\")\n    group.add_argument(\"--transform\")\n    args = parser.parse_args()\n\n    if args.directive:\n        data = json.load(sys.stdin)\n        declare_result(run_directive(args.directive, data))\n    elif args.transform:\n        raise NotImplementedError\n    elif args.role:\n        raise NotImplementedError\n    else:\n        declare_result(plugin)\n\n\nAnnoyingly, you cannot just tell MyST to put a file in a particular location (see \n\njupyter‚Äã-book‚Äã/mystmd‚Äã#1196 tracking this one). So I had to manually move this file to my build output folder in my GitHub action. Hopefully this functionality gets updated soon. Here‚Äôs what that looks like:\n\n    # Move RSS feeds to output folder\n    - name: Move RSS feeds\n      run: |\n        cp atom.xml _build/html/atom.xml\n        cp rss.xml _build/html/rss.xml\n\n    # If we've pushed to main, push the book's HTML to github-pages","type":"content","url":"/blog/2024/mystmd-with-the-blog#adding-an-rss-feed","position":15},{"hierarchy":{"lvl1":"Generate MyST with Jupyter and insert it into content programmatically"},"type":"lvl1","url":"/blog/2024/programmatic-myst-with-jupyter","position":0},{"hierarchy":{"lvl1":"Generate MyST with Jupyter and insert it into content programmatically"},"content":"While I‚Äôve been \n\nconverting my blog to use the new MyST engine, I discovered a useful MyST feature. It‚Äôs not yet possible to \n\nnatively parse Jupyter Markdown outputs as MyST but there‚Äôs a workaround if you don‚Äôt mind generating a temporary file.\n\nThe trick is to write to a temporary file in your Jupyter cell, and then include the temporary output file with an {include} directive in your MyST markdown.\nThis allows you to directly write MyST Markdown without worrying about what the MyST AST specification looks like.\n\nFor example, the following code cell writes some sample text to a .txt file in my MyST build directory.\n\nfrom pathlib import Path\np = Path(\"../_build/txt/tmp.txt\")\np.parent.mkdir(parents=True, exist_ok=True)\n# Grab a list of all the filenames\nfiles = list(p.rglob(\"../**/*.md\"))\ntxt = \"\\n- \".join(files)\n_ = p.write_text(txt)\n\nAnd we can then include it in the page with MyST markdown like so:```{include} ../_build/txt/tmp.txt\n```\n\nThe page will be executed first, and afterward, the page will be parsed into MyST, thus using the temporary file we‚Äôve included.","type":"content","url":"/blog/2024/programmatic-myst-with-jupyter","position":1},{"hierarchy":{"lvl1":"Hi, I‚Äôm Chris Holdgraf üëã"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Hi, I‚Äôm Chris Holdgraf üëã"},"content":"\n\nA bit about me...\n\nüíº Executive Director @ \n\n2i2c\n\n‚òÅÔ∏è Former Cloud DataHub team @ \n\nBerkeley CDSS\n\nüåï Distinguished Contributor @ \n\nThe Jupyter Project\n\nüß† PhD graduate in neuroscience @ \n\nUC Berkeley\n\nI also work extensively with \n\nProject Jupyter, particularly \n\nthe Binder Project and \n\nJupyter Book.\n\nAbout me ‚ÑπÔ∏è\n\nProjects I‚Äôve worked on üîß\n\nMy blog ‚úçÔ∏è","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Hi, I‚Äôm Chris Holdgraf üëã","lvl2":"Recent blog posts"},"type":"lvl2","url":"/#recent-blog-posts","position":2},{"hierarchy":{"lvl1":"Hi, I‚Äôm Chris Holdgraf üëã","lvl2":"Recent blog posts"},"content":"Better blog lists with the MyST AST\n\n\n\n\nOn my journey to learn more about writing with [the new MyST engine](https:///mystmd.org), I built upon [my recent update to my blog infrastructure](./programmatic-myst-with-jupyter.md) and made some improvements to my blog post list.\nHere's what it looks like now:\n\n````{note} Click here to see how it looks now\n:class: dropdown\n```{postlist}\n:number: 3\n```\n````\n\nHere's a quick rundown\n\nDate: November 09, 2024 | Author: Chris Holdgraf\n\nGenerate MyST with Jupyter and insert it into content programmatically\n\n\n\n\n\nWhile I've been [converting my blog to use the new MyST engine](./mystmd-with-the-blog.md), I discovered a useful MyST feature. It's not yet possible to [natively parse Jupyter Markdown outputs as MyST](https://github.com/jupyter-book/mystmd/issues/1026) but there's a workaround if you don't mind generating a temporary file.\n\nThe trick is to _write to a temporary file_\n\nDate: November 04, 2024 | Author: Chris Holdgraf\n\nRe-building my blog with MySTMD\n\n\n\nWow it has been a long time since I've last-written here. It turns out that having two small children and a very demanding job means you don't have as much time for blogging. But that's a whole different blog post...\n\nI've decided to convert my blog to use the new [MyST\n\nDate: November 01, 2024 | Author: Chris Holdgraf\n\nA few random opportunities in AI for Social Good\n\n\n\n\nRecently a few friends have reached out asking if I knew of any opportunities to work on AI-related things that also have some kind of pro-social tie-in.\nI think a lof people see AI as a technology with a lot of potential, but in an environment of companies that don't seem\n\nDate: October 02, 2023 | Author: Chris Holdgraf\n\nA Sphinx directive for social media embeds\n\n\n\n\n:::{note} This probably doesn't work anymore\nI've since moved my blog to use [the MyST Document Engine](https://mystmd.org) so this example will no longer work on my personal blog. See [this permalink for the latest working version](https://github.com/choldgraf/choldgraf.github.io/blob/ae8ee9792c74aac72f46c645d19352abc439d572/blog/2023/social-directive.md).\n:::\n\nI often want to link to social and other types of web-based media in my Sphinx\n\nDate: February 15, 2023 | Author: Chris Holdgraf\n\nReport from FOSDEM23: beautiful chaos in a conference\n\n\n\n\nI recently attended [FOSDEM 2023](https://fosdem.org/2023/), my first FOSDEM!\nI had heard of the conference before, but hadn't really looked into it too much.\nFortunately, after some urging from friends and social media, I took a deeper look and decided I should join to see what all the fuss was about.\n\nHere are a\n\nDate: February 06, 2023 | Author: Chris Holdgraf\n\nBundle extensions with your Sphinx theme\n\n\n\n\nSphinx is great because it has a ton of useful extensions that let you grow its functionality.\nHowever, a downside of this is that users have to actually _learn about_ those extensions and activate them manually.\nIt's not hard, but it's a non-trivial amount of discovery work.\n\nOne way to solve this is\n\nDate: January 19, 2023 | Author: Chris Holdgraf\n\nInstall dependencies from GitHub with `pyproject.toml` or `requirements.txt`\n\n\n\n\nThis is a short post to demonstrate how to install packages directly from GitHub with `pyprojects.toml` or `requirements.txt`, including custom branches and commits.\nIt will focus on `pyprojects.toml` because this is newer and there's less information about it, but the general pattern holds for `requirements.txt` as well.\n\nIn `pyproject.toml`, you can specify\n\nDate: December 31, 2022 | Author: Chris Holdgraf\n\nReport from the JupyterLite workshop: WebAssembly is pretty cool\n\n\n\n\nI recently attended [the JupyterLite community workshop in Paris](https://blog.jupyter.org/community-workshop-jupyterlite-e992c61f5d7f?source=collection_home---6------6-----------------------), here are some quick thoughts from the three-day event[^ack].\n\n[^ack]: Many thanks to the [QuantStack](http://quantstack.com/) team for organizing this event, and to [OVHCloud](https://www.ovhcloud.com/en/) for providing a physical space for everyone. \n\nFor those without any background, JupyterLite is a distribution of Jupyter's user\n\nDate: December 10, 2022 | Author: Chris Holdgraf\n\nLoad and plot a remote font with Matplotlib\n\n\n\n\nAs part of [my `sphinx-social-previews`](https://github.com/choldgraf/sphinx-social-previews) prototype, I wanted to be able to use the [Roboto Font from Google](https://fonts.google.com/specimen/Roboto) in image previews.\nHowever, Roboto is often not loaded on your local filesystem, so it took some digging to figure out how to make it possible to load via [Matplotlib's text plotting functionality](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.text.html).\n\nHere's\n\nDate: December 06, 2022 | Author: Chris Holdgraf","type":"content","url":"/#recent-blog-posts","position":3},{"hierarchy":{"lvl1":"Projects"},"type":"lvl1","url":"/projects","position":0},{"hierarchy":{"lvl1":"Projects"},"content":"This page describes some of the major projects that I have worked on over the past and present.","type":"content","url":"/projects","position":1},{"hierarchy":{"lvl1":"Projects","lvl2":"Current projects"},"type":"lvl2","url":"/projects#current-projects","position":2},{"hierarchy":{"lvl1":"Projects","lvl2":"Current projects"},"content":"\n\n2i2c\n\nThe International Interactive Computing Collaboration is a non-profit organization dedicated to developing and providing infrastructure for interactive computing in research and education. It is an attempt at providing sustainable, vendor-agnostic, and open infrastructure that maximizes flexibility, choice, and productivity for research and education.\n\n\n\nJupyter Book\n\nJupyter Book is an open source project for building beautiful, publication-quality books and documents from computational material.\n\nIt is stewarded by the \n\nExecutable Books Project, an international collaboration to build open-source tools that facilitate publishing computational narratives using the Jupyter ecosystem.\n\n\n\nThe Binder Project\n\nBinder allows you to create custom computing environments that can be\nshared and used by many remote users. It is powered by BinderHub,\nan open-source tool to deploy the Binder service in the cloud.\nOne such deployment lives at \n\nmybinder.org,\nwhere it is run as a free service.\n\nThe goal of Binder is to enable people to share reproducible, interactive versions\nof their code with others as easily as possible. It is used by people across\nthe scientific, education, and analytics communities.\n\n\n\nJupyterHub\n\nJupyterHub is a tool that lets an administrator serve many user sessions\nfrom a single machine. The \n\nZero to JupyterHub guide is an instructional and\nopinionated guide to deploying a JupyterHub on Kubernetes, a framework for\ndeploying / managing cloud resources.\n\nThe Zero to JupyterHub guide was originally written as an extension of\nthe technical infrastructure for UC Berkeley‚Äôs Data 8 course, and since then\nhas become the most popular method for running a JupyterHub at scale in the cloud.","type":"content","url":"/projects#current-projects","position":3},{"hierarchy":{"lvl1":"Projects","lvl2":"Past projects"},"type":"lvl2","url":"/projects#past-projects","position":4},{"hierarchy":{"lvl1":"Projects","lvl2":"Past projects"},"content":"\n\nThe Docathon\n\nThe \n\nDocathon was a week-long global sprint where we focus our efforts on\nimproving the state of documentation in the open-source and open-science\nworld. This means writing better documentation, building tools,\nand sharing skills.\n\nThe first Docathon was held in 2017, and had participants from across the globe.\n\nMore than 40 open-source projects contributed, and in total we put out a\nroughly ten-fold increase in contributions to documentation over the week!\n\n\n\nMNE-Python\n\nMNE-Python is open-source software for exploring, visualizing, and analyzing\nhuman neurophysiological data (MEG, EEG, sEEG, ECoG, etc).\n\nAfter my PhD, I spent some time generalizing the code I had written for \n\nreceptive field\nanalysis\nof human ECoG data, which now exists in MNE-Python.","type":"content","url":"/projects#past-projects","position":5},{"hierarchy":{"lvl1":"Publications"},"type":"lvl1","url":"/publications","position":0},{"hierarchy":{"lvl1":"Publications"},"content":"This page lists some major publications I have been involved with.\nIt is generated automatically via the ORCID API.\n\n\nHere‚Äôs a blog post describing the approach I use for this.\n\nüëâ ORCID page: \n\norcid.org/0000-0002-2391-0678\n\nüëâ \n\nGoogle Scholar page\n\nYear\n\nPublications\n\n2023\n\nZheng et al. (2023)\n\n2022\n\nDuPre et al. (2022)\n\n2021\n\nRokem et al. (2021), \n\nGentemann et al. (2021)\n\n2019\n\nAppelhoff et al. (2019), \n\nHoldgraf et al. (2018), \n\nWasser et al. (2019), \n\nYarkoni et al. (2019)\n\n2018\n\nGeiger et al. (2018), \n\nGeiger et al. (2018), \n\nJupyter et al. (2018)\n\n2017\n\nHoldgraf et al. (2017), \n\nHoldgraf et al. (2017)\n\n2016\n\nHoldgraf et al. (2016)\n\n2014\n\nMartin et al. (2014)\n\n2012\n\nSfondouris et al. (2012)","type":"content","url":"/publications","position":1},{"hierarchy":{"lvl1":"Talks"},"type":"lvl1","url":"/talks","position":0},{"hierarchy":{"lvl1":"Talks"},"content":"Below are a few highlighted talks that I have given recently.\n\n2i2c, Building a Cloud Computing Infrastructure for Research and Education\n\nPSL Demo Day 2023\n\nThe New Jupyter Book Stack\n\nJupyterCon 2020\n\nJupyter in Education @ Berkeley\n\nUC Berkeley Data Science Education Program Workshop 2020\n\nOpen Infrastructure for Open Science\n\nCSVConf 2019\n\nAn Overview of Project Jupyter and Binder\n\nGoogle Cloud Platform Podcast\n\nBinder 2.0: Next Gen of Reproducible Scientific Environments w/ repo2docker & BinderHub\n\nSciPy 2018\n\nVisualizing the Human Brain\n\nPlotcon 2016","type":"content","url":"/talks","position":1}]}